# GPT Model Configuration - Optimized for Jane Austen Training
# Format: PARAMETER_NAME=value

# Tokenizer configuration
# Options: char, word, sentencepiece
TOKENIZER_TYPE=char
# Vocabulary size (only used for word and sentencepiece tokenizers)
# For char tokenizer, vocab size is determined automatically from text
VOCAB_SIZE=2000

# Model architecture parameters (optimized for 4MB dataset)
D_MODEL=128
CONTEXT_LEN=256
ATTENTION_HEADS=8
DECODER_BLOCKS=4
DROPOUT_RATE=0.1

# Training parameters (balanced for good results)
LEARNING_RATE=0.0001
EPOCHS=15
STEPS_PER_EPOCH=500
WARMUP_RATIO=0.1
PEAK_LEARNING_RATE=5e-4
MIN_LEARNING_RATE=1e-6

# Training settings
BATCH_SIZE=8
RECORDS_PER_FILE=100
