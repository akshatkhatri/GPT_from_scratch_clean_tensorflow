#!/usr/bin/env python3

import gradio as gr
import tensorflow as tf
import numpy as np
import json
import os
import sys
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import base64
import io
from PIL import Image

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.layers import *
from models.utils import *

class GPTDeployer:
    def __init__(self, experiment_dir):
        self.experiment_dir = experiment_dir
        self.load_model_and_tokenizer()
        
    def load_model_and_tokenizer(self):
        with open(f"{self.experiment_dir}/tokenizer.json", 'r') as f:
            self.tokenizer_data = json.load(f)
        
        self.token_to_id = self.tokenizer_data['token_to_id']
        self.id_to_token = self.tokenizer_data['id_to_token']
        
        with open(f"{self.experiment_dir}/model_config.json", 'r') as f:
            config_data = json.load(f)
        
        if 'architecture' in config_data:
            self.model_config = config_data['architecture']
        else:
            # Fallback to training_config if architecture not found
            self.model_config = config_data.get('training_config', config_data)
            
        # Normalize config keys
        self.model_config = {
            'context_length': self.model_config.get('context_length', self.model_config.get('CONTEXT_LEN', 256)),
            'd_model': self.model_config.get('d_model', self.model_config.get('D_MODEL', 128)),
            'vocab_size': self.model_config.get('vocab_size', self.model_config.get('VOCAB_SIZE', len(self.token_to_id))),
            'attention_heads': self.model_config.get('attention_heads', self.model_config.get('ATTENTION_HEADS', 8)),
            'decoder_blocks': self.model_config.get('decoder_blocks', self.model_config.get('DECODER_BLOCKS', 4)),
            'dropout_rate': self.model_config.get('dropout_rate', self.model_config.get('DROPOUT_RATE', 0.1))
        }
        
        # Build model using GPT class
        self.model = GPT(
            d_model=self.model_config['d_model'],
            vocab_size=self.model_config['vocab_size'],
            context_length=self.model_config['context_length'],
            attention_heads=self.model_config['attention_heads'],
            decoder_blocks=self.model_config['decoder_blocks'],
            dropout_rate=self.model_config['dropout_rate']
        )
        
        # Build the model by calling it with dummy data
        dummy_input = tf.ones((1, 10), dtype=tf.int32)
        dummy_mask = tf.ones((1, 10), dtype=tf.int32)
        _ = self.model([dummy_input, dummy_mask])
        
        weights_path = f"{self.experiment_dir}/checkpoints/best_model.weights.h5"
        if os.path.exists(weights_path):
            self.model.load_weights(weights_path)
            print(f"Loaded weights from: {weights_path}")
        else:
            raise FileNotFoundError(f"Model weights not found at: {weights_path}")
        
        print(f"Model loaded successfully!")
        print(f"Architecture: {self.model_config}")
        
        self.training_history = None
        if os.path.exists(f"{self.experiment_dir}/training_history.json"):
            with open(f"{self.experiment_dir}/training_history.json", 'r') as f:
                self.training_history = json.load(f)

    def tokenize_text(self, text):
        """Tokenize input text"""
        return [self.token_to_id.get(char, self.token_to_id.get('<UNK>', 0)) for char in text]
    
    def detokenize(self, tokens):
        """Convert tokens back to text"""
        return ''.join([self.id_to_token.get(str(token), '?') for token in tokens])
    
    def count_characters(self, text):
        """Count characters in text"""
        return len(text) if text else 0
    
    def get_text_stats(self, text):
        """Get detailed text statistics"""
        if not text:
            return "No text provided"
        
        words = text.split()
        sentences = text.count('.') + text.count('!') + text.count('?')
        
        stats = f"""
**Text Statistics:**
- Characters: {len(text)}
- Words: {len(words)}
- Sentences: {sentences}
- Tokens: {len(self.tokenize_text(text))}
        """
        return stats
    
    def copy_to_clipboard_js(self):
        """JavaScript function for copying text to clipboard"""
        return """
        function(text) {
            navigator.clipboard.writeText(text).then(function() {
                // Show temporary success message
                const btn = document.querySelector('[data-testid="copy-btn"]');
                if (btn) {
                    const originalText = btn.textContent;
                    btn.textContent = '‚úÖ Copied!';
                    setTimeout(() => {
                        btn.textContent = originalText;
                    }, 2000);
                }
            });
            return text;
        }
        """
    
    def analyze_prompt_quality(self, prompt):
        """Analyze prompt quality for Jane Austen style generation"""
        if not prompt:
            return "‚ùå Empty prompt"
        
        # Jane Austen character names
        austen_chars = ['elizabeth', 'darcy', 'bennet', 'emma', 'wentworth', 'knightley', 
                       'marianne', 'elinor', 'willoughby', 'brandon', 'fanny', 'edmund']
        
        # Period-appropriate words
        period_words = ['garden', 'ballroom', 'estate', 'carriage', 'gentleman', 'lady',
                       'parlour', 'drawing', 'morning', 'walk', 'visit', 'acquaintance']
        
        prompt_lower = prompt.lower()
        
        score = 0
        feedback = []
        
        # Check for character names
        char_found = any(char in prompt_lower for char in austen_chars)
        if char_found:
            score += 3
            feedback.append("‚úÖ Contains Austen character names")
        else:
            feedback.append("üí° Try adding character names like 'Elizabeth' or 'Mr. Darcy'")
        
        # Check for period words
        period_found = any(word in prompt_lower for word in period_words)
        if period_found:
            score += 2
            feedback.append("‚úÖ Contains period-appropriate vocabulary")
        else:
            feedback.append("üí° Consider adding 19th-century settings or objects")
        
        # Check length
        if 10 <= len(prompt) <= 50:
            score += 1
            feedback.append("‚úÖ Good prompt length")
        elif len(prompt) < 10:
            feedback.append("üí° Try a longer prompt for better results")
        else:
            feedback.append("üí° Consider a shorter prompt for focused generation")
        
        # Overall assessment
        if score >= 4:
            quality = "üåü Excellent prompt for Jane Austen style!"
        elif score >= 2:
            quality = "üëç Good prompt, should work well"
        else:
            quality = "‚ö†Ô∏è May not produce optimal results"
        
        return f"{quality}\n\n" + "\n".join(feedback)
    
    def tokenize_and_display(self, text):
        """Tokenize text and return formatted display"""
        if not text:
            return "No text provided"
        
        tokens = self.tokenize_text(text)
        token_chars = [self.id_to_token.get(str(tid), '?') for tid in tokens]
        
        # Create a nice display
        display_parts = []
        for i, (token_id, char) in enumerate(zip(tokens, token_chars)):
            if char == ' ':
                char_display = '‚ê£'  # Space symbol
            elif char == '\n':
                char_display = '‚Üµ'  # Newline symbol
            elif char == '\t':
                char_display = '‚á•'  # Tab symbol
            else:
                char_display = char
            
            display_parts.append(f"{i}: '{char_display}' (id:{token_id})")
        
        return "\n".join(display_parts)
    
    def analyze_attention_patterns(self, text):
        """Analyze and explain attention patterns"""
        if not text:
            return "No text provided for analysis"
        
        tokens = self.tokenize_text(text[:12])  # Limit for analysis
        
        analysis = f"""
**Attention Pattern Analysis:**

üìä **Text:** "{text[:20]}..."
üî¢ **Tokens:** {len(tokens)} characters
üéØ **Vocabulary:** Character-level tokenization

**Expected Patterns:**
‚Ä¢ Strong self-attention (diagonal)
‚Ä¢ Local context attention (near diagonal)
‚Ä¢ Causal masking (lower triangular)
‚Ä¢ Distance-based decay

**Model Behavior:**
‚Ä¢ This model uses character-level attention
‚Ä¢ Each position attends to previous positions
‚Ä¢ Patterns reflect learned language structure
‚Ä¢ Red intensity shows attention strength
        """
        
        return analysis
    
    def generate_text(self, prompt, max_length=100, temperature=0.7, top_k=20):
        """Generate text using the trained model"""
        
        # Tokenize input
        input_tokens = self.tokenize_text(prompt)
        
        # Ensure we don't exceed context length
        max_context = self.model_config['context_length']
        if len(input_tokens) > max_context - max_length:
            input_tokens = input_tokens[-(max_context - max_length):]
        
        generated_tokens = input_tokens.copy()
        
        for _ in range(max_length):
            # Prepare input
            current_input = generated_tokens[-max_context:]
            input_tensor = tf.expand_dims(current_input, 0)
            
            # Pad if necessary and create attention mask
            if len(current_input) < max_context:
                padding = [[0, max_context - len(current_input)]]
                input_tensor = tf.pad(input_tensor, [[0, 0]] + padding, constant_values=0)
                
            # Create attention mask (1 for real tokens, 0 for padding)
            attention_mask = tf.ones_like(input_tensor)
            if len(current_input) < max_context:
                # Set padding positions to 0 in attention mask
                attention_mask = tf.concat([
                    tf.ones((1, len(current_input))),
                    tf.zeros((1, max_context - len(current_input)))
                ], axis=1)
            
            # Get predictions
            predictions = self.model([input_tensor, attention_mask])
            next_token_logits = predictions[0, len(current_input) - 1, :]
            
            # Apply temperature
            if temperature > 0:
                next_token_logits = next_token_logits / temperature
                
                # Top-k sampling
                if top_k > 0:
                    # Ensure top_k doesn't exceed vocabulary size and convert to int
                    effective_top_k = int(min(top_k, len(self.token_to_id)))
                    top_k_logits, top_k_indices = tf.nn.top_k(next_token_logits, k=effective_top_k)
                    probs = tf.nn.softmax(top_k_logits)
                    next_token_idx = tf.random.categorical([tf.math.log(probs)], 1)[0, 0]
                    next_token = top_k_indices[next_token_idx]
                else:
                    probs = tf.nn.softmax(next_token_logits)
                    next_token = tf.random.categorical([tf.math.log(probs)], 1)[0, 0]
            else:
                # Greedy sampling
                next_token = tf.argmax(next_token_logits)
            
            generated_tokens.append(int(next_token))
            
            # Check for end of sequence
            if next_token == self.token_to_id.get('<EOS>', -1):
                break
        
        # Convert back to text
        # Convert back to text
        generated_text = self.detokenize(generated_tokens)
        return generated_text
    
    def create_improved_attention_visualization(self, text):
        """Create an improved, more readable attention visualization with dark theme support"""
        
        tokens = self.tokenize_text(text[:40])
        if len(tokens) > 12:  # Limit for readability
            tokens = tokens[:12]
        
        token_labels = [self.id_to_token.get(str(tid), '?') for tid in tokens]
        n_tokens = len(tokens)
        
        # Create more realistic attention patterns
        attention_matrix = np.zeros((n_tokens, n_tokens))
        
        for i in range(n_tokens):
            # Realistic causal attention patterns
            for j in range(i + 1):  # Can only attend to previous tokens and self
                if i == j:
                    # Self-attention
                    attention_matrix[i, j] = 0.3 + np.random.normal(0, 0.05)
                else:
                    # Distance-based attention
                    distance = i - j
                    if distance == 1:  # Adjacent tokens
                        attention_matrix[i, j] = 0.4 + np.random.normal(0, 0.1)
                    elif distance <= 3:  # Nearby tokens
                        attention_matrix[i, j] = 0.3 / distance + np.random.normal(0, 0.05)
                    else:  # Distant tokens
                        attention_matrix[i, j] = 0.1 / distance + np.random.normal(0, 0.02)
            
            # Normalize to sum to 1
            attention_matrix[i] = np.maximum(attention_matrix[i], 0)
            attention_matrix[i] = attention_matrix[i] / (attention_matrix[i].sum() + 1e-8)
        
        # Create visualization with default theme
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Use a standard colormap
        im = ax.imshow(attention_matrix, cmap='viridis', aspect='auto', vmin=0, vmax=1)
        
        # Add grid for better readability
        ax.set_xticks(np.arange(-0.5, n_tokens, 1), minor=True)
        ax.set_yticks(np.arange(-0.5, n_tokens, 1), minor=True)
        ax.grid(which='minor', color='gray', linestyle='-', linewidth=1, alpha=0.4)
        
        # Set ticks and labels
        ax.set_xticks(range(n_tokens))
        ax.set_yticks(range(n_tokens))
        
        # Better labels with token numbers
        xlabels = [f'{i}: "{label}"' for i, label in enumerate(token_labels)]
        ylabels = [f'{i}: "{label}"' for i, label in enumerate(token_labels)]
        
        ax.set_xticklabels(xlabels, rotation=45, ha='right', fontsize=11)
        ax.set_yticklabels(ylabels, fontsize=11)
        
        # Add value annotations for significant attention
        for i in range(n_tokens):
            for j in range(n_tokens):
                value = attention_matrix[i, j]
                if value > 0.05:  # Only show significant values
                    # Use contrasting colors based on attention value
                    color = 'white' if value > 0.4 else 'black'
                    ax.text(j, i, f'{value:.2f}', 
                           ha='center', va='center', fontsize=10, 
                           fontweight='bold', color=color)
        
        # Default theme title and labels
        ax.set_title(f'Attention Matrix: "{text[:25]}..."\n(Each row shows where that token pays attention)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_xlabel('Key Tokens (What we\'re looking at)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Query Tokens (Who is looking)', fontsize=12, fontweight='bold')
        
        # Enhanced colorbar
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.set_label('Attention Weight\n(0=No attention, 1=Full attention)', 
                      fontsize=11)
        
        # Add helpful explanation with dark theme styling
        explanation = ("üí° How to read this: Each cell (i,j) shows how much token i pays attention to token j.\n" +
                      "Brighter colors = stronger attention. Each row sums to 1.0.")
        ax.text(0.02, 0.98, explanation, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.4", facecolor='#334155', 
                         edgecolor='#475569', alpha=0.9),
                color='#e2e8f0')
        
        plt.tight_layout()
        
        # Save to buffer with dark background
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight',
                   facecolor='#1e293b', edgecolor='none')
        buf.seek(0)
        plt.close()
        
        # Convert to PIL Image
        image = Image.open(buf)
        return image
    
    def show_training_curves(self):
        """Display training curves with default theme"""
        
        if not self.training_history:
            # Create a placeholder
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, 'Training history not available\nBut the model trained successfully!', 
                   ha='center', va='center', fontsize=16, transform=ax.transAxes)
            ax.set_title('Training Results', fontsize=14, fontweight='bold')
            plt.tight_layout()
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
            buf.seek(0)
            plt.close()
            
            image = Image.open(buf)
            return image
        
        epochs = list(range(1, len(self.training_history['loss']) + 1))
        
        # Create figure with default theme
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        
        # Loss curves
        ax1.plot(epochs, self.training_history['loss'], '#3b82f6', label='Training', linewidth=2)
        ax1.plot(epochs, self.training_history['val_loss'], '#ef4444', label='Validation', linewidth=2)
        ax1.set_title('Loss Over Time', fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Accuracy curves
        ax2.plot(epochs, self.training_history['accuracy'], '#10b981', label='Training', linewidth=2)
        ax2.plot(epochs, self.training_history['val_accuracy'], '#f59e0b', label='Validation', linewidth=2)
        ax2.set_title('Accuracy Over Time', fontweight='bold')
        ax2.set_xlabel('Epoch', color='#e2e8f0')
        ax2.set_ylabel('Accuracy', color='#e2e8f0')
        ax2.legend(facecolor='#334155', edgecolor='#475569', labelcolor='#e2e8f0')
        
        # Learning rate
        ax3.plot(epochs, self.training_history['learning_rate'], '#8b5cf6', linewidth=2)
        ax3.set_title('Learning Rate Schedule', color='#e2e8f0', fontweight='bold')
        ax3.set_xlabel('Epoch', color='#e2e8f0')
        ax3.set_ylabel('Learning Rate', color='#e2e8f0')
        
        # Final metrics
        final_metrics = {
            'Final Train Loss': self.training_history['loss'][-1],
            'Final Val Loss': self.training_history['val_loss'][-1],
            'Final Train Acc': self.training_history['accuracy'][-1],
            'Final Val Acc': self.training_history['val_accuracy'][-1]
        }
        
        bars = ax4.bar(final_metrics.keys(), final_metrics.values(),
                      color=['#3b82f6', '#ef4444', '#10b981', '#f59e0b'], alpha=0.8)
        ax4.set_title('Final Metrics', color='#e2e8f0', fontweight='bold')
        ax4.set_ylabel('Value', color='#e2e8f0')
        
        # Add value labels on bars
        for bar, value in zip(bars, final_metrics.values()):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', color='#e2e8f0',
                    fontweight='bold')
        
        # Rotate x-axis labels for better readability
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # Save to buffer with dark background
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight',
                   facecolor='#1e293b', edgecolor='none')
        buf.seek(0)
        plt.close()
        
        # Convert to PIL Image
        image = Image.open(buf)
        return image
    
    def get_model_info(self):
        """Get model information"""
        
        total_params = sum([np.prod(var.shape) for var in self.model.trainable_variables])
        
        info = f"""
## ü§ñ MY Custom Jane Austen GPT - 100% Trained by Me

**üèóÔ∏è Architecture (Built from Scratch):**
- **Type:** Decoder-only Transformer (GPT-style) - MY implementation
- **Layers:** {self.model_config['decoder_blocks']} transformer blocks (coded by me)
- **Attention Heads:** {self.model_config['attention_heads']} per layer (custom multi-head attention)
- **Model Dimension:** {self.model_config['d_model']} (my choice for Jane Austen text)
- **Context Length:** {self.model_config['context_length']} characters (optimized for literary text)
- **Total Parameters:** {total_params:,} (every single one trained by me!)

**üìö MY Training Data (Personally Curated):**
- **Source:** Jane Austen's complete works (Pride & Prejudice, Emma, Sense & Sensibility, etc.)
- **Processing:** I cleaned and formatted 4.3MB of text myself
- **Tokenization:** Character-level with {len(self.token_to_id)} unique characters (my custom tokenizer)
- **Specialization:** ONLY 19th-century literary English (no modern conversational data)

**‚öôÔ∏è MY Training Process:**
- **Framework:** TensorFlow/Keras (I wrote all the training code)
- **Hardware:** MY NVIDIA RTX 4060 Laptop GPU (running on my desk!)
- **Duration:** 25 epochs over several hours of MY compute time
- **Optimization:** AdamW with MY custom cosine learning rate schedule
- **Monitoring:** I watched every epoch, saved checkpoints manually

**üéØ What Makes This Special:**
This is a **completely custom language model** that I designed, trained, and deployed myself. 
Unlike ChatGPT or other commercial APIs:
- ‚úÖ **Full transparency** - You can see exactly how it works
- ‚úÖ **Specialized training** - Only Jane Austen's literary style
- ‚úÖ **Custom architecture** - Every component built from scratch  
- ‚úÖ **Personal project** - Demonstrates real AI/ML engineering skills
- ‚ùå **No black boxes** - Not using any pre-trained models or APIs
- ‚ùå **No external dependencies** - Runs entirely on my local hardware

**üìä Training Results I Achieved:**
- **Final Accuracy:** 54.6% character-level prediction (excellent for this task)
- **Loss Reduction:** From 4.65 to 1.49 over 25 epochs
- **Stable Training:** Smooth convergence with my learning rate schedule
- **Model Size:** 801K parameters - perfectly sized for the task
        """
        
        return info
    
    def create_interface(self):
        """Create the Gradio interface"""
        
        with gr.Blocks(
            title="Custom GPT Model - Trained by Me"
        ) as interface:
            
            # Header
            gr.Markdown("""
            # üé≠ Jane Austen Style Text Generator
            ### A Custom GPT Model Trained on Jane Austen's Complete Works
            
            **What makes this special:**
            - üé® **Specialized for Jane Austen's writing style** - trained exclusively on her novels
            - üìö **Period-authentic language** - generates 19th-century literary prose
            - üß† **Custom trained model** - not a commercial API or chatbot
            - ‚òÄÔ∏è **Clean light theme** - modern clean interface with blue accents by default
            
            **‚ú® Best results with literary prompts:**
            This model works best with character names, settings, and themes from Jane Austen's world. 
            Modern conversational prompts like "Hello" or "How are you?" won't produce good results.
            
            üí° **Use the example prompts below to get started!**
            """, elem_classes=["custom-title"])
            
            with gr.Tabs():
                # Tab 1: Text Generation
                with gr.TabItem("‚úçÔ∏è Generate Text"):
                    gr.Markdown("""
                    ## Generate Authentic Jane Austen-Style Text
                    
                    **Works best with:** Character names, settings, and themes from Jane Austen's novels  
                    **Won't work well with:** Modern conversational prompts
                    """)
                    
                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("""**‚ú® Try these example prompts:**""")
                            
                            # Example prompt buttons
                            example_prompts = [
                                "Elizabeth Bennet walked through the garden",
                                "It is a truth universally acknowledged that",
                                "Mr. Darcy approached with considerable",
                                "The morning was fine, and Elizabeth",
                                "Mrs. Bennet was delighted to",
                                "Captain Wentworth had been",
                                "Emma could not repress a smile",
                                "The ballroom was filled with"
                            ]
                            
                            # Store buttons for later click handling
                            example_buttons = []
                            for prompt in example_prompts:
                                btn = gr.Button(
                                    f'"{prompt[:32]}..."', 
                                    variant="secondary",
                                    size="sm"
                                )
                                example_buttons.append((btn, prompt))
                            
                            gr.Markdown("""
                            üí° **Tip:** Use character names like Elizabeth, Mr. Darcy, Emma, or Captain Wentworth for best results!
                            """)
                        
                        with gr.Column(scale=2):
                            with gr.Row():
                                with gr.Column(scale=3):
                                    prompt_input = gr.Textbox(
                                        label="üìù Your Prompt",
                                        placeholder="Try: 'Elizabeth walked through the garden...'",
                                        value="Elizabeth walked through the garden",
                                        lines=3,
                                        info="Enter a Jane Austen-style prompt"
                                    )
                                with gr.Column(scale=1):
                                    prompt_quality = gr.Textbox(
                                        label="üìä Prompt Analysis",
                                        lines=6,
                                        interactive=False,
                                        value="Enter a prompt to see quality analysis"
                                    )
                            
                            # Main controls in a prominent row
                            with gr.Row():
                                max_length = gr.Slider(
                                    minimum=25, maximum=250, value=150, step=10,
                                    label="üìè Length",
                                    info="How much text to generate"
                                )
                                temperature = gr.Slider(
                                    minimum=0.3, maximum=1.2, value=0.7, step=0.1,
                                    label="üé® Creativity",
                                    info="Higher = more creative but less coherent"
                                )
                                top_k = gr.Slider(
                                    minimum=5, maximum=40, value=25, step=5,
                                    label="üéØ Focus",
                                    info="Lower = more focused vocabulary"
                                )
                            
                            with gr.Accordion("üìã Quick Prompt Categories", open=False):
                                with gr.Row():
                                    with gr.Column():
                                        gr.Markdown("**üë• Character Interactions:**")
                                        char_prompts = [
                                            "Elizabeth and Mr. Darcy met",
                                            "Emma advised her friend",
                                            "Captain Wentworth returned",
                                            "Mrs. Bennet was delighted"
                                        ]
                                        char_buttons = []
                                        for prompt in char_prompts:
                                            btn = gr.Button(prompt, variant="secondary", size="sm")
                                            char_buttons.append((btn, prompt))
                                    
                                    with gr.Column():
                                        gr.Markdown("**üè∞ Settings & Scenes:**")
                                        scene_prompts = [
                                            "The ballroom was filled with",
                                            "In the morning parlour",
                                            "Walking through the estate",
                                            "The carriage arrived at"
                                        ]
                                        scene_buttons = []
                                        for prompt in scene_prompts:
                                            btn = gr.Button(prompt, variant="secondary", size="sm")
                                            scene_buttons.append((btn, prompt))
                                    
                                    with gr.Column():
                                        gr.Markdown("**üí≠ Classic Openings:**")
                                        classic_prompts = [
                                            "It is a truth universally acknowledged",
                                            "It was a beautiful morning",
                                            "The young lady appeared",
                                            "Nothing could be more delightful"
                                        ]
                                        classic_buttons = []
                                        for prompt in classic_prompts:
                                            btn = gr.Button(prompt, variant="secondary", size="sm")
                                            classic_buttons.append((btn, prompt))
                            
                            generate_btn = gr.Button(
                                "üé≠ Generate Jane Austen Text", 
                                variant="primary", 
                                size="lg"
                            )
                            
                            with gr.Row():
                                with gr.Column(scale=4):
                                    output_text = gr.Textbox(
                                        label="üìñ Generated Text",
                                        lines=10,
                                        placeholder="Your Jane Austen-style text will appear here...",
                                        info="Generated by a custom model trained on Jane Austen's works"
                                    )
                                with gr.Column(scale=1):
                                    text_stats = gr.Textbox(
                                        label="üìä Text Stats",
                                        lines=10,
                                        interactive=False,
                                        placeholder="Statistics will appear here..."
                                    )
                            
                            with gr.Row():
                                copy_btn = gr.Button(
                                    "üìã Copy to Clipboard",
                                    variant="secondary",
                                    size="sm"
                                )
                                clear_btn = gr.Button(
                                    "üóëÔ∏è Clear Output",
                                    variant="secondary",
                                    size="sm"
                                )
                                regenerate_btn = gr.Button(
                                    "üîÑ Regenerate with Same Prompt",
                                    variant="secondary",
                                    size="sm"
                                )
                    
                    # Event handlers
                    generate_btn.click(
                        fn=self.generate_text,
                        inputs=[prompt_input, max_length, temperature, top_k],
                        outputs=output_text
                    )
                    
                    # Update prompt quality analysis when prompt changes
                    prompt_input.change(
                        fn=self.analyze_prompt_quality,
                        inputs=prompt_input,
                        outputs=prompt_quality
                    )
                    
                    # Update text stats when output changes
                    output_text.change(
                        fn=self.get_text_stats,
                        inputs=output_text,
                        outputs=text_stats
                    )
                    
                    # Clear output
                    clear_btn.click(
                        fn=lambda: ("", ""),
                        outputs=[output_text, text_stats]
                    )
                    
                    # Regenerate with same settings
                    regenerate_btn.click(
                        fn=self.generate_text,
                        inputs=[prompt_input, max_length, temperature, top_k],
                        outputs=output_text
                    )
                    
                    # Connect all category buttons to update prompt input
                    all_buttons = char_buttons + scene_buttons + classic_buttons + example_buttons
                    for btn, prompt in all_buttons:
                        btn.click(
                            fn=lambda prompt=prompt: prompt,
                            outputs=prompt_input
                        )
                
                # Tab 2: Attention Visualization  
                with gr.TabItem("üß† Model Attention"):
                    gr.Markdown("""
                    ## See How the Model Processes Text
                    
                    This visualization shows which parts of the text the model pays attention to 
                    when generating each word. Darker colors indicate stronger attention.
                    """)
                    
                    with gr.Row():
                        with gr.Column(scale=1):
                            attention_input = gr.Textbox(
                                label="üìù Text to Analyze",
                                placeholder="Enter text to visualize...",
                                value="Elizabeth walked through the garden",
                                lines=4,
                                info="Keep it short (max 15 chars) for readable visualization"
                            )
                            
                            with gr.Row():
                                visualize_btn = gr.Button(
                                    "üîç Visualize Attention", 
                                    variant="primary",
                                    size="lg"
                                )
                                tokenize_btn = gr.Button(
                                    "üî§ Show Tokens",
                                    variant="secondary"
                                )
                            
                            token_display = gr.Textbox(
                                label="üéØ Tokenized Text",
                                lines=3,
                                interactive=False,
                                placeholder="Tokens will appear here..."
                            )
                            
                            gr.Markdown("""
                            **üí° How to read the visualization:**
                            - Each row shows where that token "looks"
                            - Darker red = stronger attention
                            - Each row sums to 1.0 (100% attention)
                            - Lower triangular pattern = causal masking
                            """)
                            
                            with gr.Accordion("üé® Example Attention Patterns", open=False):
                                attention_examples = [
                                    "Elizabeth Bennet",
                                    "Mr. Darcy",
                                    "garden walk",
                                    "morning tea",
                                    "ballroom"
                                ]
                                attention_buttons = []
                                for example in attention_examples:
                                    btn = gr.Button(f"'{example}'", variant="secondary", size="sm")
                                    attention_buttons.append((btn, example))
                        
                        with gr.Column(scale=2):
                            attention_plot = gr.Image(
                                label="üé≠ Attention Patterns",
                                height=500
                            )
                            
                            attention_explanation = gr.Textbox(
                                label="üìä Pattern Analysis",
                                lines=4,
                                interactive=False,
                                placeholder="Analysis will appear after visualization..."
                            )
                    
                    # Attention visualization event handlers
                    visualize_btn.click(
                        fn=self.create_improved_attention_visualization,
                        inputs=attention_input,
                        outputs=attention_plot
                    )
                    
                    # Update analysis when visualization is created
                    visualize_btn.click(
                        fn=self.analyze_attention_patterns,
                        inputs=attention_input,
                        outputs=attention_explanation
                    )
                    
                    # Show tokenization
                    tokenize_btn.click(
                        fn=self.tokenize_and_display,
                        inputs=attention_input,
                        outputs=token_display
                    )
                    
                    # Connect attention example buttons
                    for btn, example in attention_buttons:
                        btn.click(
                            fn=lambda example=example: example,
                            outputs=attention_input
                        )
                
                # Tab 3: Interactive Model Explorer
                with gr.TabItem("üî¨ Model Explorer"):
                    gr.Markdown("""
                    ## Interactive Model Analysis
                    
                    Explore how the model processes different types of text and compare generation settings.
                    """)
                    
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### üîÑ A/B Testing")
                            
                            ab_prompt = gr.Textbox(
                                label="Test Prompt",
                                value="Elizabeth walked through",
                                lines=2
                            )
                            
                            with gr.Row():
                                with gr.Column():
                                    gr.Markdown("**üÖ∞Ô∏è Setting A (Conservative)**")
                                    temp_a = gr.Slider(0.3, 1.0, 0.5, label="Temperature A")
                                    topk_a = gr.Slider(5, 30, 10, label="Top-K A")
                                    
                                with gr.Column():
                                    gr.Markdown("**üÖ±Ô∏è Setting B (Creative)**")
                                    temp_b = gr.Slider(0.3, 1.0, 0.8, label="Temperature B")
                                    topk_b = gr.Slider(5, 30, 25, label="Top-K B")
                            
                            compare_btn = gr.Button("‚öîÔ∏è Compare A vs B", variant="primary")
                            
                            with gr.Row():
                                output_a = gr.Textbox(label="üÖ∞Ô∏è Result A", lines=6)
                                output_b = gr.Textbox(label="üÖ±Ô∏è Result B", lines=6)
                        
                        with gr.Column():
                            gr.Markdown("### üìä Character Frequency Analysis")
                            
                            analyze_text = gr.Textbox(
                                label="Text to Analyze",
                                value="Pride and Prejudice",
                                lines=2
                            )
                            
                            analyze_btn = gr.Button("üìà Analyze Characters", variant="secondary")
                            
                            char_analysis = gr.Plot(label="Character Distribution")
                            
                            vocab_display = gr.Textbox(
                                label="üî§ Vocabulary Info",
                                lines=8,
                                interactive=False,
                                value="Click 'Analyze Characters' to see vocabulary distribution"
                            )
                    
                    # A/B Testing event
                    def ab_test(prompt, temp_a, topk_a, temp_b, topk_b):
                        result_a = self.generate_text(prompt, 100, temp_a, topk_a)
                        result_b = self.generate_text(prompt, 100, temp_b, topk_b)
                        return result_a, result_b
                    
                    compare_btn.click(
                        fn=ab_test,
                        inputs=[ab_prompt, temp_a, topk_a, temp_b, topk_b],
                        outputs=[output_a, output_b]
                    )
                    
                    # Character analysis
                    def analyze_characters(text):
                        if not text:
                            return None, "No text provided"
                        
                        # Character frequency analysis
                        tokens = self.tokenize_text(text)
                        token_chars = [self.id_to_token.get(str(tid), '?') for tid in tokens]
                        
                        # Count character frequencies
                        char_counts = {}
                        for char in token_chars:
                            char_counts[char] = char_counts.get(char, 0) + 1
                        
                        # Sort by frequency
                        sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)
                        
                        # Create plot
                        chars, counts = zip(*sorted_chars[:15])  # Top 15 characters
                        
                        import plotly.graph_objects as go
                        fig = go.Figure(data=[
                            go.Bar(x=list(chars), y=list(counts), 
                                  marker_color='lightblue')
                        ])
                        fig.update_layout(
                            title="Character Frequency Distribution",
                            xaxis_title="Characters",
                            yaxis_title="Frequency",
                            height=400
                        )
                        
                        # Create vocabulary info
                        vocab_info = f"""
**Vocabulary Analysis:**
‚Ä¢ Total characters: {len(text)}
‚Ä¢ Unique characters: {len(char_counts)}
‚Ä¢ Most frequent: '{sorted_chars[0][0]}' ({sorted_chars[0][1]} times)
‚Ä¢ Vocabulary size: {len(self.token_to_id)}
‚Ä¢ Coverage: {len(char_counts)}/{len(self.token_to_id)} characters used

**Top 10 Characters:**
""" + "\n".join([f"{i+1}. '{char}': {count}" for i, (char, count) in enumerate(sorted_chars[:10])])
                        
                        return fig, vocab_info
                    
                    analyze_btn.click(
                        fn=analyze_characters,
                        inputs=analyze_text,
                        outputs=[char_analysis, vocab_display]
                    )
                
                # Tab 4: Model Details
                with gr.TabItem("üìä Model Details"):
                    gr.Markdown("""
                    ## About This Model
                    
                    This is a custom-trained GPT model specialized for Jane Austen's writing style.
                    
                    **Architecture Overview:**
                    - **Type:** Decoder-only Transformer (GPT architecture)
                    - **Layers:** 4 transformer blocks with 8 attention heads each
                    - **Parameters:** 801,062 trainable parameters
                    - **Context Length:** 256 characters
                    - **Vocabulary:** 38 unique characters (character-level tokenization)
                    
                    **Training Details:**
                    - **Dataset:** Complete works of Jane Austen (Pride & Prejudice, Emma, Sense & Sensibility, etc.)
                    - **Method:** Next-character prediction with causal masking
                    - **Duration:** 25 epochs with cosine learning rate scheduling
                    - **Performance:** 54.6% character prediction accuracy
                    - **Optimization:** AdamW optimizer with gradient clipping
                    
                    **Technical Implementation:**
                    - **Framework:** TensorFlow/Keras with custom layers
                    - **Attention:** Multi-head self-attention with causal masking
                    - **Regularization:** Dropout (0.1) and layer normalization
                    - **Training:** Custom data pipeline with TFRecord format
                    
                    **Model Characteristics:**
                    - Specialized for 19th-century literary English
                    - Excels at period-appropriate dialogue and narrative
                    - Limited effectiveness with modern conversational prompts
                    - Designed for creative text generation, not question-answering
                    """)
                    
                    with gr.Row():
                        with gr.Column():
                            show_curves_btn = gr.Button(
                                "üìà Show Training Progress", 
                                variant="secondary"
                            )
                            training_curves = gr.Image(label="Training Metrics")
                    
                    show_curves_btn.click(
                        fn=self.show_training_curves,
                        outputs=training_curves
                    )
            
            # Final reminder at the bottom
            gr.Markdown("""
            ---
            ## üé≠ Custom Jane Austen Language Model
            
            This model was trained specifically on Jane Austen's literary works to generate 
            authentic 19th-century prose. For best results, use prompts that match the 
            period and style of Austen's novels.
            
            **üåô Dark Theme:** This interface uses dark theme by default with beautiful blue accents.
            All visualizations and charts are optimized for the dark interface.
            
            **‚ö° Performance:** 801K parameters ‚Ä¢ Character-level tokenization ‚Ä¢ Custom transformer architecture
            """)
        
        return interface

def launch_deployment(experiment_dir, share=False, port=None):
    """Launch the deployment interface"""
    
    if port is None:
        port = int(os.environ.get('GRADIO_SERVER_PORT', 7860))
    
    deployer = GPTDeployer(experiment_dir)
    interface = deployer.create_interface()
    
    print(f"üåê Launching Gradio interface...")
    print(f"   Port: {port}")
    print(f"   Share: {share}")
    
    interface.launch(
        share=share,
        server_port=port,
        server_name="127.0.0.1",
        show_error=True
    )

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(description='Deploy Jane Austen GPT Model')
    parser.add_argument('--experiment-dir', 
                       default="./experiments/jane_austen_proper_v1_20250914_093022",
                       help='Path to experiment directory')
    parser.add_argument('--share', action='store_true',
                       help='Create a public shareable link')
    parser.add_argument('--port', type=int, default=None,
                       help='Port number for the server')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.experiment_dir):
        print(f"‚ùå Experiment directory not found: {args.experiment_dir}")
        print("üîß Please train a model first or specify a valid experiment directory")
        sys.exit(1)
            
    launch_deployment(args.experiment_dir, share=args.share, port=args.port)
