{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76a7bc1",
   "metadata": {},
   "source": [
    "# Data Preprocessing Experiments - Messy Version\n",
    "\n",
    "Trying different ways to process text data for GPT training.\n",
    "This is all experimental code, lots of dead ends and quick hacks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy imports again\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Hardcoded paths for quick testing\n",
    "PROJECT_ROOT = '/home/akshat/GPT_from_scratch'\n",
    "TEXT_DIR = f'{PROJECT_ROOT}/text_data'\n",
    "print(f\"Working from {PROJECT_ROOT}\")\n",
    "\n",
    "# Check what text files we have\n",
    "text_files = [f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')]\n",
    "print(f\"Available text files: {text_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d57a02",
   "metadata": {},
   "source": [
    "## Loading and Basic Text Analysis\n",
    "\n",
    "Let me load different text files and see what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a34f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all available text files and analyze them\n",
    "texts = {}\n",
    "stats = {}\n",
    "\n",
    "for filename in text_files:\n",
    "    try:\n",
    "        with open(f'{TEXT_DIR}/{filename}', 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        texts[filename] = content\n",
    "        \n",
    "        # Quick stats\n",
    "        stats[filename] = {\n",
    "            'chars': len(content),\n",
    "            'words': len(content.split()),\n",
    "            'lines': len(content.split('\\n')),\n",
    "            'unique_chars': len(set(content)),\n",
    "            'unique_words': len(set(content.lower().split()))\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úì Loaded {filename}: {stats[filename]['chars']:,} chars, {stats[filename]['words']:,} words\")\n",
    "        print(f\"  Preview: {content[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(texts)} text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me combine some texts for larger training data\n",
    "# Using Alice as base since it's clean\n",
    "if 'alice_story.txt' in texts:\n",
    "    main_text = texts['alice_story.txt']\n",
    "elif 'alice_extended.txt' in texts:\n",
    "    main_text = texts['alice_extended.txt']\n",
    "else:\n",
    "    # Just use the first available text\n",
    "    main_text = list(texts.values())[0]\n",
    "    \n",
    "print(f\"Using main text with {len(main_text):,} characters\")\n",
    "print(f\"Sample: {main_text[:200]}\")\n",
    "\n",
    "# Character analysis\n",
    "char_counts = Counter(main_text)\n",
    "print(f\"\\nTop 20 characters:\")\n",
    "for char, count in char_counts.most_common(20):\n",
    "    char_display = repr(char) if char in '\\n\\t\\r' else char\n",
    "    print(f\"  '{char_display}': {count:,} ({count/len(main_text)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad89e50",
   "metadata": {},
   "source": [
    "## Tokenizer Experiments\n",
    "\n",
    "Trying different tokenization strategies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd27a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenizer with special tokens\n",
    "def build_char_tokenizer_v1(text):\n",
    "    \"\"\"Simple character tokenizer\"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "    vocab = special_tokens + chars\n",
    "    \n",
    "    char_to_id = {ch: i for i, ch in enumerate(vocab)}\n",
    "    id_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "    \n",
    "    return char_to_id, id_to_char, vocab\n",
    "\n",
    "char_to_id_v1, id_to_char_v1, vocab_v1 = build_char_tokenizer_v1(main_text)\n",
    "print(f\"Character tokenizer v1: {len(vocab_v1)} tokens\")\n",
    "print(f\"Vocab: {vocab_v1[:20]}...\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"Hello Alice!\"\n",
    "encoded = [char_to_id_v1.get(ch, char_to_id_v1['<UNK>']) for ch in test_text]\n",
    "decoded = ''.join([id_to_char_v1[idx] for idx in encoded])\n",
    "print(f\"\\nTest: '{test_text}' -> {encoded} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenizer with frequency filtering\n",
    "def build_word_tokenizer_v2(text, min_freq=2, max_vocab=5000):\n",
    "    \"\"\"Word tokenizer with frequency filtering\"\"\"\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_counts = Counter(words)\n",
    "    print(f\"Total unique words: {len(word_counts)}\")\n",
    "    \n",
    "    # Filter by frequency\n",
    "    frequent_words = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    frequent_words = sorted(frequent_words)[:max_vocab-4]  # Leave room for special tokens\n",
    "    \n",
    "    print(f\"Words with freq >= {min_freq}: {len(frequent_words)}\")\n",
    "    \n",
    "    # Build vocabulary\n",
    "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "    vocab = special_tokens + frequent_words\n",
    "    \n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    \n",
    "    return word_to_id, id_to_word, vocab\n",
    "\n",
    "word_to_id_v2, id_to_word_v2, vocab_v2 = build_word_tokenizer_v2(main_text, min_freq=2)\n",
    "print(f\"\\nWord tokenizer v2: {len(vocab_v2)} tokens\")\n",
    "print(f\"Sample vocab: {vocab_v2[4:24]}...\")  # Skip special tokens\n",
    "\n",
    "# Test word tokenization\n",
    "test_words = \"hello alice how are you today?\".split()\n",
    "encoded_words = [word_to_id_v2.get(word, word_to_id_v2['<UNK>']) for word in test_words]\n",
    "decoded_words = [id_to_word_v2[idx] for idx in encoded_words]\n",
    "print(f\"\\nWord test: {test_words}\")\n",
    "print(f\"Encoded: {encoded_words}\")\n",
    "print(f\"Decoded: {decoded_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subword tokenizer experiment (BPE-like)\n",
    "def simple_bpe_tokenizer(text, vocab_size=1000, iterations=500):\n",
    "    \"\"\"Very simple BPE implementation - just for experimentation\"\"\"\n",
    "    print(f\"Building BPE tokenizer with {vocab_size} vocab size...\")\n",
    "    \n",
    "    # Start with character-level\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab = ['<PAD>', '<UNK>'] + chars\n",
    "    \n",
    "    # Tokenize text as characters initially\n",
    "    tokens = list(text)\n",
    "    \n",
    "    print(f\"Starting with {len(vocab)} character tokens\")\n",
    "    \n",
    "    # Simple BPE iterations\n",
    "    for iteration in range(min(iterations, vocab_size - len(vocab))):\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"  BPE iteration {iteration}, vocab size: {len(vocab)}\")\n",
    "            \n",
    "        # Count adjacent pairs\n",
    "        pairs = defaultdict(int)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pairs[pair] += 1\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "            \n",
    "        # Find most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        if pairs[best_pair] < 2:  # Stop if no pair appears more than once\n",
    "            break\n",
    "            \n",
    "        # Merge the pair\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "        vocab.append(new_token)\n",
    "        \n",
    "        # Replace in tokens\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens = new_tokens\n",
    "    \n",
    "    print(f\"Final vocab size: {len(vocab)}\")\n",
    "    \n",
    "    # Build mappings\n",
    "    token_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "    id_to_token = {i: token for i, token in enumerate(vocab)}\n",
    "    \n",
    "    return token_to_id, id_to_token, vocab\n",
    "\n",
    "# Test BPE on a smaller text sample\n",
    "sample_text = main_text[:2000]  # Use first 2000 chars for speed\n",
    "bpe_to_id, id_to_bpe, bpe_vocab = simple_bpe_tokenizer(sample_text, vocab_size=200, iterations=100)\n",
    "\n",
    "print(f\"\\nBPE vocab sample: {bpe_vocab[-10:]}\")\n",
    "print(f\"Some learned subwords: {[tok for tok in bpe_vocab if len(tok) > 1][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c97a47",
   "metadata": {},
   "source": [
    "## TFRecord Creation Experiments\n",
    "\n",
    "Trying different ways to create training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick TFRecord creation function\n",
    "def create_tfrecord_v1(text, tokenizer_dict, seq_len=64, output_file='test.tfrecord'):\n",
    "    \"\"\"Create TFRecord file - messy version\"\"\"\n",
    "    print(f\"Creating TFRecord with seq_len={seq_len}\")\n",
    "    \n",
    "    # Tokenize text\n",
    "    if isinstance(list(tokenizer_dict.keys())[0], str) and len(list(tokenizer_dict.keys())[0]) == 1:\n",
    "        # Character tokenizer\n",
    "        tokens = [tokenizer_dict.get(ch, tokenizer_dict.get('<UNK>', 0)) for ch in text]\n",
    "    else:\n",
    "        # Word tokenizer - need to split first\n",
    "        words = text.lower().split()\n",
    "        tokens = [tokenizer_dict.get(word, tokenizer_dict.get('<UNK>', 0)) for word in words]\n",
    "    \n",
    "    print(f\"Tokenized {len(text)} chars into {len(tokens)} tokens\")\n",
    "    \n",
    "    # Create examples\n",
    "    examples = []\n",
    "    for i in range(0, len(tokens) - seq_len, seq_len // 2):  # Overlapping windows\n",
    "        input_seq = tokens[i:i+seq_len]\n",
    "        target_seq = tokens[i+1:i+seq_len+1]\n",
    "        \n",
    "        if len(input_seq) == seq_len and len(target_seq) == seq_len:\n",
    "            examples.append((input_seq, target_seq))\n",
    "    \n",
    "    print(f\"Created {len(examples)} training examples\")\n",
    "    \n",
    "    # Write TFRecord\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for inputs, targets in tqdm(examples, desc=\"Writing TFRecord\"):\n",
    "            # Create features\n",
    "            feature = {\n",
    "                'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inputs)),\n",
    "                'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=targets)),\n",
    "                'length': tf.train.Feature(int64_list=tf.train.Int64List(value=[seq_len]))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "    \n",
    "    print(f\"‚úì Saved {len(examples)} examples to {output_file}\")\n",
    "    return len(examples)\n",
    "\n",
    "# Test with character tokenizer\n",
    "output_dir = f\"{PROJECT_ROOT}/notebooks/tfrecords_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "char_file = f\"{output_dir}/char_test.tfrecord\"\n",
    "char_examples = create_tfrecord_v1(main_text[:5000], char_to_id_v1, seq_len=32, output_file=char_file)\n",
    "\n",
    "# Test with word tokenizer  \n",
    "word_file = f\"{output_dir}/word_test.tfrecord\"\n",
    "word_examples = create_tfrecord_v1(main_text[:5000], word_to_id_v2, seq_len=16, output_file=word_file)\n",
    "\n",
    "print(f\"\\nCreated TFRecord files:\")\n",
    "print(f\"  Character: {char_examples} examples\")\n",
    "print(f\"  Word: {word_examples} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading the TFRecord files\n",
    "def read_tfrecord_test(filename, num_examples=3):\n",
    "    \"\"\"Quick test to read TFRecord\"\"\"\n",
    "    print(f\"\\nReading {filename}...\")\n",
    "    \n",
    "    # Define feature description\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "        'target_ids': tf.io.FixedLenFeature([], tf.string),\n",
    "        'length': tf.io.FixedLenFeature([1], tf.int64)\n",
    "    }\n",
    "    \n",
    "    # Actually, let me fix this - I saved as int64_list but reading as string\n",
    "    def _parse_function(proto):\n",
    "        return tf.io.parse_single_example(proto, {\n",
    "            'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "            'target_ids': tf.io.VarLenFeature(tf.int64),\n",
    "            'length': tf.io.FixedLenFeature([1], tf.int64)\n",
    "        })\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    \n",
    "    # Show first few examples\n",
    "    for i, example in enumerate(dataset.take(num_examples)):\n",
    "        inputs = tf.sparse.to_dense(example['input_ids']).numpy()\n",
    "        targets = tf.sparse.to_dense(example['target_ids']).numpy()\n",
    "        length = example['length'].numpy()[0]\n",
    "        \n",
    "        print(f\"  Example {i+1}: length={length}\")\n",
    "        print(f\"    Inputs:  {inputs[:10]}...\")\n",
    "        print(f\"    Targets: {targets[:10]}...\")\n",
    "\n",
    "# Test reading\n",
    "read_tfrecord_test(char_file)\n",
    "read_tfrecord_test(word_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb7323",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization\n",
    "\n",
    "Let me analyze the different tokenization approaches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30672991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization efficiency\n",
    "sample_text = main_text[:1000]\n",
    "print(f\"Analyzing sample text ({len(sample_text)} chars)...\")\n",
    "\n",
    "tokenization_results = {}\n",
    "\n",
    "# Character tokenization\n",
    "char_tokens = [char_to_id_v1.get(ch, char_to_id_v1['<UNK>']) for ch in sample_text]\n",
    "tokenization_results['Character'] = {\n",
    "    'tokens': len(char_tokens),\n",
    "    'vocab_size': len(vocab_v1),\n",
    "    'compression_ratio': len(sample_text) / len(char_tokens),\n",
    "    'avg_token_length': 1.0\n",
    "}\n",
    "\n",
    "# Word tokenization\n",
    "sample_words = sample_text.lower().split()\n",
    "word_tokens = [word_to_id_v2.get(word, word_to_id_v2['<UNK>']) for word in sample_words]\n",
    "tokenization_results['Word'] = {\n",
    "    'tokens': len(word_tokens),\n",
    "    'vocab_size': len(vocab_v2),\n",
    "    'compression_ratio': len(sample_text) / len(word_tokens),\n",
    "    'avg_token_length': len(sample_text) / len(word_tokens)\n",
    "}\n",
    "\n",
    "# BPE tokenization (approximate)\n",
    "bpe_approx_tokens = len(sample_text) // 3  # Rough estimate\n",
    "tokenization_results['BPE (approx)'] = {\n",
    "    'tokens': bpe_approx_tokens,\n",
    "    'vocab_size': len(bpe_vocab),\n",
    "    'compression_ratio': len(sample_text) / bpe_approx_tokens,\n",
    "    'avg_token_length': 3.0  # Rough estimate\n",
    "}\n",
    "\n",
    "print(\"\\nTokenization Comparison:\")\n",
    "print(f\"{'Method':<15} {'Tokens':<8} {'Vocab':<8} {'Compression':<12} {'Avg Len':<8}\")\n",
    "print(\"-\" * 55)\n",
    "for method, results in tokenization_results.items():\n",
    "    print(f\"{method:<15} {results['tokens']:<8} {results['vocab_size']:<8} {results['compression_ratio']:<12.2f} {results['avg_token_length']:<8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Token count comparison\n",
    "methods = list(tokenization_results.keys())\n",
    "token_counts = [tokenization_results[m]['tokens'] for m in methods]\n",
    "vocab_sizes = [tokenization_results[m]['vocab_size'] for m in methods]\n",
    "\n",
    "axes[0, 0].bar(methods, token_counts, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Number of Tokens (1000 chars)')\n",
    "axes[0, 0].set_ylabel('Token Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Vocabulary size comparison\n",
    "axes[0, 1].bar(methods, vocab_sizes, alpha=0.7, color='lightcoral')\n",
    "axes[0, 1].set_title('Vocabulary Size')\n",
    "axes[0, 1].set_ylabel('Vocab Size')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Character frequency in original text\n",
    "char_freqs = char_counts.most_common(20)\n",
    "chars, freqs = zip(*char_freqs)\n",
    "char_labels = [repr(c) if c in '\\n\\t\\r ' else c for c in chars]\n",
    "\n",
    "axes[1, 0].bar(range(len(chars)), freqs, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Top 20 Character Frequencies')\n",
    "axes[1, 0].set_xlabel('Character')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_xticks(range(len(chars)))\n",
    "axes[1, 0].set_xticklabels(char_labels, rotation=45)\n",
    "\n",
    "# Compression ratio comparison\n",
    "compression_ratios = [tokenization_results[m]['compression_ratio'] for m in methods]\n",
    "axes[1, 1].bar(methods, compression_ratios, alpha=0.7, color='gold')\n",
    "axes[1, 1].set_title('Compression Ratio (chars/token)')\n",
    "axes[1, 1].set_ylabel('Ratio')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä Data Processing Analysis Summary:\")\n",
    "print(f\"- Original text: {len(main_text):,} characters\")\n",
    "print(f\"- Character vocab: {len(vocab_v1)} tokens\")\n",
    "print(f\"- Word vocab: {len(vocab_v2)} tokens\") \n",
    "print(f\"- BPE vocab: {len(bpe_vocab)} tokens\")\n",
    "print(f\"- Character examples created: {char_examples}\")\n",
    "print(f\"- Word examples created: {word_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizers for later use (quick and dirty)\n",
    "tokenizer_dir = f\"{PROJECT_ROOT}/notebooks/tokenizer_experiments\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Save character tokenizer\n",
    "with open(f\"{tokenizer_dir}/char_tokenizer_v1.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'char_to_id': char_to_id_v1,\n",
    "        'id_to_char': id_to_char_v1,\n",
    "        'vocab': vocab_v1,\n",
    "        'type': 'character'\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save word tokenizer\n",
    "with open(f\"{tokenizer_dir}/word_tokenizer_v2.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'word_to_id': word_to_id_v2,\n",
    "        'id_to_word': id_to_word_v2,\n",
    "        'vocab': vocab_v2,\n",
    "        'type': 'word'\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save BPE tokenizer\n",
    "with open(f\"{tokenizer_dir}/bpe_tokenizer_simple.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'token_to_id': bpe_to_id,\n",
    "        'id_to_token': id_to_bpe,\n",
    "        'vocab': bpe_vocab,\n",
    "        'type': 'bpe'\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved tokenizers to {tokenizer_dir}\")\n",
    "\n",
    "# Quick experiment notes\n",
    "experiment_notes = f\"\"\"\n",
    "Data Preprocessing Experiments - {datetime.now()}\n",
    "============================================\n",
    "\n",
    "Text Sources:\n",
    "{', '.join(texts.keys())}\n",
    "\n",
    "Tokenization Results:\n",
    "- Character: {len(vocab_v1)} vocab, {char_examples} examples\n",
    "- Word: {len(vocab_v2)} vocab, {word_examples} examples  \n",
    "- BPE: {len(bpe_vocab)} vocab (experimental)\n",
    "\n",
    "Best Approach:\n",
    "Character tokenization seems most stable for this dataset size.\n",
    "Word tokenization has large vocab but good compression.\n",
    "BPE needs more work but shows promise.\n",
    "\n",
    "Next Steps:\n",
    "- Implement proper BPE\n",
    "- Try SentencePiece\n",
    "- Compare model performance\n",
    "- Scale up to larger datasets\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{tokenizer_dir}/experiment_notes.txt\", 'w') as f:\n",
    "    f.write(experiment_notes)\n",
    "\n",
    "print(\"\\nüéâ Data preprocessing experiments complete!\")\n",
    "print(\"Ready to clean up into proper modules...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
