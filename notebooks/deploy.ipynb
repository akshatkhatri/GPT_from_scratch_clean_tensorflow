{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2184491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e83566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_vocabulary_tf(file_path_list: List[str], existing_vocab: Dict[str, int] | None = None) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a character-level vocabulary dictionary from text files.\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths containing the text corpus.\n",
    "        existing_vocab: Optional existing vocabulary to extend.\n",
    "\n",
    "    Returns:\n",
    "        token_to_id: dict mapping character to unique integer token ID.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list] # type: ignore\n",
    "    if existing_vocab is None:\n",
    "        existing_vocab = {}\n",
    "    vocab_set = set(existing_vocab.keys())\n",
    "    \n",
    "    for file_name in file_path_list:\n",
    "        if os.path.isdir(file_name):\n",
    "            raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "        with open(file_name, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            vocab_set.update(text)\n",
    "    \n",
    "    sorted_tokens = sorted(vocab_set)\n",
    "    token_to_id = {char: idx for idx, char in enumerate(sorted_tokens)}\n",
    "    return token_to_id\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id(token_to_id_dict: Dict[str, int], text_batch: List[str], max_seq_len: int, pad_value: int = 0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize a batch of text strings into character token IDs using a token dictionary,\n",
    "    then pad/truncate to max_seq_len and create attention masks.\n",
    "\n",
    "    Args:\n",
    "        token_to_id_dict: dict mapping character to integer token ID.\n",
    "        text_batch: list of text strings to tokenize.\n",
    "        max_seq_len: maximum sequence length after padding/truncation.\n",
    "        pad_value: integer ID used for padding tokens.\n",
    "\n",
    "    Returns:\n",
    "        token_ids: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "        attention_mask: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32 (1 for real tokens, 0 for padding).\n",
    "    \"\"\"\n",
    "    batch_token_ids = []\n",
    "    for text in text_batch:\n",
    "        ids = [token_to_id_dict.get(c, pad_value) for c in text]\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        batch_token_ids.append(ids)\n",
    "    \n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    \n",
    "    return tf.constant(token_ids), tf.constant(attention_mask) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0806ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def train_sentencepiece_tokenizer(file_path_list: List[str], \n",
    "                                vocab_size: int = 2000,\n",
    "                                model_prefix: str = 'spm_gpt') -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizer from text files (replaces tokenize_and_build_vocabulary_tf).\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths containing the text corpus.\n",
    "        vocab_size: Size of the subword vocabulary (default: 2000).\n",
    "        model_prefix: Prefix for output model files.\n",
    "    \n",
    "    Returns:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]\n",
    "    \n",
    "    # Validate files (same as your original)\n",
    "    for file_name in file_path_list:\n",
    "        if os.path.isdir(file_name):\n",
    "            raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "    \n",
    "    # Combine all files into one input (or use comma-separated list)\n",
    "    input_files = ','.join(file_path_list)\n",
    "    \n",
    "    # Train SentencePiece model\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_files,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe',\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "    )\n",
    "    \n",
    "    # Load and return processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'{model_prefix}.model')\n",
    "    return sp\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id_sp(sp: spm.SentencePieceProcessor, \n",
    "                                 text_batch: List[str], \n",
    "                                 max_seq_len: int, \n",
    "                                 pad_value: int = 0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize batch of text using SentencePiece (replaces tokenize_and_build_token_id).\n",
    "    \n",
    "    Args:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "        text_batch: List of text strings to tokenize.\n",
    "        max_seq_len: Maximum sequence length after padding/truncation.\n",
    "        pad_value: Integer ID used for padding tokens (should match sp.pad_id()).\n",
    "    \n",
    "    Returns:\n",
    "        token_ids: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "        attention_mask: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "    \"\"\"\n",
    "    batch_token_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        # Encode text to subword IDs\n",
    "        ids = sp.encode_as_ids(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[-max_seq_len:]  # Keep the end (recent context)\n",
    "        else:\n",
    "            # Pad to max_seq_len\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_token_ids.append(ids)\n",
    "    \n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    \n",
    "    return tf.constant(token_ids), tf.constant(attention_mask) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ec0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        max_seq_len: int = 512,\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, max_seq_len)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ae6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self,attention_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape): # Two tuples -> first tuple is (Batch Shape , Max_seq_length_in_batch,d_model) , Second tuple is (batch , max_seq_len)\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        self.Query_projection = self.add_weight(\n",
    "            name = 'Query_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name = 'Key_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name = 'Value_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "\n",
    "        self.output_projection = self.add_weight(\n",
    "        name=\"Output_projection\",\n",
    "        initializer=\"random_normal\",\n",
    "        shape=(self.d_model, self.d_model),\n",
    "        trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads # type: ignore\n",
    "        \n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self,inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0] # type: ignore\n",
    "        seq_len = tf.shape(embeddings)[1] # type: ignore\n",
    "\n",
    "        Q = embeddings @ self.Query_projection # (seq_len , d_model)\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head Attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        scores = tf.matmul(Q,K, transpose_b=True) # (batch , heads , seq_len,seq_len)\n",
    "        scores = scores / tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        # 5a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))  # (1,1,L,L)\n",
    "\n",
    "        # 5b. Token mask (B,L) -> (B,1,1,L)\n",
    "        token_mask = tf.cast(token_masks[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "\n",
    "        # 5c. Combine masks\n",
    "        combined_mask = causal_mask * token_mask  # broadcast -> (B, H, L, L)\n",
    "\n",
    "        # 6. Apply mask (replace disallowed with -1e9)\n",
    "        scores = tf.where(combined_mask > 0, scores, tf.constant(-1e9, dtype = scores.dtype))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        context = attention_weights @ V   #(batch, heads, seq_len, seq_len) × (batch, heads, seq_len, d_head) → (batch, heads, seq_len, d_head)\n",
    "        concat_context = tf.reshape(context, (batch_size,seq_len,self.attention_heads * self.d_head))  # type: ignore\n",
    "\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads,})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edee72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e118009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class CosineDecayWithWarmup(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, \n",
    "                 warmup_steps: int,\n",
    "                 total_steps: int,\n",
    "                 peak_learning_rate: float = 1e-4,\n",
    "                 min_learning_rate: float = 1e-6,\n",
    "                 name: str = \"cosine_decay_with_warmup\"):\n",
    "        super().__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase: linear increase from 0 to peak_learning_rate\n",
    "        warmup_lr = self.peak_learning_rate * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        cosine_decay_lr = self.min_learning_rate + 0.5 * (\n",
    "            self.peak_learning_rate - self.min_learning_rate\n",
    "        ) * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
    "        \n",
    "        return tf.where(step < warmup_steps, warmup_lr, cosine_decay_lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"peak_learning_rate\": self.peak_learning_rate,\n",
    "            \"min_learning_rate\": self.min_learning_rate,\n",
    "            \"name\": self.name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfcab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171a899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: \"'\",\n",
       " 4: '(',\n",
       " 5: ')',\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '0',\n",
       " 10: '1',\n",
       " 11: '2',\n",
       " 12: '3',\n",
       " 13: '4',\n",
       " 14: '5',\n",
       " 15: '6',\n",
       " 16: '7',\n",
       " 17: '8',\n",
       " 18: '9',\n",
       " 19: ':',\n",
       " 20: ';',\n",
       " 21: '?',\n",
       " 22: 'a',\n",
       " 23: 'b',\n",
       " 24: 'c',\n",
       " 25: 'd',\n",
       " 26: 'e',\n",
       " 27: 'f',\n",
       " 28: 'g',\n",
       " 29: 'h',\n",
       " 30: 'i',\n",
       " 31: 'j',\n",
       " 32: 'k',\n",
       " 33: 'l',\n",
       " 34: 'm',\n",
       " 35: 'n',\n",
       " 36: 'o',\n",
       " 37: 'p',\n",
       " 38: 'q',\n",
       " 39: 'r',\n",
       " 40: 's',\n",
       " 41: 't',\n",
       " 42: 'u',\n",
       " 43: 'v',\n",
       " 44: 'w',\n",
       " 45: 'x',\n",
       " 46: 'y',\n",
       " 47: 'z'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best_model.keras\n",
      "Vocabulary size: 38\n",
      "Model type: <class '__main__.GPT'>\n",
      "Model vocab size: 38\n",
      "Model context length: 128\n",
      "Model d_model: 64\n",
      "Model attention heads: 2\n",
      "Model decoder blocks: 1\n",
      "Vocab size matches model: True\n",
      "Sample characters in vocab: ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r']\n",
      "Common characters present: [True, True, True, True]\n",
      "Checking gibberish characters:\n",
      "  '.' -> ID 8 ✓\n",
      "  'f' -> ID 17 ✓\n",
      "  '6' -> NOT IN VOCAB ✗\n",
      "  's' -> ID 30 ✓\n",
      "  'r' -> ID 29 ✓\n",
      "  'm' -> ID 24 ✓\n",
      "  'n' -> ID 25 ✓\n",
      "  '?' -> ID 11 ✓\n",
      "  '7' -> NOT IN VOCAB ✗\n",
      "  'k' -> ID 22 ✓\n",
      "  '4' -> NOT IN VOCAB ✗\n",
      "  't' -> ID 31 ✓\n",
      "  '8' -> NOT IN VOCAB ✗\n",
      "  ' ' -> ID 1 ✓\n",
      "Testing improved model:\n",
      "\n",
      "Testing with: 'the'\n",
      "Top 5 predictions:\n",
      "  1. 'a' (ID: 12) - 0.1819\n",
      "  2. 'j' (ID: 21) - 0.0798\n",
      "  3. ')' (ID: 5) - 0.0611\n",
      "  4. 's' (ID: 30) - 0.0552\n",
      "  5. 'k' (ID: 22) - 0.0526\n",
      "\n",
      "Testing with: 'elizabeth'\n",
      "Top 5 predictions:\n",
      "  1. 'j' (ID: 21) - 0.1680\n",
      "  2. 's' (ID: 30) - 0.1328\n",
      "  3. 'a' (ID: 12) - 0.0715\n",
      "  4. 'w' (ID: 34) - 0.0713\n",
      "  5. ':' (ID: 9) - 0.0607\n",
      "\n",
      "Testing with: 'it is a'\n",
      "Top 5 predictions:\n",
      "  1. 's' (ID: 30) - 0.2940\n",
      "  2. '(' (ID: 4) - 0.1010\n",
      "  3. '\n",
      "' (ID: 0) - 0.0823\n",
      "  4. 'k' (ID: 22) - 0.0557\n",
      "  5. 't' (ID: 31) - 0.0463\n",
      "\n",
      "Testing with longer Jane Austen context:\n",
      "Context: 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a'\n",
      "Context length: 109 tokens\n",
      "Top 5 predictions after long context:\n",
      "  1. 's' (ID: 30) - 0.2223\n",
      "  2. '(' (ID: 4) - 0.0903\n",
      "  3. '\n",
      "' (ID: 0) - 0.0841\n",
      "  4. 't' (ID: 31) - 0.0757\n",
      "  5. 'k' (ID: 22) - 0.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15253/3941127056.py:267: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:6020\n",
      "* Running on public URL: https://4a5e37927586e1893f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4a5e37927586e1893f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generation Debug ---\n",
      "Input prompt: 'a' (will be lowercased)\n",
      "Input tokens: [12]\n",
      "Input tokens decoded back: 'a'\n",
      "Special tokens - PAD: None, EOS (newline): 0\n",
      "Starting generation with 1 input tokens...\n",
      "Step 0: Token 9 -> ':' (prob: 0.1236)\n",
      "Step 1: Token 34 -> 'w' (prob: 0.1939)\n",
      "Step 2: Token 34 -> 'w' (prob: 0.4320)\n",
      "Step 3: Token 34 -> 'w' (prob: 0.4292)\n",
      "Step 4: Token 34 -> 'w' (prob: 0.4280)\n",
      "Step 5: Token 34 -> 'w' (prob: 0.4284)\n",
      "Step 6: Token 34 -> 'w' (prob: 0.4303)\n",
      "Step 7: Token 34 -> 'w' (prob: 0.4323)\n",
      "Step 8: Token 34 -> 'w' (prob: 0.4322)\n",
      "Step 9: Token 34 -> 'w' (prob: 0.4294)\n",
      "Generated 30 new tokens: [9, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34]...\n",
      "Generated response: ':wwwwwwwwwwwwwwwwwwwwwwwwwwwww'\n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Generation Debug ---\n",
      "Input prompt: 'a' (will be lowercased)\n",
      "Input tokens: [12]\n",
      "Input tokens decoded back: 'a'\n",
      "Special tokens - PAD: None, EOS (newline): 0\n",
      "Starting generation with 1 input tokens...\n",
      "Step 0: Token 16 -> 'e' (prob: 0.2835)\n",
      "Step 1: Token 34 -> 'w' (prob: 0.8104)\n",
      "Step 2: Token 34 -> 'w' (prob: 0.9960)\n",
      "Step 3: Token 34 -> 'w' (prob: 0.9958)\n",
      "Step 4: Token 34 -> 'w' (prob: 0.9958)\n",
      "Step 5: Token 34 -> 'w' (prob: 0.9959)\n",
      "Step 6: Token 34 -> 'w' (prob: 0.9960)\n",
      "Step 7: Token 34 -> 'w' (prob: 0.9960)\n",
      "Step 8: Token 34 -> 'w' (prob: 0.9960)\n",
      "Step 9: Token 34 -> 'w' (prob: 0.9957)\n",
      "Generated 30 new tokens: [16, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34]...\n",
      "Generated response: 'ewwwwwwwwwwwwwwwwwwwwwwwwwwwww'\n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Generation Debug ---\n",
      "Input prompt: 'fd' (will be lowercased)\n",
      "Input tokens: [17, 15]\n",
      "Input tokens decoded back: 'fd'\n",
      "Special tokens - PAD: None, EOS (newline): 0\n",
      "Starting generation with 2 input tokens...\n",
      "Step 0: Token 9 -> ':' (prob: 0.2539)\n",
      "Step 1: Token 9 -> ':' (prob: 0.1544)\n",
      "Step 2: Token 9 -> ':' (prob: 0.1508)\n",
      "Step 3: Token 16 -> 'e' (prob: 0.0346)\n",
      "Step 4: Token 16 -> 'e' (prob: 0.0661)\n",
      "Step 5: Token 34 -> 'w' (prob: 0.2165)\n",
      "Step 6: Token 21 -> 'j' (prob: 0.0593)\n",
      "Step 7: Token 9 -> ':' (prob: 0.2241)\n",
      "Step 8: Token 16 -> 'e' (prob: 0.0370)\n",
      "Step 9: Token 34 -> 'w' (prob: 0.2201)\n",
      "Stopping at step 30: hit EOS token (newline)\n",
      "Generated 30 new tokens: [9, 9, 9, 16, 16, 34, 21, 9, 16, 34, 0, 9, 1, 34, 16, 16, 14, 37, 34, 34]...\n",
      "Generated response: ':::eewj:ew\n",
      ": weeczwwwwwvwj:eww'\n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Generation Debug ---\n",
      "Input prompt: 'w' (will be lowercased)\n",
      "Input tokens: [34]\n",
      "Input tokens decoded back: 'w'\n",
      "Special tokens - PAD: None, EOS (newline): 0\n",
      "Starting generation with 1 input tokens...\n",
      "Step 0: Token 34 -> 'w' (prob: 0.3468)\n",
      "Step 1: Token 34 -> 'w' (prob: 0.4144)\n",
      "Step 2: Token 0 -> '\n",
      "' (prob: 0.0442)\n",
      "Step 3: Token 21 -> 'j' (prob: 0.0535)\n",
      "Step 4: Token 34 -> 'w' (prob: 0.1876)\n",
      "Step 5: Token 34 -> 'w' (prob: 0.4158)\n",
      "Step 6: Token 34 -> 'w' (prob: 0.4199)\n",
      "Step 7: Token 11 -> '?' (prob: 0.0188)\n",
      "Step 8: Token 21 -> 'j' (prob: 0.0451)\n",
      "Step 9: Token 11 -> '?' (prob: 0.0741)\n",
      "Stopping at step 13: hit EOS token (newline)\n",
      "Generated 13 new tokens: [34, 34, 0, 21, 34, 34, 34, 11, 21, 11, 21, 34, 34]...\n",
      "Generated response: 'ww\n",
      "jwww?j?jww'\n",
      "--- End Debug ---\n",
      "\n",
      "\n",
      "--- Generation Debug ---\n",
      "Input prompt: 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a' (will be lowercased)\n",
      "Input tokens: [20, 31, 1, 20, 30, 1, 12, 1, 31, 29, 32, 31, 19, 1, 32, 25, 20, 33, 16, 29, 30, 12, 23, 23, 36, 1, 12, 14, 22, 25, 26, 34, 23, 16, 15, 18, 16, 15, 1, 31, 19, 12, 31, 1, 12, 1, 30, 20, 25, 18, 23, 16, 1, 24, 12, 25, 1, 20, 25, 1, 27, 26, 30, 30, 16, 30, 30, 20, 26, 25, 1, 26, 17, 1, 12, 1, 18, 26, 26, 15, 1, 17, 26, 29, 31, 32, 25, 16, 1, 24, 32, 30, 31, 1, 13, 16, 1, 20, 25, 1, 34, 12, 25, 31, 1, 26, 17, 1, 12]\n",
      "Input tokens decoded back: 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a'\n",
      "Special tokens - PAD: None, EOS (newline): 0\n",
      "Starting generation with 109 input tokens...\n",
      "Step 0: Token 0 -> '\n",
      "' (prob: 0.0957)\n",
      "Step 1: Token 34 -> 'w' (prob: 0.1607)\n",
      "Step 2: Token 37 -> 'z' (prob: 0.0208)\n",
      "Step 3: Token 9 -> ':' (prob: 0.0684)\n",
      "Step 4: Token 34 -> 'w' (prob: 0.2253)\n",
      "Step 5: Token 34 -> 'w' (prob: 0.4702)\n",
      "Step 6: Token 34 -> 'w' (prob: 0.4695)\n",
      "Step 7: Token 34 -> 'w' (prob: 0.4688)\n",
      "Step 8: Token 35 -> 'x' (prob: 0.0092)\n",
      "Step 9: Token 32 -> 'u' (prob: 0.0187)\n",
      "Stopping at step 24: hit EOS token (newline)\n",
      "Generated 19 new tokens: [34, 34, 34, 35, 32, 35, 25, 34, 34, 34, 34, 14, 34, 12, 32, 34, 34, 12, 16]...\n",
      "Generated response: 'wwwxuxnwwwwcwauwwae'\n",
      "--- End Debug ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "token_to_id_dict = tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'])\n",
    "id_to_token_dict = {id_val: token for token, id_val in token_to_id_dict.items()}\n",
    "\n",
    "# Use the latest and best model - try the best_model.keras first, then latest checkpoint\n",
    "try:\n",
    "    model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/rewrite_char_level_checkpoints/model_epoch_233_val_loss_1.8321.keras')\n",
    "    print(\"✅ Loaded best_model.keras\")\n",
    "except:\n",
    "    try:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras')\n",
    "        print(\"✅ Loaded epoch 163 model\")\n",
    "    except:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras')\n",
    "        print(\"✅ Loaded epoch 161 model\")\n",
    "\n",
    "CONTEXT_LEN = model._context_length  # Use the model's actual context length\n",
    "\n",
    "# Debug: Print vocabulary info\n",
    "print(f\"Vocabulary size: {len(token_to_id_dict)}\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Get model info from your custom GPT model\n",
    "try:\n",
    "    print(f\"Model vocab size: {model._vocab_size}\")\n",
    "    print(f\"Model context length: {model._context_length}\")\n",
    "    print(f\"Model d_model: {model._d_model}\")\n",
    "    print(f\"Model attention heads: {model._attention_heads}\")\n",
    "    print(f\"Model decoder blocks: {model._decoder_blocks}\")\n",
    "    print(f\"Vocab size matches model: {model._vocab_size == len(token_to_id_dict)}\")\n",
    "    \n",
    "    if model._vocab_size != len(token_to_id_dict):\n",
    "        print(f\"⚠️  VOCAB SIZE MISMATCH! Model expects {model._vocab_size}, got {len(token_to_id_dict)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting model info: {e}\")\n",
    "\n",
    "print(f\"Sample characters in vocab: {list(token_to_id_dict.keys())[:30]}\")\n",
    "print(f\"Common characters present: {['a' in token_to_id_dict, 'e' in token_to_id_dict, ' ' in token_to_id_dict, '.' in token_to_id_dict]}\")\n",
    "\n",
    "# Check for problematic characters in the gibberish output\n",
    "gibberish = \"4ff.mtm 64m86rfstmfm?.fmmftms777mtmkf  tm7n7m77m77\"\n",
    "print(f\"Checking gibberish characters:\")\n",
    "for char in set(gibberish):\n",
    "    if char in token_to_id_dict:\n",
    "        print(f\"  '{char}' -> ID {token_to_id_dict[char]} ✓\")\n",
    "    else:\n",
    "        print(f\"  '{char}' -> NOT IN VOCAB ✗\")\n",
    "\n",
    "def encode_text(text, token_to_id_dict):\n",
    "    \"\"\"Encode text to token IDs using character-level tokenizer - convert to lowercase since dataset is lowercase\"\"\"\n",
    "    # Convert input to lowercase since your dataset was lowercased\n",
    "    text = text.lower()\n",
    "    \n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "        if char in token_to_id_dict:\n",
    "            token_ids.append(token_to_id_dict[char])\n",
    "        else:\n",
    "            print(f\"Warning: '{char}' (ord: {ord(char)}) not in vocabulary, skipping\")\n",
    "            continue\n",
    "    return token_ids\n",
    "\n",
    "def decode_ids(token_ids, id_to_token_dict):\n",
    "    \"\"\"Decode token IDs back to text using character-level tokenizer\"\"\"\n",
    "    text = \"\"\n",
    "    for token_id in token_ids:\n",
    "        if token_id in id_to_token_dict:\n",
    "            text += id_to_token_dict[token_id]\n",
    "        else:\n",
    "            print(f\"Warning: token ID {token_id} not in vocabulary\")\n",
    "    return text\n",
    "\n",
    "def get_special_token_ids():\n",
    "    \"\"\"Get special token IDs - adjust these based on your tokenizer setup\"\"\"\n",
    "    # For Jane Austen data, likely no special PAD token, use newline as EOS\n",
    "    pad_id = token_to_id_dict.get('<PAD>', None)\n",
    "    eos_id = token_to_id_dict.get('\\n', None)  # Use newline as natural stopping point\n",
    "    print(f\"Special tokens - PAD: {pad_id}, EOS (newline): {eos_id}\")\n",
    "    return pad_id, eos_id\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    # Ensure we don't sample more than available tokens\n",
    "    k = min(k, len(logits))\n",
    "    \n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    \n",
    "    # Add small epsilon to avoid numerical issues\n",
    "    probs = probs + 1e-10\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_k=10, use_argmax=False):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"\\n--- Generation Debug ---\")\n",
    "    print(f\"Input prompt: '{prompt}' (will be lowercased)\")\n",
    "    \n",
    "    # Tokenize prompt with character-level tokenizer\n",
    "    input_tokens = encode_text(prompt, token_to_id_dict)\n",
    "    print(f\"Input tokens: {input_tokens}\")\n",
    "    print(f\"Input tokens decoded back: '{decode_ids(input_tokens, id_to_token_dict)}'\")\n",
    "    \n",
    "    if not input_tokens:\n",
    "        return \"Error: Could not tokenize input\"\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    pad_id, eos_id = get_special_token_ids()\n",
    "    \n",
    "    print(f\"Starting generation with {len(input_tokens)} input tokens...\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Prepare inputs - pad from left to maintain most recent context\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        \n",
    "        # Place tokens at the end of the context window\n",
    "        current_len = min(len(generated_tokens), CONTEXT_LEN)\n",
    "        start_idx = CONTEXT_LEN - current_len\n",
    "        input_ids[0, start_idx:] = generated_tokens[-current_len:]\n",
    "        attention_mask[0, start_idx:] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        try:\n",
    "            logits = model((input_ids, attention_mask), training=False)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if not use_argmax:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "        except Exception as e:\n",
    "            print(f\"Model forward pass error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Sample next token\n",
    "        try:\n",
    "            if use_argmax:\n",
    "                # Use argmax (greedy) sampling for testing\n",
    "                next_token = int(np.argmax(next_token_logits))\n",
    "            else:\n",
    "                # Use top-k sampling\n",
    "                next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Debug: Print first few tokens\n",
    "        if step < 10:\n",
    "            sampled_char = id_to_token_dict.get(next_token, f\"<UNK:{next_token}>\")\n",
    "            prob = float(tf.nn.softmax(next_token_logits)[next_token])\n",
    "            print(f\"Step {step}: Token {next_token} -> '{sampled_char}' (prob: {prob:.4f})\")\n",
    "        \n",
    "        # Check if token is valid\n",
    "        if next_token >= len(id_to_token_dict):\n",
    "            print(f\"Warning: Invalid token {next_token}, vocab size is {len(id_to_token_dict)}\")\n",
    "            break\n",
    "        \n",
    "        # Stop on special tokens\n",
    "        if pad_id is not None and next_token == pad_id:\n",
    "            print(f\"Stopping at step {step}: hit PAD token\")\n",
    "            break\n",
    "        if eos_id is not None and next_token == eos_id and step > 10:  # Don't stop too early\n",
    "            print(f\"Stopping at step {step}: hit EOS token (newline)\")\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        # Maintain sliding window\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = decode_ids(new_tokens, id_to_token_dict)\n",
    "    print(f\"Generated {len(new_tokens)} new tokens: {new_tokens[:20]}...\")  # Show first 20\n",
    "    print(f\"Generated response: '{response}'\")\n",
    "    print(f\"--- End Debug ---\\n\")\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k, use_argmax):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k, use_argmax=use_argmax)\n",
    "    history.append((message, bot_response))\n",
    "    return \"\", history\n",
    "\n",
    "# Quick test with the better model\n",
    "print(\"Testing improved model:\")\n",
    "test_cases = [\"the\", \"elizabeth\", \"it is a\"]\n",
    "\n",
    "for prompt in test_cases:\n",
    "    print(f\"\\nTesting with: '{prompt}'\")\n",
    "    tokens = encode_text(prompt, token_to_id_dict)\n",
    "    \n",
    "    # Create model input\n",
    "    input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "    attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "    input_ids[0, -len(tokens):] = tokens\n",
    "    attention_mask[0, -len(tokens):] = 1\n",
    "    \n",
    "    # Get model predictions\n",
    "    logits = model((input_ids, attention_mask), training=False)\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for i in range(5):\n",
    "        token_id = int(top_indices[i])\n",
    "        prob = float(top_probs[i])\n",
    "        char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "        print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "# Test with longer context\n",
    "print(f\"\\nTesting with longer Jane Austen context:\")\n",
    "long_prompt = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a\"\n",
    "tokens = encode_text(long_prompt, token_to_id_dict)\n",
    "print(f\"Context: '{long_prompt}'\")\n",
    "print(f\"Context length: {len(tokens)} tokens\")\n",
    "\n",
    "# Use reasonable context length\n",
    "context_tokens = tokens[-100:] if len(tokens) > 100 else tokens\n",
    "\n",
    "input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "input_ids[0, -len(context_tokens):] = context_tokens\n",
    "attention_mask[0, -len(context_tokens):] = 1\n",
    "\n",
    "logits = model((input_ids, attention_mask), training=False)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "print(\"Top 5 predictions after long context:\")\n",
    "for i in range(5):\n",
    "    token_id = int(top_indices[i])\n",
    "    prob = float(top_probs[i])\n",
    "    char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "    print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "with gr.Blocks(title=\"My Character-Level GPT Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Akshat's Character-Level GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with character-level tokenization on Jane Austen data. Be gentle with me :)\")\n",
    "    \n",
    "    # Add vocab info\n",
    "    gr.Markdown(f\"**Model Info:** Vocabulary size: {len(token_to_id_dict)} characters\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.3, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=10, maximum=200, value=30, step=10, label=\"Max Length\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=5, step=1, label=\"Top-K Sampling\")\n",
    "        use_argmax = gr.Checkbox(label=\"Use Argmax (Greedy) - for testing\", value=True)\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6020,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250acbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 'it is a'\n",
      "Tokens: [30, 41, 1, 30, 40, 1, 22]\n",
      "Top 10 predictions:\n",
      "  1. 'f' (ID: 27) - 0.3225\n",
      "  2. 'c' (ID: 24) - 0.0669\n",
      "  3. ',' (ID: 6) - 0.0521\n",
      "  4. ' ' (ID: 1) - 0.0432\n",
      "  5. '-' (ID: 7) - 0.0400\n",
      "  6. 't' (ID: 41) - 0.0397\n",
      "  7. '9' (ID: 18) - 0.0386\n",
      "  8. 'p' (ID: 37) - 0.0377\n",
      "  9. 'j' (ID: 31) - 0.0376\n",
      "  10. 's' (ID: 40) - 0.0365\n"
     ]
    }
   ],
   "source": [
    "# Test with a very simple prompt\n",
    "test_prompt = \"it is a\"\n",
    "print(f\"Testing with: '{test_prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = encode_text(test_prompt, token_to_id_dict)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Create model input\n",
    "input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "input_ids[0, -len(tokens):] = tokens\n",
    "attention_mask[0, -len(tokens):] = 1\n",
    "\n",
    "# Get model predictions\n",
    "logits = model((input_ids, attention_mask), training=False)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Show top 10 predictions\n",
    "top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=10)\n",
    "print(\"Top 10 predictions:\")\n",
    "for i in range(10):\n",
    "    token_id = int(top_indices[i])\n",
    "    prob = float(top_probs[i])\n",
    "    char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "    print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c030eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent characters:\n",
      "' ': 716875\n",
      "'e': 433350\n",
      "'t': 296790\n",
      "'a': 268929\n",
      "'o': 264312\n",
      "'n': 244375\n",
      "'i': 234267\n",
      "'s': 212583\n",
      "'h': 212370\n",
      "'r': 209492\n"
     ]
    }
   ],
   "source": [
    "# Check character frequency in your data\n",
    "with open('/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt', 'r') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "char_counts = {}\n",
    "for char in text:\n",
    "    char_counts[char] = char_counts.get(char, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most frequent characters:\")\n",
    "for char, count in sorted_chars[:10]:\n",
    "    print(f\"'{char}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06f0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Hello' -> Tokens: [26, 33, 33, 36] -> Decoded: 'ello'\n"
     ]
    }
   ],
   "source": [
    "# Test your tokenizer\n",
    "test_text = \"Hello\"\n",
    "tokens = encode_text(test_text, token_to_id_dict)\n",
    "decoded = decode_ids(tokens, id_to_token_dict)\n",
    "print(f\"Original: '{test_text}' -> Tokens: {tokens} -> Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248e0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
