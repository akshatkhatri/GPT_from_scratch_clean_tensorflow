{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62adc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 06:58:01.712780: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-24 06:58:01.782120: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-24 06:58:03.104953: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "CONTEXT_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b8fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007f70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str] = [r\"/home/akshat/GPT_from_scratch/text_data/pg76702.txt\"],\n",
    "#     existing_vocab: Dict[str, int] = None # type: ignore\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TensorFlow lookup tables.\n",
    "    \n",
    "#     Returns:\n",
    "#         token_to_id_table: tf.lookup.StaticHashTable mapping char -> int\n",
    "#         id_to_token_table: tf.lookup.StaticHashTable mapping int -> char\n",
    "#     \"\"\"\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "\n",
    "#     # Collect characters from all files\n",
    "#     for file_name in file_path_list:\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     # Sort for consistency\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "\n",
    "#     # Assign IDs (keep existing IDs if possible)\n",
    "#     token_to_id = {token: i for i, token in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "#     # Convert dicts to tensors\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()))\n",
    "#     token_values = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_values = tf.constant(list(id_to_token.values()))\n",
    "\n",
    "#     # Create TensorFlow lookup tables\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_values),\n",
    "#         default_value=-1  # unknown token\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_values),\n",
    "#         default_value=\"\"  # unknown ID\n",
    "#     )\n",
    "\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9856e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import List, Dict, Tuple\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str],\n",
    "#     existing_vocab: Dict[str, int] | None = None\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TF lookup tables:\n",
    "#       token_to_id: char -> int\n",
    "#       id_to_token: int -> char\n",
    "#     \"\"\"\n",
    "#     if isinstance(file_path_list, (str, bytes)):\n",
    "#         file_path_list = [file_path_list] # type: ignore\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "#     for file_name in file_path_list:\n",
    "#         if os.path.isdir(file_name):\n",
    "#             raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "#         if not os.path.isfile(file_name):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "#     token_to_id = {tok: i for i, tok in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()), dtype=tf.string)\n",
    "#     token_vals = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_vals = tf.constant(list(id_to_token.values()), dtype=tf.string)\n",
    "\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_vals),\n",
    "#         default_value=-1\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_vals),\n",
    "#         default_value=tf.constant(\"\", dtype=tf.string)\n",
    "#     )\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_token_id(token_to_id_table: tf.lookup.StaticHashTable,\n",
    "#                                 text_batch: tf.Tensor,\n",
    "#                                 pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Tokenize a batch of strings character by character, pad sequences,\n",
    "#     and return attention masks.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_table: TF lookup table mapping char -> int\n",
    "#         text_batch: tf.Tensor of shape [batch_size], dtype=tf.string\n",
    "#         pad_value: int, ID to use for padding\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: tf.Tensor [batch_size, max_seq_len]\n",
    "#         attention_mask: tf.Tensor [batch_size, max_seq_len]\n",
    "#     \"\"\"\n",
    "#     token_ids_list = []\n",
    "\n",
    "#     for text in text_batch.numpy():  # type: ignore\n",
    "#         # Convert bytes to TF string\n",
    "\n",
    "#         # Split into characters\n",
    "#         char_tensor = tf.strings.bytes_split(text)\n",
    "\n",
    "#         # Lookup token IDs\n",
    "#         token_ids = token_to_id_table.lookup(char_tensor)\n",
    "\n",
    "#         token_ids_list.append(token_ids)\n",
    "\n",
    "#     # Pad all sequences to the same length\n",
    "#     token_ids_padded = tf.ragged.stack(token_ids_list).to_tensor(default_value=pad_value) # type: ignore\n",
    "#     # Create attention mask: 1 for real tokens, 0 for padding\n",
    "#     attention_mask = tf.cast(token_ids_padded != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids_padded, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import Tuple\n",
    "\n",
    "\n",
    "# def tokenize_and_build_token_id(\n",
    "#     token_to_id_dict: dict,\n",
    "#     text_batch: list[str],\n",
    "#     max_seq_len: int,\n",
    "#     pad_value: int = 0,\n",
    "#     unk_value: int = None\n",
    "# ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "#     \"\"\"\n",
    "#     TensorFlow-compatible tokenization converting batch of strings to char token IDs,\n",
    "#     padded/truncated to max_seq_len, along with attention mask.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_dict: dict mapping character str -> int ID\n",
    "#         text_batch: list of strings to tokenize\n",
    "#         max_seq_len: max length to pad/truncate sequences\n",
    "#         pad_value: int ID for padding tokens\n",
    "#         unk_value: int ID for unknown tokens; if None, uses pad_value\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: (batch_size, max_seq_len) tf.int32 tensor of token IDs\n",
    "#         attention_mask: (batch_size, max_seq_len) tf.int32 tensor (1 for tokens, 0 for padding)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if unk_value is None:\n",
    "#         unk_value = pad_value\n",
    "\n",
    "#     # Create lookup table from token_to_id_dict\n",
    "#     keys = tf.constant(list(token_to_id_dict.keys()))\n",
    "#     values = tf.constant(list(token_to_id_dict.values()), dtype=tf.int32)\n",
    "#     table = tf.lookup.StaticHashTable(\n",
    "#         tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "#         default_value=unk_value\n",
    "#     )\n",
    "\n",
    "#     # Convert text batch to a RaggedTensor of chars\n",
    "#     rt_chars = tf.strings.unicode_split(text_batch, 'UTF-8')  # shape: [batch_size, (seq_len)]\n",
    "\n",
    "#     # Lookup token IDs for each char\n",
    "#     token_ids = table.lookup(rt_chars)\n",
    "\n",
    "#     # Pad or truncate sequences to max_seq_len\n",
    "#     token_ids = token_ids.to_tensor(default_value=pad_value, shape=[None, max_seq_len])\n",
    "#     token_ids = token_ids[:, :max_seq_len]  # truncate if longer\n",
    "\n",
    "#     # Construct attention mask: 1 where not pad_value, else 0\n",
    "#     attention_mask = tf.cast(token_ids != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e3a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def train_sentencepiece_tokenizer(file_path_list: List[str], \n",
    "                                vocab_size: int = 2000,\n",
    "                                model_prefix: str = 'spm_gpt') -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizer from text files (replaces tokenize_and_build_vocabulary_tf).\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths containing the text corpus.\n",
    "        vocab_size: Size of the subword vocabulary (default: 2000).\n",
    "        model_prefix: Prefix for output model files.\n",
    "    \n",
    "    Returns:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]\n",
    "    \n",
    "    # Validate files (same as your original)\n",
    "    for file_name in file_path_list:\n",
    "        if os.path.isdir(file_name):\n",
    "            raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "    \n",
    "    # Combine all files into one input (or use comma-separated list)\n",
    "    input_files = ','.join(file_path_list)\n",
    "    \n",
    "    # Train SentencePiece model\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_files,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe',\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "    )\n",
    "    \n",
    "    # Load and return processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'{model_prefix}.model')\n",
    "    return sp\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id_sp(sp: spm.SentencePieceProcessor, \n",
    "                                 text_batch: List[str], \n",
    "                                 max_seq_len: int, \n",
    "                                 pad_value: int = 0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize batch of text using SentencePiece (replaces tokenize_and_build_token_id).\n",
    "    \n",
    "    Args:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "        text_batch: List of text strings to tokenize.\n",
    "        max_seq_len: Maximum sequence length after padding/truncation.\n",
    "        pad_value: Integer ID used for padding tokens (should match sp.pad_id()).\n",
    "    \n",
    "    Returns:\n",
    "        token_ids: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "        attention_mask: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "    \"\"\"\n",
    "    batch_token_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        # Encode text to subword IDs\n",
    "        ids = sp.encode_as_ids(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[-max_seq_len:]  # Keep the end (recent context)\n",
    "        else:\n",
    "            # Pad to max_seq_len\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_token_ids.append(ids)\n",
    "    \n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    \n",
    "    return tf.constant(token_ids), tf.constant(attention_mask) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9a2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_gpt\n",
      "  model_type: BPE\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: /home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\n",
      "trainer_interface.cc(382) LOG(WARNING) Found too long line (4217 > 4192).\n",
      "trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1170367), which may slow down training.\n",
      "trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1170367 sentences\n",
      "trainer_interface.cc(418) LOG(INFO) Skipped 14 too long sentences.\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=535316653\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9505% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=85\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1170367 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1170367\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 266193\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11221220 min_freq=2922\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3179521 size=20 all=3965 active=2146 piece=it\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1883967 size=40 all=5480 active=3661 piece=ion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1146454 size=60 all=6943 active=5124 piece=am\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=885396 size=80 all=8884 active=7065 piece=▁@-@\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=701151 size=100 all=10990 active=9171 piece=▁with\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=691610 min_freq=52979\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=572520 size=120 all=12473 active=2477 piece=▁L\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=484395 size=140 all=14740 active=4744 piece=▁his\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=373668 size=160 all=17235 active=7239 piece=igh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=328901 size=180 all=20147 active=10151 piece=ong\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=282416 size=200 all=22269 active=12273 piece=▁St\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=281427 min_freq=41329\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=246866 size=220 all=24561 active=3321 piece=ight\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=211296 size=240 all=26516 active=5276 piece=fter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=193383 size=260 all=28161 active=6921 piece=ub\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181625 size=280 all=30021 active=8781 piece=▁He\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167869 size=300 all=31866 active=10626 piece=per\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=167631 min_freq=25301\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=156349 size=320 all=33184 active=2785 piece=ugh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=147728 size=340 all=34350 active=3951 piece=▁j\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139234 size=360 all=35688 active=5289 piece=▁des\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129427 size=380 all=37354 active=6955 piece=ail\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121330 size=400 all=39572 active=9173 piece=oin\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=121015 min_freq=18247\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113841 size=420 all=41488 active=3821 piece=▁inter\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105156 size=440 all=43252 active=5585 piece=▁sec\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99928 size=460 all=44960 active=7293 piece=ft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=94519 size=480 all=45966 active=8299 piece=▁pr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=90243 size=500 all=46951 active=9284 piece=ob\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=90233 min_freq=14421\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86442 size=520 all=48303 active=3430 piece=hn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82800 size=540 all=49537 active=4664 piece=▁Americ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78423 size=560 all=50784 active=5911 piece=▁Com\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75563 size=580 all=51598 active=6725 piece=▁ep\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71944 size=600 all=52423 active=7550 piece=iel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=71929 min_freq=12221\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69583 size=620 all=53645 active=3728 piece=▁call\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66239 size=640 all=54401 active=4484 piece=▁17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=63943 size=660 all=55428 active=5511 piece=▁rece\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61932 size=680 all=56484 active=6567 piece=▁pol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59775 size=700 all=57833 active=7916 piece=▁main\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=59759 min_freq=10398\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58087 size=720 all=59496 active=4545 piece=ities\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56777 size=740 all=60984 active=6033 piece=▁episode\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55113 size=760 all=61802 active=6851 piece=though\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53441 size=780 all=62898 active=7947 piece=▁11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51817 size=800 all=63864 active=8913 piece=ange\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=51791 min_freq=8993\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50722 size=820 all=64998 active=4282 piece=▁own\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49507 size=840 all=66077 active=5361 piece=stem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47718 size=860 all=66882 active=6166 piece=▁group\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46155 size=880 all=68011 active=7295 piece=▁Ad\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44986 size=900 all=68724 active=8008 piece=▁gu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=44937 min_freq=7960\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43930 size=920 all=69708 active=4388 piece=yp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42898 size=940 all=70144 active=4824 piece=▁Pl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41789 size=960 all=70831 active=5511 piece=▁class\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40427 size=980 all=71662 active=6342 piece=ling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39816 size=1000 all=72425 active=7105 piece=gin\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=39791 min_freq=7295\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38694 size=1020 all=73213 active=4332 piece=▁mar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37701 size=1040 all=74041 active=5160 piece=raft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36774 size=1060 all=74795 active=5914 piece=▁allow\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35963 size=1080 all=75541 active=6660 piece=▁south\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35225 size=1100 all=76380 active=7499 piece=▁due\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=35198 min_freq=6669\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34592 size=1120 all=76812 active=4251 piece=▁short\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34060 size=1140 all=77463 active=4902 piece=▁port\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33391 size=1160 all=78195 active=5634 piece=ideo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32314 size=1180 all=78781 active=6220 piece=▁now\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31632 size=1200 all=79341 active=6780 piece=▁best\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=31628 min_freq=6207\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31135 size=1220 all=80067 active=4687 piece=▁belie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30560 size=1240 all=80820 active=5440 piece=▁view\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29951 size=1260 all=81518 active=6138 piece=ery\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29517 size=1280 all=82147 active=6767 piece=▁release\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28901 size=1300 all=83070 active=7690 piece=ards\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28897 min_freq=5716\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28365 size=1320 all=84018 active=5024 piece=ption\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27832 size=1340 all=85218 active=6224 piece=▁town\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27392 size=1360 all=86097 active=7103 piece=▁post\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26882 size=1380 all=87018 active=8024 piece=▁England\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26423 size=1400 all=88141 active=9147 piece=▁level\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26389 min_freq=5215\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25930 size=1420 all=88955 active=5216 piece=▁Park\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25477 size=1440 all=90176 active=6437 piece=▁every\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25125 size=1460 all=90633 active=6894 piece=bo\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24753 size=1480 all=91551 active=7812 piece=ize\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24051 size=1500 all=92248 active=8509 piece=craft\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24040 min_freq=4820\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23682 size=1520 all=92634 active=4977 piece=▁defe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23316 size=1540 all=93047 active=5390 piece=▁comb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22951 size=1560 all=93957 active=6300 piece=▁2006\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22560 size=1580 all=94773 active=7116 piece=echn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22152 size=1600 all=95300 active=7643 piece=▁eight\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22119 min_freq=4531\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21796 size=1620 all=95698 active=5158 piece=▁Australia\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21521 size=1640 all=96222 active=5682 piece=▁fr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21268 size=1660 all=97026 active=6486 piece=▁Ste\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20829 size=1680 all=97532 active=6992 piece=astern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20512 size=1700 all=98200 active=7660 piece=▁Comp\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20481 min_freq=4269\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20171 size=1720 all=98746 active=5419 piece=itted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19860 size=1740 all=99444 active=6117 piece=▁fav\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19422 size=1760 all=99752 active=6425 piece=▁television\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19153 size=1780 all=100669 active=7342 piece=reg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18928 size=1800 all=101243 active=7916 piece=▁Reg\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18924 min_freq=4037\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18737 size=1820 all=101788 active=5554 piece=elling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18560 size=1840 all=102595 active=6361 piece=▁seven\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18275 size=1860 all=103033 active=6799 piece=▁little\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18052 size=1880 all=103533 active=7299 piece=▁thought\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17914 size=1900 all=103822 active=7588 piece=▁coast\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17910 min_freq=3868\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17658 size=1920 all=104166 active=5526 piece=ript\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17382 size=1940 all=105003 active=6363 piece=ators\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17223 size=1960 all=106090 active=7450 piece=▁Hist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17087 size=1980 all=106754 active=8114 piece=▁arg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16879 size=2000 all=107683 active=9043 piece=omm\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16873 min_freq=3612\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16559 size=2020 all=108387 active=6015 piece=ament\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16405 size=2040 all=108749 active=6377 piece=▁186\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16191 size=2060 all=109367 active=6995 piece=▁bomb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15969 size=2080 all=109668 active=7296 piece=▁Some\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15776 size=2100 all=109971 active=7599 piece=▁damage\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15759 min_freq=3453\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15606 size=2120 all=110577 active=6105 piece=▁case\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15432 size=2140 all=111232 active=6760 piece=▁Vir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15321 size=2160 all=111889 active=7417 piece=▁resul\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15120 size=2180 all=112233 active=7761 piece=▁office\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14986 size=2200 all=112832 active=8360 piece=▁Hol\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14971 min_freq=3278\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14785 size=2220 all=113551 active=6295 piece=▁joined\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14571 size=2240 all=114482 active=7226 piece=▁believed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14394 size=2260 all=115089 active=7833 piece=▁Great\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14232 size=2280 all=115678 active=8422 piece=go\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14051 size=2300 all=116452 active=9196 piece=inist\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14038 min_freq=3098\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13883 size=2320 all=116941 active=6285 piece=▁indic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13773 size=2340 all=117617 active=6961 piece=▁exc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13677 size=2360 all=118407 active=7751 piece=▁sequ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13557 size=2380 all=119211 active=8555 piece=▁Kingdom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13418 size=2400 all=120005 active=9349 piece=▁wall\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13408 min_freq=2935\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13277 size=2420 all=120350 active=6334 piece=▁priv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13123 size=2440 all=120864 active=6848 piece=▁novel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12952 size=2460 all=121365 active=7349 piece=▁meas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12794 size=2480 all=121815 active=7799 piece=▁California\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12653 size=2500 all=122604 active=8588 piece=▁Island\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12647 min_freq=2828\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12554 size=2520 all=123091 active=6613 piece=elled\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12439 size=2540 all=123737 active=7259 piece=quare\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12328 size=2560 all=123961 active=7483 piece=▁elements\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12206 size=2580 all=124419 active=7941 piece=tr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12114 size=2600 all=125355 active=8877 piece=▁Gold\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12113 min_freq=2706\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11969 size=2620 all=125727 active=6598 piece=▁viol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11854 size=2640 all=126298 active=7169 piece=bers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11725 size=2660 all=126780 active=7651 piece=▁Frank\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11624 size=2680 all=127239 active=8110 piece=▁idea\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11481 size=2700 all=127722 active=8593 piece=usp\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11474 min_freq=2589\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11416 size=2720 all=128121 active=6749 piece=▁32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11287 size=2740 all=128591 active=7219 piece=▁magazine\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11162 size=2760 all=128902 active=7530 piece=▁shows\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11040 size=2780 all=129322 active=7950 piece=▁Virginia\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10909 size=2800 all=129647 active=8275 piece=▁lyric\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10909 min_freq=2488\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10834 size=2820 all=130118 active=6950 piece=ulf\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10767 size=2840 all=130493 active=7325 piece=change\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10708 size=2860 all=130891 active=7723 piece=▁fleet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10576 size=2880 all=131141 active=7973 piece=▁date\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10512 size=2900 all=131347 active=8179 piece=▁arrang\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10511 min_freq=2399\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10405 size=2920 all=131731 active=6945 piece=ores\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10345 size=2940 all=132153 active=7367 piece=▁big\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10226 size=2960 all=132481 active=7695 piece=▁favor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10114 size=2980 all=133129 active=8343 piece=▁language\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10013 size=3000 all=133580 active=8794 piece=▁feature\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10009 min_freq=2308\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9960 size=3020 all=133805 active=6902 piece=▁temper\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9828 size=3040 all=134083 active=7180 piece=▁Super\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9745 size=3060 all=134552 active=7649 piece=▁syn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9700 size=3080 all=134891 active=7988 piece=▁voice\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9611 size=3100 all=135217 active=8314 piece=▁immediately\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9610 min_freq=2247\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9550 size=3120 all=135708 active=7252 piece=▁officers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9462 size=3140 all=136021 active=7565 piece=uz\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9403 size=3160 all=136423 active=7967 piece=▁via\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9305 size=3180 all=136786 active=8330 piece=▁How\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9240 size=3200 all=137174 active=8718 piece=▁tow\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9239 min_freq=2180\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9181 size=3220 all=137678 active=7350 piece=▁consist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9137 size=3240 all=138305 active=7977 piece=▁Center\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9034 size=3260 all=138518 active=8190 piece=▁Ol\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8972 size=3280 all=139312 active=8984 piece=▁altern\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8912 size=3300 all=139814 active=9486 piece=▁attempted\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8910 min_freq=2113\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8856 size=3320 all=140087 active=7264 piece=▁Miss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8787 size=3340 all=140536 active=7713 piece=▁slight\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8720 size=3360 all=141018 active=8195 piece=▁selected\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8656 size=3380 all=141345 active=8522 piece=ste\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8613 size=3400 all=141880 active=9057 piece=▁Penn\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8612 min_freq=2047\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8544 size=3420 all=142407 active=7592 piece=▁audience\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8474 size=3440 all=142850 active=8035 piece=▁artist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8430 size=3460 all=143188 active=8373 piece=▁Gar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8354 size=3480 all=143491 active=8676 piece=▁Dis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8287 size=3500 all=144064 active=9249 piece=▁More\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8286 min_freq=1983\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8207 size=3520 all=144302 active=7423 piece=aped\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8140 size=3540 all=144569 active=7690 piece=▁abandon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8084 size=3560 all=144913 active=8034 piece=▁energy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8013 size=3580 all=145515 active=8636 piece=ening\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7950 size=3600 all=145878 active=8999 piece=▁Mexico\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7948 min_freq=1937\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7888 size=3620 all=146160 active=7576 piece=▁400\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7831 size=3640 all=146333 active=7749 piece=▁Sum\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7782 size=3660 all=146722 active=8138 piece=oper\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7724 size=3680 all=147158 active=8574 piece=▁direction\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7662 size=3700 all=147436 active=8852 piece=▁Manc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7661 min_freq=1888\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7619 size=3720 all=147819 active=7742 piece=▁avoid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7566 size=3740 all=148087 active=8010 piece=▁morning\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7532 size=3760 all=148586 active=8509 piece=▁forms\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7484 size=3780 all=148884 active=8807 piece=▁Records\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7407 size=3800 all=149195 active=9118 piece=▁source\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7407 min_freq=1831\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7357 size=3820 all=149672 active=7936 piece=▁impact\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7305 size=3840 all=150078 active=8342 piece=ait\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7246 size=3860 all=150313 active=8577 piece=▁foreign\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7188 size=3880 all=150879 active=9143 piece=▁owned\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7144 size=3900 all=151076 active=9340 piece=▁federal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7137 min_freq=1782\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7104 size=3920 all=151273 active=7745 piece=▁Brad\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7065 size=3940 all=151942 active=8414 piece=▁Swed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7003 size=3960 all=152168 active=8640 piece=▁paid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6970 size=3980 all=152439 active=8911 piece=▁actor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6924 size=4000 all=152640 active=9112 piece=▁Life\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6924 min_freq=1738\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6894 size=4020 all=152952 active=7939 piece=ricket\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6842 size=4040 all=153286 active=8273 piece=▁completely\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6819 size=4060 all=153786 active=8773 piece=▁Max\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6786 size=4080 all=154231 active=9218 piece=▁conver\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6747 size=4100 all=154576 active=9563 piece=▁thir\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6747 min_freq=1695\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6693 size=4120 all=154776 active=7925 piece=alle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6651 size=4140 all=155066 active=8215 piece=aga\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6619 size=4160 all=155436 active=8585 piece=▁Van\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6572 size=4180 all=156108 active=9257 piece=▁northwest\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6517 size=4200 all=156265 active=9414 piece=hew\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6517 min_freq=1657\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6466 size=4220 all=156531 active=8053 piece=tation\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6415 size=4240 all=156970 active=8492 piece=▁Sant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6385 size=4260 all=157415 active=8937 piece=▁Dev\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6345 size=4280 all=157829 active=9351 piece=▁ahead\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6292 size=4300 all=158310 active=9832 piece=▁Lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6292 min_freq=1607\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6260 size=4320 all=158665 active=8205 piece=▁executive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6222 size=4340 all=158825 active=8365 piece=AA\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6183 size=4360 all=159030 active=8570 piece=▁Kr\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6147 size=4380 all=159421 active=8961 piece=▁Av\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6106 size=4400 all=159853 active=9393 piece=▁rang\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6106 min_freq=1570\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6088 size=4420 all=160227 active=8357 piece=▁Daniel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6032 size=4440 all=160639 active=8769 piece=▁Gal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6004 size=4460 all=160895 active=9025 piece=▁1988\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5976 size=4480 all=161133 active=9263 piece=▁Field\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5943 size=4500 all=161342 active=9472 piece=▁risk\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5942 min_freq=1532\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5927 size=4520 all=161586 active=8308 piece=▁Senate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5900 size=4540 all=161989 active=8711 piece=▁sens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5864 size=4560 all=162242 active=8964 piece=ledge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5838 size=4580 all=162706 active=9428 piece=▁Christmas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5795 size=4600 all=162864 active=9586 piece=▁simply\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5793 min_freq=1498\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5769 size=4620 all=163203 active=8483 piece=▁1986\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5747 size=4640 all=163441 active=8721 piece=▁workers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5721 size=4660 all=163686 active=8966 piece=aming\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5685 size=4680 all=164230 active=9510 piece=▁offensive\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5659 size=4700 all=164402 active=9682 piece=▁actors\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5658 min_freq=1466\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5635 size=4720 all=164691 active=8510 piece=▁56\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5606 size=4740 all=165124 active=8943 piece=▁compar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5571 size=4760 all=165732 active=9551 piece=▁deploy\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5546 size=4780 all=165910 active=9729 piece=▁expansion\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5513 size=4800 all=166269 active=10088 piece=▁famous\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5513 min_freq=1430\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5479 size=4820 all=166605 active=8650 piece=ef\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5440 size=4840 all=167326 active=9371 piece=anda\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5405 size=4860 all=167690 active=9735 piece=▁armed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5384 size=4880 all=167892 active=9937 piece=▁diam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5359 size=4900 all=168147 active=10192 piece=▁pet\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5359 min_freq=1398\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: spm_gpt.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: spm_gpt.vocab\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756018738.753873  376420 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 128), dtype=int32, numpy=\n",
       "array([[ 222, 4921,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [  51, 4939, 1266,   18, 3060,   18,  146,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [  91,  220, 4921,  899,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [1017,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New SentencePiece approach\n",
    "print('hello')\n",
    "sp = train_sentencepiece_tokenizer([r'/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt'], vocab_size=5000)\n",
    "VOCAB_SIZE = sp.get_piece_size()  # 5000\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids, attention_mask = tokenize_and_build_token_id_sp(sp, batch_text, CONTEXT_LEN)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa871d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ids_to_text(sp: spm.SentencePieceProcessor, id_list: List[int]) -> str:\n",
    "    \"\"\"Convert list of token IDs back to text string.\"\"\"\n",
    "    return sp.decode_ids(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec9075b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=string, numpy=\n",
       "array([b'A', b'k', b's', b'h', b'a', b't', b' ', b'K', b'h', b'a', b't',\n",
       "       b'r', b'i'], dtype=object)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = tf.constant('Akshat Khatri')\n",
    "tf.strings.bytes_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45984209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello'\n",
      "b'Worlds '\n"
     ]
    }
   ],
   "source": [
    "name = tf.constant(['Hello','Worlds '])\n",
    "for name in name.numpy():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ba567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1.,2.,3.])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "046629d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       " array([[  51, 4939, 1266, ...,    0,    0,    0],\n",
       "        [ 100,  747, 4921, ...,    0,    0,    0],\n",
       "        [ 538,    0,    0, ...,    0,    0,    0]],\n",
       "       shape=(3, 512), dtype=int32)>,\n",
       " <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]], shape=(3, 512), dtype=int32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Akshat Khatri','hello ','me']\n",
    "tokenize_and_build_token_id_sp(sp,name,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d1a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable\n",
    "\n",
    "# class InitializePositionalEmbeddings(keras.layers.Layer): # Receives input of sequence of text\n",
    "#     def __init__(self,d_model: int = 128,sinusoidal_lookup_table = [],token_to_id_dict : tf.lookup.StaticHashTable = {} ,max_seq_len : int = 512,**kwargs): # type: ignore\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.d_model = d_model # d_model\n",
    "#         self.max_seq_len = max_seq_len\n",
    "        \n",
    "#         assert len(sinusoidal_lookup_table) > 0\n",
    "#         assert token_to_id_dict.size().numpy() > 0\n",
    "#         self.VOCAB_SIZE = token_to_id_dict.size().numpy()\n",
    "\n",
    "#         self.pos_table = sinusoidal_lookup_table\n",
    "#         self._embedding_dim = [self.VOCAB_SIZE,d_model]\n",
    "#         self.token_to_id_dict = token_to_id_dict\n",
    "    \n",
    "#     def build(self, input_shape): # this is batch input shape\n",
    "#         print(input_shape)\n",
    "#         self.embedding_matrix = self.add_weight(\n",
    "#             name=\"embedding_matrix\",\n",
    "#             shape=(self.VOCAB_SIZE, self.d_model),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True   # important\n",
    "#         )\n",
    "#         self.input_seq_list = input_shape[-1]\n",
    "\n",
    "#     def call(self,inputs):\n",
    "#         # print(inputs)\n",
    "#         tokens_in_id,non_padded_tokens_mask = tokenize_and_build_token_id(self.token_to_id_dict,inputs)\n",
    "#         # print(tokens_in_id,non_padded_tokens_mask,sep = '\\n')\n",
    "#         token_embeddings = tf.nn.embedding_lookup(self.embedding_matrix, tokens_in_id)\n",
    "#         # Positional embeddings\n",
    "#         seq_len = tf.shape(tokens_in_id)[1] # type: ignore\n",
    "#         pos_embeddings = self.pos_table[:seq_len, :]\n",
    "#         pos_embeddings = tf.expand_dims(pos_embeddings, 0)  # broadcast along batch\n",
    "#         # Add token + position embeddings\n",
    "#         embeddings = token_embeddings + pos_embeddings\n",
    "#         return embeddings,non_padded_tokens_mask\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         base_config = super().get_config()\n",
    "#         return {**base_config,'EMBEDDING_SIZE' : self.EMBEDDING_SIZE,'VOCAB_SIZE' : self.VOCAB_SIZE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d037241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        max_seq_len: int = 128,\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, max_seq_len)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64c431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_id_dict= tokenize_and_build_vocabulary_tf(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt') # Size 94\n",
    "VOCAB_SIZE = sp.get_piece_size()  # Instead of len(token_to_id_dict)\n",
    "D_MODEL = 128\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd66a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL)\n",
    "# batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "# batch_text = tokenize_and_build_token_id(token_to_id_dict,batch_text,MAX_SEQ_LEN) # type: ignore\n",
    "# token_ids,attention_mask = batch_text\n",
    "\n",
    "# layer = InitializePositionalEmbeddings(D_MODEL,VOCAB_SIZE)\n",
    "\n",
    "# @tf.function\n",
    "# def call_some(batch_text):\n",
    "#     embeddings = layer(batch_text)\n",
    "#     return embeddings\n",
    "\n",
    "# call_some(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb6e1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self,attention_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape): # Two tuples -> first tuple is (Batch Shape , Max_seq_length_in_batch,d_model) , Second tuple is (batch , max_seq_len)\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        self.Query_projection = self.add_weight(\n",
    "            name = 'Query_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name = 'Key_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name = 'Value_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "\n",
    "        self.output_projection = self.add_weight(\n",
    "        name=\"Output_projection\",\n",
    "        initializer=\"random_normal\",\n",
    "        shape=(self.d_model, self.d_model),\n",
    "        trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads # type: ignore\n",
    "        \n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self,inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0] # type: ignore\n",
    "        seq_len = tf.shape(embeddings)[1] # type: ignore\n",
    "\n",
    "        Q = embeddings @ self.Query_projection # (seq_len , d_model)\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head Attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        scores = tf.matmul(Q,K, transpose_b=True) # (batch , heads , seq_len,seq_len)\n",
    "        \n",
    "        # 5a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))  # (1,1,L,L)\n",
    "\n",
    "        # 5b. Token mask (B,L) -> (B,1,1,L)\n",
    "        token_mask = tf.cast(token_masks[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "\n",
    "        # 5c. Combine masks\n",
    "        combined_mask = causal_mask * token_mask  # broadcast -> (B, H, L, L)\n",
    "\n",
    "        # 6. Apply mask (replace disallowed with -1e9)\n",
    "        scores = tf.where(combined_mask > 0, scores, tf.constant(-1e9, dtype = scores.dtype))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scores / tf.sqrt(tf.cast(self.d_head, tf.float32)), axis=-1)\n",
    "        context = attention_weights @ V   #(batch, heads, seq_len, seq_len) × (batch, heads, seq_len, d_head) → (batch, heads, seq_len, d_head)\n",
    "        concat_context = tf.reshape(context, (batch_size,seq_len,self.attention_heads * self.d_head))  # type: ignore\n",
    "\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads,})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40a50e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 13, 4, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], shape=(16, 13, 4, 3), dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = np.random.rand(3, 13, 64)  # batch, seq_len, d_model\n",
    "q = np.ones((64, 64))          # d_model × d_model\n",
    "\n",
    "a = tf.constant(a, dtype=tf.float32)\n",
    "q = tf.constant(q, dtype=tf.float32)\n",
    "\n",
    "s = a @ q   # type: ignore # [3, 13, 64]\n",
    "d_head = 16\n",
    "num_heads = 64 // d_head # 4\n",
    "\n",
    "# split into heads\n",
    "s = tf.reshape(s, (3, num_heads, 13, d_head))  # [3, 4, 13, 16]\n",
    "f = tf.constant(np.ones_like(s))\n",
    "\n",
    "tf.transpose(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "873ee087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.95257413, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.04742587, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,9,4,5],[4,5,6,7,8]])\n",
    "keras.activations.softmax(tf.cast(arr,dtype = tf.float32),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb91875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.04742587, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.95257413, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,3,4,5],[4,5,6,7,8]])\n",
    "arr = tf.cast(arr,dtype = tf.float32)\n",
    "\n",
    "tf.nn.softmax(arr,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7cc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000000.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0da4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a994b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, seq_len, input_dim)\n",
    "        output = tf.matmul(inputs, self.kernel) + self.bias  # shape: (batch, seq_len, units))  # shape: (batch, seq_len, units)\n",
    "        return output + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch, seq_len, input_dim)\n",
    "        return (input_shape, input_shape[1], self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cb7c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2,3,4,5,6)\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea053e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd143548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_id_dict= tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt']) # Size 94\n",
    "\n",
    "CONTEXT_LEN = 128\n",
    "D_model = 128\n",
    "VOCAB_SIZE = sp.get_piece_size()  # Instead of len(token_to_id_dict)\n",
    "\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids,attention_mask = tokenize_and_build_token_id_sp(sp,batch_text,CONTEXT_LEN) # type: ignore # Unpacking Values\n",
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3d639c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 128), dtype=int32, numpy=\n",
       " array([[ 222, 4921,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0],\n",
       "        [  51, 4939, 1266,   18, 3060,   18,  146,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0],\n",
       "        [  91,  220, 4921,  899,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0],\n",
       "        [1017,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(4, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_ids,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45b6a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">645,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m640,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (\u001b[38;5;33mDropout\u001b[0m)          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5000\u001b[0m)         │       \u001b[38;5;34m645,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,296</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,076,296\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,296</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,076,296\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build once to get .summary()\n",
    "GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,8,0.00001,4,0.1)\n",
    "_ = GPT_model((token_ids, attention_mask))\n",
    "GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# training (stable): use logits\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-4)\n",
    "GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# inference probs (when you actually need them)\n",
    "logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "probs = keras.ops.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc4eb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 128, 5000), dtype=float32, numpy=\n",
       "array([[[0.00015125, 0.00025029, 0.00020538, ..., 0.00014284,\n",
       "         0.00023571, 0.00020445],\n",
       "        [0.00015819, 0.00023126, 0.00022176, ..., 0.00014168,\n",
       "         0.00023141, 0.00022169],\n",
       "        [0.00016647, 0.00021197, 0.00023165, ..., 0.0001596 ,\n",
       "         0.00021503, 0.00023326],\n",
       "        ...,\n",
       "        [0.00027478, 0.00018127, 0.00017574, ..., 0.00023053,\n",
       "         0.00023281, 0.00019574],\n",
       "        [0.00026411, 0.00017775, 0.00018281, ..., 0.00022458,\n",
       "         0.00023135, 0.0001937 ],\n",
       "        [0.00026058, 0.00018697, 0.00019378, ..., 0.00021447,\n",
       "         0.0002227 , 0.00018922]],\n",
       "\n",
       "       [[0.00013369, 0.00027216, 0.00019796, ..., 0.00014022,\n",
       "         0.00023844, 0.00017291],\n",
       "        [0.00013753, 0.00024946, 0.00020658, ..., 0.00014042,\n",
       "         0.00023516, 0.00017875],\n",
       "        [0.00014337, 0.00022397, 0.00021794, ..., 0.00015489,\n",
       "         0.00021729, 0.00018459],\n",
       "        ...,\n",
       "        [0.00028284, 0.00017448, 0.00017409, ..., 0.00022705,\n",
       "         0.00022841, 0.00020504],\n",
       "        [0.00027345, 0.00017344, 0.00018243, ..., 0.00022057,\n",
       "         0.00022653, 0.00020475],\n",
       "        [0.00026732, 0.00018466, 0.00019362, ..., 0.00020993,\n",
       "         0.0002179 , 0.0001989 ]],\n",
       "\n",
       "       [[0.00014249, 0.00026784, 0.00019574, ..., 0.00014802,\n",
       "         0.00023789, 0.00018419],\n",
       "        [0.0001468 , 0.00023913, 0.00020988, ..., 0.0001519 ,\n",
       "         0.00023306, 0.0001945 ],\n",
       "        [0.00015167, 0.00021973, 0.00021569, ..., 0.00015514,\n",
       "         0.00022097, 0.00020067],\n",
       "        ...,\n",
       "        [0.00027539, 0.00018011, 0.0001782 , ..., 0.00023007,\n",
       "         0.00022446, 0.00019933],\n",
       "        [0.00026562, 0.00017677, 0.00018587, ..., 0.00022477,\n",
       "         0.00022312, 0.00019737],\n",
       "        [0.00026183, 0.0001866 , 0.00019797, ..., 0.00021538,\n",
       "         0.00021381, 0.0001924 ]],\n",
       "\n",
       "       [[0.00014996, 0.0002467 , 0.00021103, ..., 0.00013884,\n",
       "         0.00025025, 0.00021514],\n",
       "        [0.0001633 , 0.00021855, 0.00023349, ..., 0.00014129,\n",
       "         0.00023534, 0.0002336 ],\n",
       "        [0.00016683, 0.00020797, 0.00023937, ..., 0.00015121,\n",
       "         0.00021975, 0.00023806],\n",
       "        ...,\n",
       "        [0.00026608, 0.00018013, 0.00017361, ..., 0.00023259,\n",
       "         0.00022595, 0.00019228],\n",
       "        [0.00025538, 0.00017761, 0.0001808 , ..., 0.0002264 ,\n",
       "         0.00022374, 0.00019039],\n",
       "        [0.00025133, 0.00018711, 0.00019064, ..., 0.00021539,\n",
       "         0.00021546, 0.00018617]]], shape=(4, 128, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81260874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 128, 5000), dtype=float32, numpy=\n",
       "array([[[0.00015125, 0.00025029, 0.00020538, ..., 0.00014284,\n",
       "         0.00023571, 0.00020445],\n",
       "        [0.00015819, 0.00023126, 0.00022176, ..., 0.00014168,\n",
       "         0.00023141, 0.00022169],\n",
       "        [0.00016647, 0.00021197, 0.00023165, ..., 0.0001596 ,\n",
       "         0.00021503, 0.00023326],\n",
       "        ...,\n",
       "        [0.00027478, 0.00018127, 0.00017574, ..., 0.00023053,\n",
       "         0.00023281, 0.00019574],\n",
       "        [0.00026411, 0.00017775, 0.00018281, ..., 0.00022458,\n",
       "         0.00023135, 0.0001937 ],\n",
       "        [0.00026058, 0.00018697, 0.00019378, ..., 0.00021447,\n",
       "         0.0002227 , 0.00018922]],\n",
       "\n",
       "       [[0.00013369, 0.00027216, 0.00019796, ..., 0.00014022,\n",
       "         0.00023844, 0.00017291],\n",
       "        [0.00013753, 0.00024946, 0.00020658, ..., 0.00014042,\n",
       "         0.00023516, 0.00017875],\n",
       "        [0.00014337, 0.00022397, 0.00021794, ..., 0.00015489,\n",
       "         0.00021729, 0.00018459],\n",
       "        ...,\n",
       "        [0.00028284, 0.00017448, 0.00017409, ..., 0.00022705,\n",
       "         0.00022841, 0.00020504],\n",
       "        [0.00027345, 0.00017344, 0.00018243, ..., 0.00022057,\n",
       "         0.00022653, 0.00020475],\n",
       "        [0.00026732, 0.00018466, 0.00019362, ..., 0.00020993,\n",
       "         0.0002179 , 0.0001989 ]],\n",
       "\n",
       "       [[0.00014249, 0.00026784, 0.00019574, ..., 0.00014802,\n",
       "         0.00023789, 0.00018419],\n",
       "        [0.0001468 , 0.00023913, 0.00020988, ..., 0.0001519 ,\n",
       "         0.00023306, 0.0001945 ],\n",
       "        [0.00015167, 0.00021973, 0.00021569, ..., 0.00015514,\n",
       "         0.00022097, 0.00020067],\n",
       "        ...,\n",
       "        [0.00027539, 0.00018011, 0.0001782 , ..., 0.00023007,\n",
       "         0.00022446, 0.00019933],\n",
       "        [0.00026562, 0.00017677, 0.00018587, ..., 0.00022477,\n",
       "         0.00022312, 0.00019737],\n",
       "        [0.00026183, 0.0001866 , 0.00019797, ..., 0.00021538,\n",
       "         0.00021381, 0.0001924 ]],\n",
       "\n",
       "       [[0.00014996, 0.0002467 , 0.00021103, ..., 0.00013884,\n",
       "         0.00025025, 0.00021514],\n",
       "        [0.0001633 , 0.00021855, 0.00023349, ..., 0.00014129,\n",
       "         0.00023534, 0.0002336 ],\n",
       "        [0.00016683, 0.00020797, 0.00023937, ..., 0.00015121,\n",
       "         0.00021975, 0.00023806],\n",
       "        ...,\n",
       "        [0.00026608, 0.00018013, 0.00017361, ..., 0.00023259,\n",
       "         0.00022595, 0.00019228],\n",
       "        [0.00025538, 0.00017761, 0.0001808 , ..., 0.0002264 ,\n",
       "         0.00022374, 0.00019039],\n",
       "        [0.00025133, 0.00018711, 0.00019064, ..., 0.00021539,\n",
       "         0.00021546, 0.00018617]]], shape=(4, 128, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = GPT_model((token_ids, attention_mask))\n",
    "probs = tf.nn.softmax(outputs, axis=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ebf9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">645,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m640,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m197,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m5000\u001b[0m)         │       \u001b[38;5;34m645,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,296</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,076,296\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,296</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,076,296\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e575a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diagram to gpt_model.png\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "try:\n",
    "    plot_model(\n",
    "        GPT_model,\n",
    "        to_file=\"gpt_model.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        expand_nested=True,\n",
    "        dpi=160\n",
    "    )\n",
    "    print(\"Saved diagram to gpt_model.png\")\n",
    "except Exception as e:\n",
    "    print(\"plot_model failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fdd9c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InitializePositionalEmbeddings name=init_embeddings, built=True>,\n",
       " <DecoderBlock name=decoder_block_0, built=True>,\n",
       " <DecoderBlock name=decoder_block_1, built=True>,\n",
       " <DecoderBlock name=decoder_block_2, built=True>,\n",
       " <DecoderBlock name=decoder_block_3, built=True>,\n",
       " <LayerNormalization name=layer_normalization_8, built=True>,\n",
       " <Dense name=Model_head, built=True>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe5fecf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076296"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7809a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               GPT MODEL SUMMARY                                \n",
      "================================================================================\n",
      "Total parameters:      2,076,296\n",
      "Total layers:          7\n",
      "Trainable weights:     53\n",
      "Final output shape(s): ['(unavailable)']\n",
      "--------------------------------------------------------------------------------\n",
      "Idx | Layer Type               | Layer Name              | Weight Name                  | Shape           |   Params\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "000 | InitializePositionalEmbeddings | init_embeddings         | embedding_matrix             | (5000, 128)     |  640,000\n",
      "001 | DecoderBlock             | decoder_block_0         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | Query_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Key_Vector_for_projection    | (128, 128)      |   16,384\n",
      "    |                          |                         | Value_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Output_projection            | (128, 128)      |   16,384\n",
      "    |                          |                         | kernel                       | (128, 512)      |   65,536\n",
      "    |                          |                         | bias                         | (512,)          |      512\n",
      "    |                          |                         | kernel                       | (512, 128)      |   65,536\n",
      "    |                          |                         | bias                         | (128,)          |      128\n",
      "    |                          |                         |                              |                 |         \n",
      "002 | DecoderBlock             | decoder_block_1         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | Query_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Key_Vector_for_projection    | (128, 128)      |   16,384\n",
      "    |                          |                         | Value_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Output_projection            | (128, 128)      |   16,384\n",
      "    |                          |                         | kernel                       | (128, 512)      |   65,536\n",
      "    |                          |                         | bias                         | (512,)          |      512\n",
      "    |                          |                         | kernel                       | (512, 128)      |   65,536\n",
      "    |                          |                         | bias                         | (128,)          |      128\n",
      "    |                          |                         |                              |                 |         \n",
      "003 | DecoderBlock             | decoder_block_2         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | Query_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Key_Vector_for_projection    | (128, 128)      |   16,384\n",
      "    |                          |                         | Value_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Output_projection            | (128, 128)      |   16,384\n",
      "    |                          |                         | kernel                       | (128, 512)      |   65,536\n",
      "    |                          |                         | bias                         | (512,)          |      512\n",
      "    |                          |                         | kernel                       | (512, 128)      |   65,536\n",
      "    |                          |                         | bias                         | (128,)          |      128\n",
      "    |                          |                         |                              |                 |         \n",
      "004 | DecoderBlock             | decoder_block_3         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         | Query_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Key_Vector_for_projection    | (128, 128)      |   16,384\n",
      "    |                          |                         | Value_Vector_for_projection  | (128, 128)      |   16,384\n",
      "    |                          |                         | Output_projection            | (128, 128)      |   16,384\n",
      "    |                          |                         | kernel                       | (128, 512)      |   65,536\n",
      "    |                          |                         | bias                         | (512,)          |      512\n",
      "    |                          |                         | kernel                       | (512, 128)      |   65,536\n",
      "    |                          |                         | bias                         | (128,)          |      128\n",
      "    |                          |                         |                              |                 |         \n",
      "005 | LayerNormalization       | layer_normalization_8   | alpha                        | (128,)          |      128\n",
      "    |                          |                         | beta                         | (128,)          |      128\n",
      "    |                          |                         |                              |                 |         \n",
      "006 | Dense                    | Model_head              | kernel                       | (128, 5000)     |  640,000\n",
      "    |                          |                         | bias                         | (5000,)         |    5,000\n",
      "    |                          |                         |                              |                 |         \n",
      "================================================================================\n",
      "Note: Only trainable weights are listed above. Output shapes may be unavailable\n",
      "for subclassed models or models not built symbolically.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_model_summary(model):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'GPT MODEL SUMMARY':^80}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    total_layers = len(model.layers)\n",
    "    total_weights = sum(len(layer.trainable_weights) for layer in model.layers)\n",
    "    try:\n",
    "        output_shapes = [tuple(out.shape) for out in model.outputs]\n",
    "    except Exception:\n",
    "        output_shapes = [\"(unavailable)\"]\n",
    "    \n",
    "    print(f\"{'Total parameters:':<22} {total_params:,}\")\n",
    "    print(f\"{'Total layers:':<22} {total_layers}\")\n",
    "    print(f\"{'Trainable weights:':<22} {total_weights}\")\n",
    "    print(f\"{'Final output shape(s):':<22} {output_shapes if output_shapes else '(N/A)'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    header = f\"{'Idx':>3} | {'Layer Type':<24} | {'Layer Name':<23} | {'Weight Name':<28} | {'Shape':<15} | {'Params':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_type = layer.__class__.__name__\n",
    "        layer_name = layer.name\n",
    "        weights = layer.trainable_weights\n",
    "        layer_weight_count = len(weights)\n",
    "\n",
    "        if layer_weight_count == 0:\n",
    "            print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {'-':<28} | {'-':<15} | {'0':>8}\")\n",
    "        else:\n",
    "            for j, w in enumerate(weights):\n",
    "                n = int(np.prod(w.shape)) if hasattr(w, \"shape\") else \"?\"\n",
    "                shape_str = str(tuple(w.shape))\n",
    "                weight_name = w.name\n",
    "                if j == 0:\n",
    "                    print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "                else:\n",
    "                    print(f\"    | {'':<24} | {'':<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "        if layer_weight_count > 1:\n",
    "            print(f\"    | {'':<24} | {'':<23} | {'':<28} | {'':<15} | {'':>8}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Note: Only trainable weights are listed above. Output shapes may be unavailable\\n\"\n",
    "          \"for subclassed models or models not built symbolically.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usage:\n",
    "format_model_summary(GPT_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab03dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_book_training_data(book_text: str, \n",
    "#                              token_to_id_dict: Dict[str, int], \n",
    "#                              context_length: int = 512,\n",
    "#                              pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Prepare training data from a Gutenberg book\n",
    "#     \"\"\"\n",
    "#     # 1. Tokenize the entire book\n",
    "#     token_ids = [token_to_id_dict.get(c, pad_value) for c in book_text]\n",
    "    \n",
    "#     # 2. Create sliding windows\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "    \n",
    "#     # Slide window across the entire book\n",
    "#     for i in range(0, len(token_ids) - context_length, context_length):\n",
    "#         # Extract window of context_length + 1 tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "        \n",
    "#         if len(window) < context_length + 1:\n",
    "#             break  # Skip incomplete windows at the end\n",
    "        \n",
    "#         # Create input-target pair\n",
    "#         input_seq = window[:-1]   # [t1, t2, ..., t512]\n",
    "#         target_seq = window[1:]   # [t2, t3, ..., t513]\n",
    "        \n",
    "#         inputs.append(input_seq)\n",
    "#         targets.append(target_seq)\n",
    "    \n",
    "#     # 3. Convert to numpy arrays\n",
    "#     inputs = np.array(inputs, dtype=np.int32)\n",
    "#     targets = np.array(targets, dtype=np.int32)\n",
    "    \n",
    "#     # 4. Create padding masks (all 1s since we're using full context)\n",
    "#     masks = np.ones_like(inputs, dtype=np.int32)\n",
    "    \n",
    "#     return inputs, targets, masks\n",
    "\n",
    "# # Usage:\n",
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r', encoding='utf-8') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea8b1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# print(f\"Total characters: {len(book_text)}\")\n",
    "# print(f\"Expected examples (rough): {len(book_text) // 512}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fc7c7",
   "metadata": {},
   "source": [
    "''' Stop '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "649d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to TensorFlow tensors\n",
    "# input_ids = tf.constant(inputs, dtype=tf.int32)\n",
    "# target_ids = tf.constant(targets, dtype=tf.int32)\n",
    "# attention_masks = tf.constant(masks, dtype=tf.int32)\n",
    "\n",
    "# model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,8,0.00001,4,0.1,sinusoidal_lookup_table)\n",
    "\n",
    "# # Compile your model\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Train with .fit()\n",
    "# history = model.fit(\n",
    "#     x=[input_ids, ,  # Your model expects (token_ids, attention_mask)\n",
    "#     y=target_ids,\n",
    "#     batch_size=16,  # Start small since 677 examples isn't huge\n",
    "#     epochs=50,\n",
    "#     validation_split=0.1,  # Use 10% for validation\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),\n",
    "#         keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e775cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_text, token_to_id, id_to_token, max_len=100, temperature=1.0):\n",
    "#     # Encode start text\n",
    "#     token_ids, attention_mask = tokenize_and_build_token_id(\n",
    "#         token_to_id, [start_text], max_seq_len=512\n",
    "#     )\n",
    "    \n",
    "#     generated = list(token_ids[0].numpy())  # flatten out\n",
    "#     mask = list(attention_mask[0].numpy())\n",
    "    \n",
    "#     for _ in range(max_len):\n",
    "#         # Trim to last 512 tokens\n",
    "#         x_tokens = np.array([generated[-512:]])\n",
    "#         x_mask   = np.array([mask[-512:]])\n",
    "\n",
    "#         # Forward pass with both inputs\n",
    "#         logits = model((x_tokens, x_mask), training=False)\n",
    "\n",
    "#         # Take last position logits\n",
    "#         next_logits = logits[0, -1] / temperature\n",
    "#         probs = tf.nn.softmax(next_logits).numpy()\n",
    "\n",
    "#         # Sample next token\n",
    "#         next_id = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "#         # Append\n",
    "#         generated.append(next_id)\n",
    "#         mask.append(1)  # mark as valid token\n",
    "\n",
    "#     # Decode\n",
    "#     return ''.join(id_to_token[i] for i in generated if i in id_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d50c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_token_dict = {v: k for k, v in token_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43978b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = \"Akshat Khatri\"\n",
    "# result = generate_text(model, start, token_to_id_dict, id_to_token_dict, max_len=100, temperature=0.8)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94cf8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-v1')# Concatenate train + validation + test\n",
    "# all_texts = []\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     all_texts.extend(dataset[split][\"text\"])\n",
    "\n",
    "# # Remove empty lines\n",
    "# all_texts = [t.strip() for t in all_texts if t.strip() != \"\"]\n",
    "\n",
    "# # Join into one giant string\n",
    "# big_text = \"\\n\".join(all_texts)\n",
    "\n",
    "# # Write to file\n",
    "# with open(\"wikitext_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(big_text)\n",
    "\n",
    "# print(\"Saved dataset to wikitext_full.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c759f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f51f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming you already have:\n",
    "# # - prepare_book_training_data()\n",
    "# # - token_to_id_dict\n",
    "\n",
    "# file_path = r\"/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\"\n",
    "\n",
    "# inputs_list, targets_list, masks_list = [], [], []\n",
    "\n",
    "# buffer = \"\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         buffer += line.strip() + \" \"\n",
    "#         # Process every ~5000 chars to avoid memory spike\n",
    "#         if len(buffer) > 5000:\n",
    "#             inp, tgt, msk = prepare_book_training_data(\n",
    "#                 buffer, token_to_id_dict, context_length=512\n",
    "#             )\n",
    "#             inputs_list.append(inp)\n",
    "#             targets_list.append(tgt)\n",
    "#             masks_list.append(msk)\n",
    "#             buffer = \"\"  # reset buffer\n",
    "\n",
    "# # Process any leftover buffer\n",
    "# if buffer.strip():\n",
    "#     inp, tgt, msk = prepare_book_training_data(\n",
    "#         buffer, token_to_id_dict, context_length=512\n",
    "#     )\n",
    "#     inputs_list.append(inp)\n",
    "#     targets_list.append(tgt)\n",
    "#     masks_list.append(msk)\n",
    "\n",
    "# # Concatenate all batches into final arrays\n",
    "# inputs = np.concatenate(inputs_list, axis=0)\n",
    "# targets = np.concatenate(targets_list, axis=0)\n",
    "# masks = np.concatenate(masks_list, axis=0)\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")\n",
    "# print(f\"Masks shape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b06fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from typing import Dict, List\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "#     \"\"\"\n",
    "#     Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "#     Args:\n",
    "#         input_ids: Token sequence for model input\n",
    "#         target_ids: Token sequence for model targets (shifted by 1)\n",
    "#         attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "#     Returns:\n",
    "#         Serialized TFRecord example\n",
    "#     \"\"\"\n",
    "#     feature = {\n",
    "#         'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "#         'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "#         'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "#     }\n",
    "    \n",
    "#     example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#     return example.SerializeToString()\n",
    "\n",
    "\n",
    "# def convert_text_to_tfrecord(\n",
    "#     text_file_path: str,\n",
    "#     token_to_id_dict: Dict[str, int],\n",
    "#     output_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     records_per_file: int = 1000,\n",
    "#     pad_value: int = 0\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Convert text file to TFRecord files for GPT training.\n",
    "    \n",
    "#     Process:\n",
    "#     1. Read and tokenize entire text file\n",
    "#     2. Create sliding windows of context_length + 1 tokens\n",
    "#     3. Split each window into input/target pairs (shifted by 1)\n",
    "#     4. Save as TFRecord files with specified number of records per file\n",
    "    \n",
    "#     Args:\n",
    "#         text_file_path: Path to your text file (e.g., WikiText-103)\n",
    "#         token_to_id_dict: Character-to-ID mapping dictionary\n",
    "#         output_dir: Directory to save TFRecord files\n",
    "#         context_length: Sequence length for training\n",
    "#         records_per_file: Number of examples per TFRecord file\n",
    "#         pad_value: Token ID used for unknown characters\n",
    "        \n",
    "#     Returns:\n",
    "#         Path to output directory containing TFRecord files\n",
    "#     \"\"\"\n",
    "#     # Setup\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     print(f\"Reading text file: {text_file_path}\")\n",
    "    \n",
    "#     # Step 1: Load and tokenize text\n",
    "#     with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "#         text = f.read()\n",
    "    \n",
    "#     print(f\"Text length: {len(text):,} characters\")\n",
    "    \n",
    "#     # Convert each character to token ID\n",
    "#     print(\"Tokenizing text...\")\n",
    "#     token_ids = [token_to_id_dict.get(char, pad_value) for char in text]\n",
    "#     print(f\"Token length: {len(token_ids):,} tokens\")\n",
    "    \n",
    "#     # Step 2: Calculate output size\n",
    "#     num_examples = (len(token_ids) - context_length) // context_length\n",
    "#     print(f\"Will create {num_examples:,} training examples\")\n",
    "    \n",
    "#     # Step 3: Process sliding windows and write TFRecord files\n",
    "#     file_count = 0\n",
    "#     examples_in_current_file = 0\n",
    "#     writer = None\n",
    "    \n",
    "#     print(\"Creating TFRecord files...\")\n",
    "    \n",
    "#     # Slide window across token sequence\n",
    "#     for i in tqdm(range(0, len(token_ids) - context_length, context_length)):\n",
    "        \n",
    "#         # Extract window of tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "#         if len(window) < context_length + 1:\n",
    "#             break\n",
    "            \n",
    "#         # Create input-target pair (GPT training format)\n",
    "#         input_ids = window[:-1]    # First 512 tokens: [t1, t2, ..., t512]\n",
    "#         target_ids = window[1:]    # Shifted by 1: [t2, t3, ..., t513]\n",
    "#         attention_mask = [1] * context_length  # All valid tokens (no padding)\n",
    "        \n",
    "#         # Start new TFRecord file if needed\n",
    "#         if writer is None or examples_in_current_file >= records_per_file:\n",
    "#             if writer is not None:\n",
    "#                 writer.close()\n",
    "            \n",
    "#             tfrecord_filename = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#             writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "#             file_count += 1\n",
    "#             examples_in_current_file = 0\n",
    "        \n",
    "#         # Write training example to current TFRecord file\n",
    "#         tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#         writer.write(tf_example)\n",
    "#         examples_in_current_file += 1\n",
    "    \n",
    "#     # Cleanup\n",
    "#     if writer is not None:\n",
    "#         writer.close()\n",
    "    \n",
    "#     # Step 4: Save summary information\n",
    "#     print(f\"\\nConversion complete!\")\n",
    "#     print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "#     print(f\"Total examples: {num_examples:,}\")\n",
    "    \n",
    "#     # Write metadata file\n",
    "#     metadata = {\n",
    "#         'context_length': context_length,\n",
    "#         'vocab_size': len(token_to_id_dict),\n",
    "#         'num_examples': num_examples,\n",
    "#         'num_files': file_count,\n",
    "#         'records_per_file': records_per_file\n",
    "#     }\n",
    "    \n",
    "#     metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         for key, value in metadata.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "#     print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "#     return output_dir\n",
    "\n",
    "\n",
    "# def create_tf_data_pipeline(\n",
    "#     tfrecord_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     batch_size: int = 32,\n",
    "#     shuffle_buffer: int = 1000,\n",
    "#     prefetch_buffer: int = tf.data.AUTOTUNE\n",
    "# ) -> tf.data.Dataset:\n",
    "#     \"\"\"\n",
    "#     Create tf.data pipeline from TFRecord files for training.\n",
    "    \n",
    "#     Process:\n",
    "#     1. Find all TFRecord files in directory\n",
    "#     2. Create dataset that reads and parses TFRecord examples\n",
    "#     3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "#     Args:\n",
    "#         tfrecord_dir: Directory containing TFRecord files\n",
    "#         context_length: Expected sequence length in records\n",
    "#         batch_size: Number of examples per training batch\n",
    "#         shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "#         prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "#     Returns:\n",
    "#         tf.data.Dataset ready for model.fit()\n",
    "#     \"\"\"\n",
    "#     # Step 1: Find TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     # Step 2: Define how to parse each TFRecord example\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_tfrecord_example(example_proto):\n",
    "#         \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "#         # Parse the serialized example\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         # Convert to correct data types\n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "#         model_inputs = (input_ids, attention_mask)\n",
    "#         model_targets = target_ids\n",
    "        \n",
    "#         return model_inputs, model_targets\n",
    "    \n",
    "#     # Step 3: Create and configure dataset pipeline\n",
    "#     dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "#     dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(shuffle_buffer)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# # Example usage (commented out)\n",
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Example usage for WikiText-103 or similar large text files\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "#     # example_vocab = your_token_to_id_dict\n",
    "    \n",
    "#     # # Step 2: Convert text to TFRecord format (run once)\n",
    "#     # tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     #     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     #     token_to_id_dict=example_vocab,\n",
    "#     #     output_dir='./tfrecords',\n",
    "#     #     context_length=512,\n",
    "#     #     records_per_file=1000\n",
    "#     # )\n",
    "    \n",
    "#     # # Step 3: Create training pipeline (use for training)\n",
    "#     # train_dataset = create_tf_data_pipeline(\n",
    "#     #     tfrecord_dir=tfrecord_dir,\n",
    "#     #     context_length=512,\n",
    "#     #     batch_size=16\n",
    "#     # )\n",
    "    \n",
    "#     # print(\"TFRecord pipeline ready for training!\")\n",
    "#     # # Now you can use train_dataset with your model's .fit() method\n",
    "    \n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6ab8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and tokenizing text in chunks from: /home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\n",
      "Loaded SentencePiece model from: /home/akshat/GPT_from_scratch/notebooks/spm_gpt.model\n",
      "Vocabulary size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text:   0%|          | 0.00/538M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 538M/538M [09:31<00:00, 942kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete!\n",
      "Created 1515 TFRecord files in: ./tfrecords\n",
      "Approximate total examples: 1515000\n",
      "Metadata saved to: ./tfrecords/metadata.txt\n",
      "SentencePiece TFRecord pipeline ready for training!\n",
      "Vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "    \"\"\"\n",
    "    Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token sequence for model input\n",
    "        target_ids: Token sequence for model targets (shifted by 1)\n",
    "        attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "    Returns:\n",
    "        Serialized TFRecord example\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "        'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "        'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_text_to_tfrecord_sp(\n",
    "    text_file_path: str,\n",
    "    sp_model_path: str,\n",
    "    output_dir: str,\n",
    "    context_length: int = 512,\n",
    "    records_per_file: int = 1000,\n",
    "    overlap_size: int = 64,\n",
    "    chunk_size: int = 100_000  # Number of characters read at a time\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Streaming version of text to TFRecord conversion with SentencePiece.\n",
    "    Reads and tokenizes file incrementally to limit memory usage.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_size = os.path.getsize(text_file_path)\n",
    "    print(f\"Reading and tokenizing text in chunks from: {text_file_path}\")\n",
    "    \n",
    "    # Load SentencePiece processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(sp_model_path)\n",
    "    \n",
    "    print(f\"Loaded SentencePiece model from: {sp_model_path}\")\n",
    "    print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "\n",
    "    buffer_tokens = []\n",
    "    step_size = context_length - overlap_size\n",
    "    file_count = 0\n",
    "    examples_in_current_file = 0\n",
    "    writer = None\n",
    "    \n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc='Processing text') as pbar:\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                buffer_tokens.extend(sp.encode_as_ids(chunk))\n",
    "                pbar.update(len(chunk.encode('utf-8')))  # update by bytes read\n",
    "                \n",
    "                # Process windows to create examples\n",
    "                while len(buffer_tokens) >= context_length + 1:\n",
    "                    window = buffer_tokens[:context_length + 1]\n",
    "                    input_ids = window[:-1]\n",
    "                    target_ids = window[1:]\n",
    "                    attention_mask = [1] * context_length\n",
    "                    \n",
    "                    if writer is None or examples_in_current_file >= records_per_file:\n",
    "                        if writer:\n",
    "                            writer.close()\n",
    "                        tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "                        writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "                        file_count += 1\n",
    "                        examples_in_current_file = 0\n",
    "                    \n",
    "                    tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "                    writer.write(tf_example)\n",
    "                    examples_in_current_file += 1\n",
    "                    \n",
    "                    # Slide window forward by step_size tokens\n",
    "                    buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "    # Process any remaining tokens\n",
    "    while len(buffer_tokens) >= context_length + 1:\n",
    "        window = buffer_tokens[:context_length + 1]\n",
    "        input_ids = window[:-1]\n",
    "        target_ids = window[1:]\n",
    "        attention_mask = [1] * context_length\n",
    "        \n",
    "        if writer is None or examples_in_current_file >= records_per_file:\n",
    "            if writer:\n",
    "                writer.close()\n",
    "            tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "            writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "            file_count += 1\n",
    "            examples_in_current_file = 0\n",
    "\n",
    "        tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "        writer.write(tf_example)\n",
    "        examples_in_current_file += 1\n",
    "        buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    num_examples = file_count * records_per_file  # Approximate count\n",
    "\n",
    "    print(f\"\\nConversion complete!\")\n",
    "    print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "    print(f\"Approximate total examples: {num_examples}\")\n",
    "\n",
    "    metadata = {\n",
    "        'context_length': context_length,\n",
    "        'vocab_size': sp.get_piece_size(),\n",
    "        'num_examples': num_examples,\n",
    "        'num_files': file_count,\n",
    "        'records_per_file': records_per_file,\n",
    "        'overlap_size': overlap_size,\n",
    "        'sp_model_path': sp_model_path,\n",
    "        'tokenization': 'SentencePiece'\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "\n",
    "def create_tf_data_pipeline_sp(\n",
    "    tfrecord_dir: str,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    shuffle_buffer: int = 1000,\n",
    "    prefetch_buffer: int = tf.data.AUTOTUNE\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create tf.data pipeline from TFRecord files for training (SentencePiece version).\n",
    "    \n",
    "    Process:\n",
    "    1. Find all TFRecord files in directory\n",
    "    2. Create dataset that reads and parses TFRecord examples\n",
    "    3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_dir: Directory containing TFRecord files\n",
    "        context_length: Expected sequence length in records\n",
    "        batch_size: Number of examples per training batch\n",
    "        shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "        prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "    Returns:\n",
    "        tf.data.Dataset ready for model.fit()\n",
    "    \"\"\"\n",
    "    # Step 1: Find TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    if not tfrecord_files:\n",
    "        raise FileNotFoundError(f\"No TFRecord files found in {tfrecord_dir}\")\n",
    "    \n",
    "    # Step 2: Define how to parse each TFRecord example\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_tfrecord_example(example_proto):\n",
    "        \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "        # Parse the serialized example\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Convert to correct data types\n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "        model_inputs = (input_ids, attention_mask)\n",
    "        model_targets = target_ids\n",
    "        \n",
    "        return model_inputs, model_targets\n",
    "    \n",
    "    # Step 3: Create and configure dataset pipeline\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example usage assuming you have a pre-trained SentencePiece model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your pre-trained SentencePiece model path\n",
    "    sp_model_path = r'/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # You provide this\n",
    "    \n",
    "    # Step 1: Convert text to TFRecords using your pre-trained model\n",
    "    tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "        text_file_path=r'/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt',\n",
    "        sp_model_path=sp_model_path,  # Your trained model\n",
    "        output_dir='./tfrecords',\n",
    "        context_length=CONTEXT_LEN,  # Match your CONTEXT_LEN\n",
    "        records_per_file=1000,\n",
    "        overlap_size=32,\n",
    "        chunk_size = 120000\n",
    "    )\n",
    "    \n",
    "    # # Step 2: Create training pipeline\n",
    "    # train_dataset = create_tf_data_pipeline_sp(\n",
    "    #     tfrecord_dir=tfrecord_dir,\n",
    "    #     context_length=128,  # Match your CONTEXT_LEN\n",
    "    #     batch_size=16\n",
    "    # )\n",
    "    \n",
    "    print(\"SentencePiece TFRecord pipeline ready for training!\")\n",
    "    \n",
    "    # Step 3: Load your SentencePiece model for vocab size and generation\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(sp_model_path)\n",
    "    VOCAB_SIZE = sp.get_piece_size()\n",
    "    \n",
    "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "    # Now you can use:\n",
    "    # - sp for tokenization in generation\n",
    "    # - train_dataset for model.fit()\n",
    "    # - VOCAB_SIZE for your model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "# example_vocab = token_to_id_dict\n",
    "\n",
    "# # Step 2: Convert text to TFRecord format (run once)\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=example_vocab,\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=128,\n",
    "#     records_per_file=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caffa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Create training and validation pipeline (use for training)\n",
    "# def create_train_val_datasets(tfrecord_dir: str, \n",
    "#                              context_length: int,\n",
    "#                              batch_size: int = 16,\n",
    "#                              val_split: float = 0.1):\n",
    "#     \"\"\"\n",
    "#     Create training and validation datasets from TFRecord files\n",
    "#     \"\"\"\n",
    "#     # Find all TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     # Split files for train/val\n",
    "#     num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "#     val_files = tfrecord_files[:num_val_files]\n",
    "#     train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "#     print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "#     # Feature description\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_function(example_proto):\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "#         return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "#     # Create training dataset\n",
    "#     train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "#     train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "#     train_dataset = train_dataset.batch(batch_size)\n",
    "#     train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     # Create validation dataset\n",
    "#     val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "#     val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     val_dataset = val_dataset.batch(batch_size)\n",
    "#     val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fb97b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_datasets_sp(tfrecord_dir: str, \n",
    "                                context_length: int,\n",
    "                                batch_size: int = 16,\n",
    "                                val_split: float = 0.1):\n",
    "    \"\"\"\n",
    "    Create training and validation datasets from TFRecord files (SentencePiece version)\n",
    "    \"\"\"\n",
    "    # Find all TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Split files for train/val\n",
    "    num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "    val_files = tfrecord_files[:num_val_files]\n",
    "    train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "    print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "    # Feature description\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_function(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "        return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "    train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "    val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def prepare_tfrecords_sp(\n",
    "    text_file_path: str,\n",
    "    sp_model_path: str,\n",
    "    context_length: int = 128,\n",
    "    records_per_file: int = 1000,\n",
    "    output_base_dir: str = './tfrecords',\n",
    "    version_name: str = None,\n",
    "    batch_size: int = 16,\n",
    "    val_split: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create TFRecords from text using SentencePiece and return train/val datasets.\n",
    "    Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "    Args:\n",
    "        text_file_path: Path to input text file.\n",
    "        sp_model_path: Path to trained SentencePiece model (.model file).\n",
    "        context_length: Sequence length for training.\n",
    "        records_per_file: Number of examples per TFRecord file.\n",
    "        output_base_dir: Base folder to store TFRecords.\n",
    "        version_name: Optional unique folder name. If None, uses context_length.\n",
    "        batch_size: Batch size for dataset.\n",
    "        val_split: Fraction of data to use as validation.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, steps_per_epoch, vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    # Load SentencePiece model to get vocab size\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(sp_model_path)\n",
    "    vocab_size = sp.get_piece_size()\n",
    "\n",
    "    # Determine output folder\n",
    "    if version_name is None:\n",
    "        version_name = f\"sp_context_{context_length}_bs{batch_size}_vocab{vocab_size}\"\n",
    "    output_dir = os.path.join(output_base_dir, version_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert text to TFRecords using SentencePiece\n",
    "    tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "        text_file_path=text_file_path,\n",
    "        sp_model_path=sp_model_path,\n",
    "        output_dir=output_dir,\n",
    "        context_length=context_length,\n",
    "        records_per_file=records_per_file\n",
    "    )\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset, val_dataset = create_train_val_datasets_sp(\n",
    "        tfrecord_dir=tfrecord_dir,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        val_split=val_split\n",
    "    )\n",
    "\n",
    "    # Calculate steps per epoch\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    total_examples = len(tfrecord_files) * records_per_file\n",
    "    train_examples = int(total_examples * (1 - val_split))\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "    print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Train examples: {train_examples}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    return train_dataset, val_dataset, steps_per_epoch, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64a8c3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1515 TFRecord files\n",
      "Using 1364 files for training, 151 for validation\n",
      "train done\n",
      "steps per epoch are 85218\n"
     ]
    }
   ],
   "source": [
    "# Usage with SentencePiece\n",
    "sp_model_path = '/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # Your pre-trained SentencePiece model\n",
    "\n",
    "# train_ds_16, val_ds_16, steps_16, VOCAB_SIZE = prepare_tfrecords_sp(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt',\n",
    "#     sp_model_path=sp_model_path,  # Your trained SentencePiece model\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16\n",
    "# )\n",
    "train_ds_16, val_ds_16 = create_train_val_datasets_sp(\n",
    "    tfrecord_dir=r'/home/akshat/GPT_from_scratch/notebooks/tfrecords',\n",
    "    context_length=CONTEXT_LEN,\n",
    "    batch_size= 16\n",
    ")\n",
    "print('train done')\n",
    "# Calculate steps per epoch\n",
    "tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "total_examples = len(tfrecord_files) * 1000 # max_records per file = 1000\n",
    "train_examples = int(total_examples * (1 - 0.1)) # Validation - split = 0.1\n",
    "steps_per_epoch = train_examples // 16 # batch size = 16\n",
    "print(f'steps per epoch are {steps_per_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DATASETS\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"TFRecord pipeline ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4819661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Remove existing tfrecords directory\n",
    "# if os.path.exists('./tfrecords'):\n",
    "#     shutil.rmtree('./tfrecords')\n",
    "#     print(\"Removed existing tfrecords directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SOLUTION 1: Recreate TFRecords with correct context length\n",
    "# # Delete the existing tfrecords directory and recreate with CONTEXT_LEN\n",
    "\n",
    "\n",
    "# # Recreate with correct context length\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,  # Make sure this variable is defined\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # Use your actual context length (128)\n",
    "#     records_per_file=1000\n",
    "# )\n",
    "\n",
    "# # Now create datasets with matching context length\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # This will now match\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "# # Calculate steps per epoch\n",
    "# records_per_file = 1000  \n",
    "# tfrecord_files = tf.io.gfile.glob(os.path.join(\"./tfrecords\", \"*.tfrecord\"))\n",
    "# total_examples = len(tfrecord_files) * records_per_file\n",
    "# train_examples = int(total_examples * 0.9)\n",
    "# steps_per_epoch = train_examples // 16\n",
    "\n",
    "# print(f\"Files: {len(tfrecord_files)}\")\n",
    "# print(f\"Total examples: {total_examples}\")\n",
    "# print(f\"Steps per epoch: {steps_per_epoch}\") # When combined with batch_size sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a189899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def prepare_tfrecords(\n",
    "#     text_file_path: str,\n",
    "#     token_to_id_dict: dict,\n",
    "#     context_length: int = 128,\n",
    "#     records_per_file: int = 1000,\n",
    "#     output_base_dir: str = './tfrecords',\n",
    "#     version_name: str = None,\n",
    "#     batch_size: int = 16,\n",
    "#     val_split: float = 0.1\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create TFRecords from text and return train/val datasets.\n",
    "#     Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "#     Args:\n",
    "#         text_file_path: Path to input text file.\n",
    "#         token_to_id_dict: Character-to-id dictionary.\n",
    "#         context_length: Sequence length for training.\n",
    "#         records_per_file: Number of examples per TFRecord file.\n",
    "#         output_base_dir: Base folder to store TFRecords.\n",
    "#         version_name: Optional unique folder name. If None, uses context_length.\n",
    "#         batch_size: Batch size for dataset.\n",
    "#         val_split: Fraction of data to use as validation.\n",
    "\n",
    "#     Returns:\n",
    "#         train_dataset, val_dataset, steps_per_epoch\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Determine output folder\n",
    "#     if version_name is None:\n",
    "#         version_name = f\"context_{context_length}_bs{batch_size}\"\n",
    "#     output_dir = os.path.join(output_base_dir, version_name)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Convert text to TFRecords\n",
    "#     tfrecord_dir = convert_text_to_tfrecord(\n",
    "#         text_file_path=text_file_path,\n",
    "#         token_to_id_dict=token_to_id_dict,\n",
    "#         output_dir=output_dir,\n",
    "#         context_length=context_length,\n",
    "#         records_per_file=records_per_file\n",
    "#     )\n",
    "\n",
    "#     # Create train/val datasets\n",
    "#     train_dataset, val_dataset = create_train_val_datasets(\n",
    "#         tfrecord_dir=tfrecord_dir,\n",
    "#         context_length=context_length,\n",
    "#         batch_size=batch_size,\n",
    "#         val_split=val_split\n",
    "#     )\n",
    "\n",
    "#     # Calculate steps per epoch\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     total_examples = len(tfrecord_files) * records_per_file\n",
    "#     train_examples = int(total_examples * (1 - val_split))\n",
    "#     steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "#     print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "#     print(f\"Total examples: {total_examples}\")\n",
    "#     print(f\"Train examples: {train_examples}\")\n",
    "#     print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "#     return train_dataset, val_dataset, steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9722d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 128\n",
    "D_MODEL = 128\n",
    "# VOCAB_SIZE now comes from SentencePiece (e.g., 2000 instead of 94)\n",
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "\n",
    "# Create model with new vocab size\n",
    "model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "\n",
    "# Compile your model (same as before)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a331a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 07:26:11.593421: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:390] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-08-24 07:26:11.744039: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f7dc800a5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-24 07:26:11.744080: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-08-24 07:26:11.976124: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-24 07:26:12.584623: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-24 07:26:13.206225: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91200\n",
      "2025-08-24 07:26:15.965064: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 1292 bytes spill stores, 1260 bytes spill loads\n",
      "\n",
      "2025-08-24 07:26:16.410895: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-08-24 07:26:16.597942: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10307', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-24 07:26:16.601438: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7991', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1756020388.457428  376718 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85186/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5219 - loss: 3.0893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 07:43:01.737736: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-24 07:43:04.380246: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-08-24 07:43:04.383910: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7991', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-24 07:43:04.559652: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10307', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-24 07:43:04.674863: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 1292 bytes spill stores, 1260 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85189/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5219 - loss: 3.0893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 07:43:14.667764: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-08-24 07:43:14.667826: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-08-24 07:43:15.513034: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-24 07:44:00.608715: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 0.64427, saving model to checkpoints/sp_model_epoch_01.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 07:44:05.015947: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1084s\u001b[0m 12ms/step - accuracy: 0.6978 - loss: 1.8360 - val_accuracy: 0.8940 - val_loss: 0.6443 - learning_rate: 1.0000e-04\n",
      "Epoch 2/250\n",
      "\u001b[1m85185/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8953 - loss: 0.6031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:01:05.621275: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 08:01:05.621336: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.64427 to 0.39706, saving model to checkpoints/sp_model_epoch_02.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:01:45.187131: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1059s\u001b[0m 12ms/step - accuracy: 0.9059 - loss: 0.5378 - val_accuracy: 0.9335 - val_loss: 0.3971 - learning_rate: 1.0000e-04\n",
      "Epoch 3/250\n",
      "\u001b[1m85189/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9359 - loss: 0.3608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:18:58.583716: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n",
      "2025-08-24 08:18:58.583790: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 08:18:58.583821: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 0.39706 to 0.16813, saving model to checkpoints/sp_model_epoch_03.keras\n",
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1071s\u001b[0m 13ms/step - accuracy: 0.9469 - loss: 0.2899 - val_accuracy: 0.9708 - val_loss: 0.1681 - learning_rate: 1.0000e-04\n",
      "Epoch 4/250\n",
      "\u001b[1m85186/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9651 - loss: 0.1884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:36:30.844488: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 08:36:30.844540: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.16813 to 0.15760, saving model to checkpoints/sp_model_epoch_04.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:37:08.685705: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-08-24 08:37:08.685768: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n",
      "2025-08-24 08:37:08.685775: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 08:37:08.685808: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1053s\u001b[0m 12ms/step - accuracy: 0.9670 - loss: 0.1815 - val_accuracy: 0.9736 - val_loss: 0.1576 - learning_rate: 1.0000e-04\n",
      "Epoch 5/250\n",
      "\u001b[1m85189/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9695 - loss: 0.1721\n",
      "Epoch 5: val_loss improved from 0.15760 to 0.15641, saving model to checkpoints/sp_model_epoch_05.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 08:54:05.210457: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1016s\u001b[0m 12ms/step - accuracy: 0.9698 - loss: 0.1703 - val_accuracy: 0.9737 - val_loss: 0.1564 - learning_rate: 1.0000e-04\n",
      "Epoch 6/250\n",
      "\u001b[1m85189/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9714 - loss: 0.1649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 09:09:46.836382: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n",
      "2025-08-24 09:09:46.837810: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 09:09:46.837850: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss improved from 0.15641 to 0.15556, saving model to checkpoints/sp_model_epoch_06.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 09:10:28.486794: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m983s\u001b[0m 12ms/step - accuracy: 0.9715 - loss: 0.1642 - val_accuracy: 0.9738 - val_loss: 0.1556 - learning_rate: 1.0000e-04\n",
      "Epoch 7/250\n",
      "\u001b[1m85187/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9732 - loss: 0.1547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 09:26:36.092823: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n",
      "2025-08-24 09:26:36.093675: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15307511684714498086\n",
      "2025-08-24 09:26:36.093720: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 5955961210079712758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.15556 to 0.11633, saving model to checkpoints/sp_model_epoch_07.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 09:27:16.374365: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6069378436476908133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85218/85218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1008s\u001b[0m 12ms/step - accuracy: 0.9754 - loss: 0.1404 - val_accuracy: 0.9804 - val_loss: 0.1163 - learning_rate: 1.0000e-04\n",
      "Epoch 8/250\n",
      "\u001b[1m  222/85218\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:58\u001b[0m 11ms/step - accuracy: 0.9783 - loss: 0.1244"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Now training should work with SentencePiece tokenization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds_16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds_16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoints/sp_model_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{epoch:02d}\u001b[39;49;00m\u001b[33;43m.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCSVLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msp_training_log.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs_sp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhistogram_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprofile_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:221\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m ):\n\u001b[32m    220\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:321\u001b[39m, in \u001b[36m_EagerTensorBase.__bool__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m   x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray):\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(x.size > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:406\u001b[39m, in \u001b[36m_EagerTensorBase._numpy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> npt.ArrayLike:\n\u001b[32m    405\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Now training should work with SentencePiece tokenization\n",
    "history = model.fit(\n",
    "    train_ds_16,\n",
    "    validation_data=val_ds_16,\n",
    "    epochs=250,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='checkpoints/sp_model_epoch_{epoch:02d}.keras',\n",
    "            save_freq='epoch',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=10, \n",
    "            restore_best_weights=True, \n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=10, \n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.CSVLogger('sp_training_log.csv'),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=\"./logs_sp\", \n",
    "            histogram_freq=1, \n",
    "            profile_batch=0,\n",
    "            write_graph=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset,val_dataset,steps_per_epoch = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d131044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_24, val_ds_24, steps_24 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=24\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_32, val_ds_32, steps_32 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_44, val_ds_44, steps_44 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=44\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9933902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file: /home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt\n",
      "Text length: 537,161,800 characters\n",
      "Tokenizing text...\n",
      "Token length: 537,161,800 tokens\n",
      "Will create 4,196,575 training examples\n",
      "Creating TFRecord files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4196576/4196576 [02:18<00:00, 30371.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete!\n",
      "Created 4197 TFRecord files in: ./tfrecords/context_128_bs64\n",
      "Total examples: 4,196,575\n",
      "Metadata saved to: ./tfrecords/context_128_bs64/metadata.txt\n",
      "Found 4197 TFRecord files\n",
      "Using 3778 files for training, 419 for validation\n",
      "TFRecord folder: ./tfrecords/context_128_bs64\n",
      "Total examples: 4197000\n",
      "Train examples: 3777300\n",
      "Steps per epoch: 59020\n"
     ]
    }
   ],
   "source": [
    "# train_ds_64, val_ds_64, steps_64 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT_LEN = 128\n",
    "# D_model = 128\n",
    "# VOCAB_SIZE = len(token_to_id_dict) # 94 currently char level\n",
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)\n",
    "\n",
    "# model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "\n",
    "# # Compile your model (same as before)\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now training should work\n",
    "# history = model.fit(\n",
    "#     train_ds_64,\n",
    "#     validation_data=val_ds_64,\n",
    "#     epochs=2,\n",
    "#     steps_per_epoch=steps_64,\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint(filepath='model_epoch_{epoch:02d}.keras',save_freq='epoch'),  # saves with epoch number   save_freq='epoch',                        # save every epoch    save_best_only=False,                     # save all epochs    verbose=1)\n",
    "#         keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, verbose=1),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, verbose=1),\n",
    "#         keras.callbacks.CSVLogger('training_log.csv'),\n",
    "#         keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1, profile_batch=0)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45dbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Positional embeddings are working. Shape: (1, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "# pick a small dummy batch\n",
    "import tensorflow as tf\n",
    "\n",
    "dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)  # batch_size=1, length=CONTEXT_LEN\n",
    "dum_model.save('model_epoch_1.keras')\n",
    "# run the embeddings layer only\n",
    "pos_layer = dum_model.get_layer('init_embeddings')  # or however your layer is named\n",
    "try:\n",
    "    pos_emb = pos_layer(dummy_input)\n",
    "    print(\"✅ Positional embeddings are working. Shape:\", pos_emb.shape)\n",
    "except Exception as e:\n",
    "    print(\"❌ Embedding test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b33b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'final_gpt_model_sentence_piece.keras'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtraining_history.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     pickle.dump(\u001b[43mhistory\u001b[49m.history, f)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining history saved as \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining_history_sentence_piece.pkl\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Step 5: Load model later (when needed)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the final model\n",
    "model.save('final_gpt_model_sentence_piece.keras')\n",
    "print(\"Model saved as 'final_gpt_model_sentence_piece.keras'\")\n",
    "\n",
    "# Optional: Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved as 'training_history_sentence_piece.pkl'\")\n",
    "\n",
    "# Step 5: Load model later (when needed)\n",
    "def load_trained_model():\n",
    "    \"\"\"Load your saved model\"\"\"\n",
    "    loaded_model = keras.models.load_model('best_model_sentence_piece.keras')  # or 'final_gpt_model.keras'\n",
    "    return loaded_model\n",
    "\n",
    "# Usage for inference later:\n",
    "# model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ce64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257939/2192788961.py:82: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:5714\n",
      "* Running on public URL: https://e2d87bd39408007117.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e2d87bd39408007117.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import keras\n",
    "\n",
    "# Load trained GPT model\n",
    "model = keras.models.load_model(r'f/home/akshat/GPT_from_scratch/notebooks/checkpoints/sp_model_epoch_04.keras')\n",
    "\n",
    "# Load your pre-trained SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model')  # Path to your .model file\n",
    "\n",
    "CONTEXT_LEN = 128  # Should match your model's context length\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_k=10):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize prompt with SentencePiece\n",
    "    input_tokens = sp.encode_as_ids(prompt)\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Prepare inputs\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        logits = model((input_ids, attention_mask), training=False)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample next token with top-k\n",
    "        next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        \n",
    "        # Stop on padding token id or EOS token id (adjust if needed)\n",
    "        if next_token == sp.pad_id() or next_token == sp.eos_id():\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = sp.decode_ids(new_tokens)\n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k)\n",
    "    history.append((message, bot_response))\n",
    "    return \"\", history\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"My Generative Pre-trained transformer (GPT) Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Akshat's My GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with SentencePiece tokenizer. And NO i DON'T Use any API calls or local LLM's Akshat pre-trained me on wikipedia data from scratch , so be gentle :)\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=10, maximum=200, value=100, step=10, label=\"Max Length\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=10, step=1, label=\"Top-K Sampling\")\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6000,\n",
    "        show_error=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d11a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sinusoidal_lookup_table.shape)  # should be (CONTEXT_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67921403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive testing for YOUR custom model implementation...\n",
      "\n",
      "🔍 Testing sinusoidal lookup table creation...\n",
      "✅ Sinusoidal lookup table created successfully. Shape: (128, 128)\n",
      "   Table type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "   Table dtype: <dtype: 'float32'>\n",
      "\n",
      "🔍 Testing InitializePositionalEmbeddings layer...\n",
      "❌ Positional embeddings test failed: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing LayerNormalization layer...\n",
      "✅ LayerNormalization working. Input shape: (2, 128, 128), Output shape: (2, 128, 128)\n",
      "   Output mean (should be ~0): 0.000000\n",
      "   Output variance (should be ~1): 0.999990\n",
      "\n",
      "🔍 Testing YOUR SelfAttentionLayer...\n",
      "   Input embeddings shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "   Mask sample - seq 1: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "✅ SelfAttentionLayer working. Output shape: (2, 128, 128)\n",
      "   Output range: [-0.7532, 0.7810]\n",
      "   Output mean: 0.0010\n",
      "   Output std: 0.0782\n",
      "   Attention heads: 8\n",
      "   d_head: 16\n",
      "   d_model: 128\n",
      "\n",
      "🔍 Testing YOUR DecoderBlock...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 29, in test_positional_embeddings\n",
      "    pos_layer = InitializePositionalEmbeddings(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_206177/3965528913.py\", line 11, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Input shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "✅ DecoderBlock working (training=False). Output shape: (2, 128, 128)\n",
      "✅ DecoderBlock working (training=True). Output shape: (2, 128, 128)\n",
      "   Input mean: 0.0003, Output mean: -0.0041\n",
      "   Output range: [-4.3376, 4.7553]\n",
      "\n",
      "🔍 Testing YOUR complete GPT model...\n",
      "❌ Full model test failed: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing the specific embedding issue from your original code...\n",
      "❌ Original embedding test still fails: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n",
      "\n",
      "======================================================================\n",
      "🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\n",
      "======================================================================\n",
      "Sinusoidal Lookup Table........................... ✅ PASSED\n",
      "Positional Embeddings............................. ❌ FAILED\n",
      "Layer Normalization............................... ✅ PASSED\n",
      "YOUR Self Attention Layer......................... ✅ PASSED\n",
      "YOUR Decoder Block................................ ✅ PASSED\n",
      "YOUR Full Model Forward Pass...................... ❌ FAILED\n",
      "YOUR Original Embedding Issue..................... ❌ FAILED\n",
      "Model Compilation & Training...................... ❌ FAILED\n",
      "\n",
      "🏆 Overall: 4/8 tests passed\n",
      "⚠️  Some tests failed. Please fix the issues before training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 186, in test_your_full_model\n",
      "    model = GPT(\n",
      "            ^^^^\n",
      "  File \"/tmp/ipykernel_206177/1791215164.py\", line 64, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/models/model.py\", line 158, in __init__\n",
      "    Layer.__init__(self, *args, **kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 255, in test_specific_embedding_issue\n",
      "    dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "CONTEXT_LEN = 128\n",
    "D_MODEL = 128\n",
    "VOCAB_SIZE = 94\n",
    "\n",
    "def test_sinusoidal_lookup_table():\n",
    "    \"\"\"Test the sinusoidal lookup table creation\"\"\"\n",
    "    print(\"🔍 Testing sinusoidal lookup table creation...\")\n",
    "    try:\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        print(f\"✅ Sinusoidal lookup table created successfully. Shape: {sinusoidal_lookup_table.shape}\")\n",
    "        print(f\"   Table type: {type(sinusoidal_lookup_table)}\")\n",
    "        print(f\"   Table dtype: {sinusoidal_lookup_table.dtype}\")\n",
    "        return sinusoidal_lookup_table\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sinusoidal lookup table creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_positional_embeddings(sinusoidal_lookup_table):\n",
    "    \"\"\"Test the positional embeddings layer\"\"\"\n",
    "    print(\"\\n🔍 Testing InitializePositionalEmbeddings layer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create the layer\n",
    "        pos_layer = InitializePositionalEmbeddings(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_pos_embeddings\"\n",
    "        )\n",
    "        \n",
    "        # Test input\n",
    "        dummy_input = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Positional embeddings working. Output shape: {pos_emb.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {D_MODEL})\")\n",
    "        \n",
    "        # Check if embeddings are reasonable\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(pos_emb):.4f}\")\n",
    "        \n",
    "        return pos_layer, pos_emb\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Positional embeddings test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_layer_normalization():\n",
    "    \"\"\"Test LayerNormalization layer\"\"\"\n",
    "    print(\"\\n🔍 Testing LayerNormalization layer...\")\n",
    "    \n",
    "    try:\n",
    "        ln = LayerNormalization(eps=1e-5, name=\"test_ln\")\n",
    "        test_input = tf.random.normal((2, CONTEXT_LEN, D_MODEL))\n",
    "        \n",
    "        output = ln(test_input)\n",
    "        print(f\"✅ LayerNormalization working. Input shape: {test_input.shape}, Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check normalization properties\n",
    "        mean = tf.reduce_mean(output, axis=-1)\n",
    "        var = tf.reduce_mean(tf.square(output - tf.expand_dims(mean, -1)), axis=-1)\n",
    "        print(f\"   Output mean (should be ~0): {tf.reduce_mean(mean):.6f}\")\n",
    "        print(f\"   Output variance (should be ~1): {tf.reduce_mean(var):.6f}\")\n",
    "        \n",
    "        return ln, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LayerNormalization test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_self_attention():\n",
    "    \"\"\"Test YOUR custom SelfAttentionLayer\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR SelfAttentionLayer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your attention layer with correct parameter name\n",
    "        attn_layer = SelfAttentionLayer(attention_heads=8, name=\"test_attention\")\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        embeddings = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        # Make some positions masked (set to 0)\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 50:] = 0  # Mask second half of first sequence\n",
    "        attention_mask[1, 80:] = 0  # Mask last part of second sequence\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample - seq 1: {attention_mask[0, :10].numpy()} ... {attention_mask[0, -10:].numpy()}\")\n",
    "        \n",
    "        # Test with your layer's expected input format: (embeddings, mask)\n",
    "        output = attn_layer([embeddings, attention_mask])\n",
    "        print(f\"✅ SelfAttentionLayer working. Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(output):.4f}\")\n",
    "        print(f\"   Output std: {tf.math.reduce_std(output):.4f}\")\n",
    "        \n",
    "        # Verify attention heads are working\n",
    "        print(f\"   Attention heads: {attn_layer.attention_heads}\")\n",
    "        print(f\"   d_head: {attn_layer.d_head}\")\n",
    "        print(f\"   d_model: {attn_layer.d_model}\")\n",
    "        \n",
    "        return attn_layer, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ SelfAttentionLayer test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_decoder_block():\n",
    "    \"\"\"Test YOUR custom DecoderBlock\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR DecoderBlock...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your decoder block\n",
    "        decoder = DecoderBlock(\n",
    "            d_model=D_MODEL,\n",
    "            n_heads=8,\n",
    "            dropout_rate=0.1,\n",
    "            epsilon=1e-5,\n",
    "            name=\"test_decoder\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        test_input = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        \n",
    "        # Make some positions masked\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 60:] = 0\n",
    "        attention_mask[1, 90:] = 0\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input shape: {test_input.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Test training=False\n",
    "        output = decoder(test_input, attention_mask, training=False)\n",
    "        print(f\"✅ DecoderBlock working (training=False). Output shape: {output.shape}\")\n",
    "        \n",
    "        # Test training=True\n",
    "        output_train = decoder(test_input, attention_mask, training=True)\n",
    "        print(f\"✅ DecoderBlock working (training=True). Output shape: {output_train.shape}\")\n",
    "        \n",
    "        # Check residual connections work (output should be different from input)\n",
    "        input_mean = tf.reduce_mean(test_input)\n",
    "        output_mean = tf.reduce_mean(output)\n",
    "        print(f\"   Input mean: {input_mean:.4f}, Output mean: {output_mean:.4f}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        \n",
    "        return decoder, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ DecoderBlock test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_full_model(sinusoidal_lookup_table):\n",
    "    \"\"\"Test YOUR complete GPT model\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR complete GPT model...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you do\n",
    "        model = GPT(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            context_length=CONTEXT_LEN,\n",
    "            attention_heads=8,\n",
    "            epsilon=1e-5,\n",
    "            decoder_blocks=4,\n",
    "            dropout_rate=0.1,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_gpt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        token_ids = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        attention_mask = tf.ones((1, CONTEXT_LEN), dtype=tf.float32)\n",
    "        \n",
    "        # Set mask to 0 for padding tokens\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 5:] = 0  # Only first 5 tokens are real\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Token IDs shape: {token_ids.shape}\")\n",
    "        print(f\"   Token IDs sample: {token_ids[0, :10].numpy()}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample: {attention_mask[0, :10].numpy()}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        logits = model([token_ids, attention_mask], training=False)\n",
    "        print(f\"✅ Full model forward pass successful!\")\n",
    "        print(f\"   Output logits shape: {logits.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {VOCAB_SIZE})\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Logits range: [{tf.reduce_min(logits):.4f}, {tf.reduce_max(logits):.4f}]\")\n",
    "        print(f\"   Logits mean: {tf.reduce_mean(logits):.4f}\")\n",
    "        \n",
    "        # Test with different batch size\n",
    "        token_ids_batch = tf.constant([\n",
    "            [1, 2, 3] + [0] * (CONTEXT_LEN - 3),\n",
    "            [4, 5, 6, 7] + [0] * (CONTEXT_LEN - 4)\n",
    "        ], dtype=tf.int32)\n",
    "        attention_mask_batch = tf.ones((2, CONTEXT_LEN), dtype=tf.float32)\n",
    "        attention_mask_batch = attention_mask_batch.numpy()\n",
    "        attention_mask_batch[0, 3:] = 0  # First seq has 3 real tokens\n",
    "        attention_mask_batch[1, 4:] = 0  # Second seq has 4 real tokens\n",
    "        attention_mask_batch = tf.constant(attention_mask_batch)\n",
    "        \n",
    "        logits_batch = model([token_ids_batch, attention_mask_batch], training=False)\n",
    "        print(f\"✅ Batch processing successful! Output shape: {logits_batch.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        logits_train = model([token_ids, attention_mask], training=True)\n",
    "        print(f\"✅ Training mode successful! Output shape: {logits_train.shape}\")\n",
    "        \n",
    "        return model, logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Full model test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_specific_embedding_issue():\n",
    "    \"\"\"Test the specific embedding issue you encountered\"\"\"\n",
    "    print(\"\\n🔍 Testing the specific embedding issue from your original code...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you did\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
    "        \n",
    "        # Test exactly as you did\n",
    "        dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)\n",
    "        \n",
    "        # Get the embeddings layer\n",
    "        pos_layer = dum_model.get_layer('init_embeddings')\n",
    "        \n",
    "        # Run the embeddings layer\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Your original embedding test now works! Shape: {pos_emb.shape}\")\n",
    "        print(f\"   Input was all zeros: {dummy_input[0, :5].numpy()}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Original embedding test still fails: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_model_compilation_and_training(model):\n",
    "    \"\"\"Test model compilation and training capability\"\"\"\n",
    "    print(\"\\n🔍 Testing model compilation and training...\")\n",
    "    \n",
    "    try:\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.AdamW(learning_rate=1e-4),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"✅ Model compilation successful!\")\n",
    "        \n",
    "        # Create dummy training data\n",
    "        batch_size = 4\n",
    "        dummy_x = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        dummy_mask = tf.ones((batch_size, CONTEXT_LEN), dtype=tf.float32)\n",
    "        # Create some realistic masking\n",
    "        for i in range(batch_size):\n",
    "            seq_len = tf.random.uniform([], minval=10, maxval=CONTEXT_LEN, dtype=tf.int32)\n",
    "            dummy_mask = dummy_mask.numpy()\n",
    "            dummy_mask[i, seq_len:] = 0\n",
    "            dummy_mask = tf.constant(dummy_mask)\n",
    "        \n",
    "        dummy_y = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        \n",
    "        print(f\"   Training data shapes: X={dummy_x.shape}, mask={dummy_mask.shape}, Y={dummy_y.shape}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        predictions = model.predict([dummy_x, dummy_mask], verbose=0)\n",
    "        print(f\"✅ Model prediction successful! Predictions shape: {predictions.shape}\")\n",
    "        \n",
    "        # Test training step\n",
    "        loss = model.train_on_batch([dummy_x, dummy_mask], dummy_y)\n",
    "        print(f\"✅ Training step successful! Loss: {loss}\")\n",
    "        \n",
    "        # Test model summary\n",
    "        print(f\"\\n📊 Model has {model.count_params():,} parameters\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model compilation/training test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def run_all_tests_for_your_model():\n",
    "    \"\"\"Run all tests specifically for your model implementation\"\"\"\n",
    "    print(\"🚀 Starting comprehensive testing for YOUR custom model implementation...\\n\")\n",
    "    \n",
    "    # Test 1: Sinusoidal lookup table\n",
    "    sinusoidal_lookup_table = test_sinusoidal_lookup_table()\n",
    "    if sinusoidal_lookup_table is None:\n",
    "        print(\"❌ Cannot proceed without sinusoidal lookup table\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Positional embeddings\n",
    "    pos_layer, pos_emb = test_positional_embeddings(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 3: Layer normalization\n",
    "    ln_layer, ln_output = test_layer_normalization()\n",
    "    \n",
    "    # Test 4: Your self attention\n",
    "    attn_layer, attn_output = test_your_self_attention()\n",
    "    \n",
    "    # Test 5: Your decoder block\n",
    "    decoder_layer, decoder_output = test_your_decoder_block()\n",
    "    \n",
    "    # Test 6: Your full model\n",
    "    model, logits = test_your_full_model(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 7: Your specific embedding issue\n",
    "    embedding_issue_fixed = test_specific_embedding_issue()\n",
    "    \n",
    "    # Test 8: Model compilation and training\n",
    "    compilation_success = False\n",
    "    if model is not None:\n",
    "        compilation_success = test_model_compilation_and_training(model)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Sinusoidal Lookup Table\", sinusoidal_lookup_table is not None),\n",
    "        (\"Positional Embeddings\", pos_emb is not None),\n",
    "        (\"Layer Normalization\", ln_output is not None),\n",
    "        (\"YOUR Self Attention Layer\", attn_output is not None),\n",
    "        (\"YOUR Decoder Block\", decoder_output is not None),\n",
    "        (\"YOUR Full Model Forward Pass\", logits is not None),\n",
    "        (\"YOUR Original Embedding Issue\", embedding_issue_fixed),\n",
    "        (\"Model Compilation & Training\", compilation_success)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, result in tests if result)\n",
    "    total = len(tests)\n",
    "    \n",
    "    for test_name, result in tests:\n",
    "        status = \"✅ PASSED\" if result else \"❌ FAILED\"\n",
    "        print(f\"{test_name:.<50} {status}\")\n",
    "    \n",
    "    print(f\"\\n🏆 Overall: {passed}/{total} tests passed\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"🎉 All tests passed! Your model is working perfectly!\")\n",
    "        print(\"🚀 Your model is ready for training and inference!\")\n",
    "    elif passed >= total - 2:\n",
    "        print(\"🎊 Almost all tests passed! Your model is mostly working correctly!\")\n",
    "        print(\"🔧 Check the failed tests above for minor issues.\")\n",
    "    else:\n",
    "        print(\"⚠️  Some tests failed. Please fix the issues before training.\")\n",
    "    \n",
    "    return model if logits is not None else None\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    final_model = run_all_tests_for_your_model()\n",
    "    \n",
    "    if final_model is not None:\n",
    "        print(f\"\\n🎁 Model returned successfully!\")\n",
    "        print(f\"   Total parameters: {final_model.count_params():,}\")\n",
    "        print(f\"   Ready for: training, inference, and saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_bot(model, token_to_id_dict, prompt, max_length=100, temperature=0.7, context_len=128):\n",
    "    \"\"\"Generate text response from your GPT model\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_tokens = [token_to_id_dict.get(char, 0) for char in prompt]\n",
    "    \n",
    "    # Handle context length\n",
    "    if len(input_tokens) > context_len:\n",
    "        input_tokens = input_tokens[-context_len:]\n",
    "    \n",
    "    # Pad to context length\n",
    "    input_ids = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        input_ids[-len(input_tokens):] = input_tokens\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        attention_mask[-len(input_tokens):] = 1\n",
    "    \n",
    "    # Prepare for model\n",
    "    input_ids = np.expand_dims(input_ids, axis=0)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    \n",
    "    # Generate response token by token\n",
    "    generated_tokens = input_tokens.copy()\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        \n",
    "        # Get last token logits (find the last non-zero position in attention mask)\n",
    "        last_pos = np.sum(attention_mask[0]) - 1\n",
    "        if last_pos < 0:\n",
    "            last_pos = 0\n",
    "        next_token_logits = predictions[0, last_pos, :] / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "        \n",
    "        # Stop if we hit a stop token or newline\n",
    "        if next_token == 0 or (next_token in token_to_id_dict.values() and id_to_token[next_token] == '\\n'):\n",
    "            break\n",
    "            \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Update input for next iteration\n",
    "        if len(generated_tokens) > context_len:\n",
    "            generated_tokens = generated_tokens[-context_len:]\n",
    "        \n",
    "        # Create new input\n",
    "        new_input_ids = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        \n",
    "        new_attention_mask = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        input_ids = new_input_ids\n",
    "        attention_mask = new_attention_mask\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    response_tokens = generated_tokens[len(input_tokens):]  # Only the new tokens\n",
    "    response = ''.join([id_to_token.get(token, '') for token in response_tokens])\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b200e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"]8'Vi3A;#sFöXöxr3ö][xöM6!dlwx—$pb:Orxx1JkW0:pöyyö;94œ!ööGHQöG:‘::$fwrg3Rg!R!/gxrgg/PöJIYPlö6öJ%6RpLp\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_from_bot(model,token_to_id_dict,prompt = 'yoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
