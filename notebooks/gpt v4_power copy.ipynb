{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62adc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:52:56.496219: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-27 12:52:56.663127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-27 12:52:58.912136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b8fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007f70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str] = [r\"/home/akshat/GPT_from_scratch/text_data/pg76702.txt\"],\n",
    "#     existing_vocab: Dict[str, int] = None # type: ignore\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TensorFlow lookup tables.\n",
    "    \n",
    "#     Returns:\n",
    "#         token_to_id_table: tf.lookup.StaticHashTable mapping char -> int\n",
    "#         id_to_token_table: tf.lookup.StaticHashTable mapping int -> char\n",
    "#     \"\"\"\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "\n",
    "#     # Collect characters from all files\n",
    "#     for file_name in file_path_list:\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     # Sort for consistency\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "\n",
    "#     # Assign IDs (keep existing IDs if possible)\n",
    "#     token_to_id = {token: i for i, token in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "#     # Convert dicts to tensors\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()))\n",
    "#     token_values = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_values = tf.constant(list(id_to_token.values()))\n",
    "\n",
    "#     # Create TensorFlow lookup tables\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_values),\n",
    "#         default_value=-1  # unknown token\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_values),\n",
    "#         default_value=\"\"  # unknown ID\n",
    "#     )\n",
    "\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9856e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import List, Dict, Tuple\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str],\n",
    "#     existing_vocab: Dict[str, int] | None = None\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TF lookup tables:\n",
    "#       token_to_id: char -> int\n",
    "#       id_to_token: int -> char\n",
    "#     \"\"\"\n",
    "#     if isinstance(file_path_list, (str, bytes)):\n",
    "#         file_path_list = [file_path_list] # type: ignore\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "#     for file_name in file_path_list:\n",
    "#         if os.path.isdir(file_name):\n",
    "#             raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "#         if not os.path.isfile(file_name):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "#     token_to_id = {tok: i for i, tok in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()), dtype=tf.string)\n",
    "#     token_vals = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_vals = tf.constant(list(id_to_token.values()), dtype=tf.string)\n",
    "\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_vals),\n",
    "#         default_value=-1\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_vals),\n",
    "#         default_value=tf.constant(\"\", dtype=tf.string)\n",
    "#     )\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_token_id(token_to_id_table: tf.lookup.StaticHashTable,\n",
    "#                                 text_batch: tf.Tensor,\n",
    "#                                 pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Tokenize a batch of strings character by character, pad sequences,\n",
    "#     and return attention masks.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_table: TF lookup table mapping char -> int\n",
    "#         text_batch: tf.Tensor of shape [batch_size], dtype=tf.string\n",
    "#         pad_value: int, ID to use for padding\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: tf.Tensor [batch_size, max_seq_len]\n",
    "#         attention_mask: tf.Tensor [batch_size, max_seq_len]\n",
    "#     \"\"\"\n",
    "#     token_ids_list = []\n",
    "\n",
    "#     for text in text_batch.numpy():  # type: ignore\n",
    "#         # Convert bytes to TF string\n",
    "\n",
    "#         # Split into characters\n",
    "#         char_tensor = tf.strings.bytes_split(text)\n",
    "\n",
    "#         # Lookup token IDs\n",
    "#         token_ids = token_to_id_table.lookup(char_tensor)\n",
    "\n",
    "#         token_ids_list.append(token_ids)\n",
    "\n",
    "#     # Pad all sequences to the same length\n",
    "#     token_ids_padded = tf.ragged.stack(token_ids_list).to_tensor(default_value=pad_value) # type: ignore\n",
    "#     # Create attention mask: 1 for real tokens, 0 for padding\n",
    "#     attention_mask = tf.cast(token_ids_padded != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids_padded, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import Tuple\n",
    "\n",
    "\n",
    "# def tokenize_and_build_token_id(\n",
    "#     token_to_id_dict: dict,\n",
    "#     text_batch: list[str],\n",
    "#     max_seq_len: int,\n",
    "#     pad_value: int = 0,\n",
    "#     unk_value: int = None\n",
    "# ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "#     \"\"\"\n",
    "#     TensorFlow-compatible tokenization converting batch of strings to char token IDs,\n",
    "#     padded/truncated to max_seq_len, along with attention mask.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_dict: dict mapping character str -> int ID\n",
    "#         text_batch: list of strings to tokenize\n",
    "#         max_seq_len: max length to pad/truncate sequences\n",
    "#         pad_value: int ID for padding tokens\n",
    "#         unk_value: int ID for unknown tokens; if None, uses pad_value\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: (batch_size, max_seq_len) tf.int32 tensor of token IDs\n",
    "#         attention_mask: (batch_size, max_seq_len) tf.int32 tensor (1 for tokens, 0 for padding)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if unk_value is None:\n",
    "#         unk_value = pad_value\n",
    "\n",
    "#     # Create lookup table from token_to_id_dict\n",
    "#     keys = tf.constant(list(token_to_id_dict.keys()))\n",
    "#     values = tf.constant(list(token_to_id_dict.values()), dtype=tf.int32)\n",
    "#     table = tf.lookup.StaticHashTable(\n",
    "#         tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "#         default_value=unk_value\n",
    "#     )\n",
    "\n",
    "#     # Convert text batch to a RaggedTensor of chars\n",
    "#     rt_chars = tf.strings.unicode_split(text_batch, 'UTF-8')  # shape: [batch_size, (seq_len)]\n",
    "\n",
    "#     # Lookup token IDs for each char\n",
    "#     token_ids = table.lookup(rt_chars)\n",
    "\n",
    "#     # Pad or truncate sequences to max_seq_len\n",
    "#     token_ids = token_ids.to_tensor(default_value=pad_value, shape=[None, max_seq_len])\n",
    "#     token_ids = token_ids[:, :max_seq_len]  # truncate if longer\n",
    "\n",
    "#     # Construct attention mask: 1 where not pad_value, else 0\n",
    "#     attention_mask = tf.cast(token_ids != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e3a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def train_sentencepiece_tokenizer(file_path_list: List[str], \n",
    "                                vocab_size: int = 2000,\n",
    "                                model_prefix: str = 'spm_gpt') -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizer from text files.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]\n",
    "    \n",
    "    # Validate files\n",
    "    for file_name in file_path_list:\n",
    "        if os.path.isdir(file_name):\n",
    "            raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "    \n",
    "    # Show file info\n",
    "    for file_path in file_path_list:\n",
    "        size_mb = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"Processing file: {file_path} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    input_files = ','.join(file_path_list)\n",
    "    \n",
    "    # Add progress info\n",
    "    print(f\"Starting SentencePiece training with {vocab_size} vocab size...\")\n",
    "    print(\"This may take 5-15 minutes for large files. Please wait...\")\n",
    "    \n",
    "    # Train SentencePiece model\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_files,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe',\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "        num_threads=8\n",
    "    )\n",
    "    \n",
    "    print(f\"Training complete! Model saved as {model_prefix}.model\")\n",
    "    \n",
    "    # Load and return processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'{model_prefix}.model')\n",
    "    \n",
    "    print(f\"Actual vocabulary size: {sp.get_piece_size()}\")\n",
    "    return sp\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id_sp(sp: spm.SentencePieceProcessor, \n",
    "                                 text_batch: List[str], \n",
    "                                 max_seq_len: int, \n",
    "                                 pad_value: int = 0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize batch of text using SentencePiece (replaces tokenize_and_build_token_id).\n",
    "    \n",
    "    Args:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "        text_batch: List of text strings to tokenize.\n",
    "        max_seq_len: Maximum sequence length after padding/truncation.\n",
    "        pad_value: Integer ID used for padding tokens (should match sp.pad_id()).\n",
    "    \n",
    "    Returns:\n",
    "        token_ids: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "        attention_mask: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "    \"\"\"\n",
    "    batch_token_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        # Encode text to subword IDs\n",
    "        ids = sp.encode_as_ids(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[-max_seq_len:]  # Keep the end (recent context)\n",
    "        else:\n",
    "            # Pad to max_seq_len\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_token_ids.append(ids)\n",
    "    \n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    \n",
    "    return tf.constant(token_ids), tf.constant(attention_mask) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4fcfa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me\n"
     ]
    }
   ],
   "source": [
    "print('me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9a2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Loading pre-trained SentencePiece model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756299183.263869  568712 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 256), dtype=int32, numpy=\n",
       "array([[  58, 7974,    0, ...,    0,    0,    0],\n",
       "       [7970,    1,  306, ...,    0,    0,    0],\n",
       "       [7970,    1,  254, ...,    0,    0,    0],\n",
       "       [7970,    1, 7971, ...,    0,    0,    0]],\n",
       "      shape=(4, 256), dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New SentencePiece approach - Load existing model\n",
    "print('hello')\n",
    "print('Loading pre-trained SentencePiece model...')\n",
    "\n",
    "# Just load the model (no training)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model')\n",
    "\n",
    "VOCAB_SIZE = sp.get_piece_size()  \n",
    "CONTEXT_LEN = 256\n",
    "\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids, attention_mask = tokenize_and_build_token_id_sp(sp, batch_text, CONTEXT_LEN)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa871d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ids_to_text(sp: spm.SentencePieceProcessor, id_list: List[int]) -> str:\n",
    "    \"\"\"Convert list of token IDs back to text string.\"\"\"\n",
    "    return sp.decode_ids(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9075b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=string, numpy=\n",
       "array([b'A', b'k', b's', b'h', b'a', b't', b' ', b'K', b'h', b'a', b't',\n",
       "       b'r', b'i'], dtype=object)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = tf.constant('Akshat Khatri')\n",
    "tf.strings.bytes_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45984209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello'\n",
      "b'Worlds '\n"
     ]
    }
   ],
   "source": [
    "name = tf.constant(['Hello','Worlds '])\n",
    "for name in name.numpy():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ba567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1.,2.,3.])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "046629d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       " array([[7970,    1,  306, ...,    0,    0,    0],\n",
       "        [6579,    0,    0, ...,    0,    0,    0],\n",
       "        [ 112,    0,    0, ...,    0,    0,    0]],\n",
       "       shape=(3, 512), dtype=int32)>,\n",
       " <tf.Tensor: shape=(3, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]], shape=(3, 512), dtype=int32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Akshat Khatri','hello ','me']\n",
    "tokenize_and_build_token_id_sp(sp,name,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d1a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable\n",
    "\n",
    "# class InitializePositionalEmbeddings(keras.layers.Layer): # Receives input of sequence of text\n",
    "#     def __init__(self,d_model: int = 128,sinusoidal_lookup_table = [],token_to_id_dict : tf.lookup.StaticHashTable = {} ,max_seq_len : int = 512,**kwargs): # type: ignore\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.d_model = d_model # d_model\n",
    "#         self.max_seq_len = max_seq_len\n",
    "        \n",
    "#         assert len(sinusoidal_lookup_table) > 0\n",
    "#         assert token_to_id_dict.size().numpy() > 0\n",
    "#         self.VOCAB_SIZE = token_to_id_dict.size().numpy()\n",
    "\n",
    "#         self.pos_table = sinusoidal_lookup_table\n",
    "#         self._embedding_dim = [self.VOCAB_SIZE,d_model]\n",
    "#         self.token_to_id_dict = token_to_id_dict\n",
    "    \n",
    "#     def build(self, input_shape): # this is batch input shape\n",
    "#         print(input_shape)\n",
    "#         self.embedding_matrix = self.add_weight(\n",
    "#             name=\"embedding_matrix\",\n",
    "#             shape=(self.VOCAB_SIZE, self.d_model),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True   # important\n",
    "#         )\n",
    "#         self.input_seq_list = input_shape[-1]\n",
    "\n",
    "#     def call(self,inputs):\n",
    "#         # print(inputs)\n",
    "#         tokens_in_id,non_padded_tokens_mask = tokenize_and_build_token_id(self.token_to_id_dict,inputs)\n",
    "#         # print(tokens_in_id,non_padded_tokens_mask,sep = '\\n')\n",
    "#         token_embeddings = tf.nn.embedding_lookup(self.embedding_matrix, tokens_in_id)\n",
    "#         # Positional embeddings\n",
    "#         seq_len = tf.shape(tokens_in_id)[1] # type: ignore\n",
    "#         pos_embeddings = self.pos_table[:seq_len, :]\n",
    "#         pos_embeddings = tf.expand_dims(pos_embeddings, 0)  # broadcast along batch\n",
    "#         # Add token + position embeddings\n",
    "#         embeddings = token_embeddings + pos_embeddings\n",
    "#         return embeddings,non_padded_tokens_mask\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         base_config = super().get_config()\n",
    "#         return {**base_config,'EMBEDDING_SIZE' : self.EMBEDDING_SIZE,'VOCAB_SIZE' : self.VOCAB_SIZE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d037241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        max_seq_len: int = 512, # Change this if any changes happen\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, max_seq_len)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        \n",
    "        pos_emb = tf.cast(pos_emb, token_emb.dtype)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_id_dict= tokenize_and_build_vocabulary_tf(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt') # Size 94\n",
    "\n",
    "VOCAB_SIZE = sp.get_piece_size()  # Instead of len(token_to_id_dict)\n",
    "D_MODEL = 384 # 2.5x increase\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd66a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL)\n",
    "# batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "# batch_text = tokenize_and_build_token_id(token_to_id_dict,batch_text,MAX_SEQ_LEN) # type: ignore\n",
    "# token_ids,attention_mask = batch_text\n",
    "\n",
    "# layer = InitializePositionalEmbeddings(D_MODEL,VOCAB_SIZE)\n",
    "\n",
    "# @tf.function\n",
    "# def call_some(batch_text):\n",
    "#     embeddings = layer(batch_text)\n",
    "#     return embeddings\n",
    "\n",
    "# call_some(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb6e1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape): # Two tuples -> first tuple is (Batch Shape , Max_seq_length_in_batch,d_model) , Second tuple is (batch , max_seq_len)\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        self.Query_projection = self.add_weight(\n",
    "            name='glorot_uniform',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name='Key_Vector_for_projection',\n",
    "            initializer='glorot_uniform',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name='Value_Vector_for_projection',\n",
    "            initializer='glorot_uniform',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "\n",
    "        self.output_projection = self.add_weight(\n",
    "            name=\"Output_projection\",\n",
    "            initializer='glorot_uniform',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads # type: ignore\n",
    "        \n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0] # type: ignore\n",
    "        seq_len = tf.shape(embeddings)[1] # type: ignore\n",
    "\n",
    "        Q = embeddings @ self.Query_projection # (seq_len , d_model)\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head Attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) # (batch , heads , seq_len,seq_len)\n",
    "        \n",
    "        # 5a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))  # (1,1,L,L)\n",
    "\n",
    "        # 5b. Token mask (B,L) -> (B,1,1,L) - cast to match scores dtype\n",
    "        token_mask = tf.cast(token_masks[:, tf.newaxis, tf.newaxis, :], scores.dtype)\n",
    "\n",
    "        # 5c. Combine masks - cast causal_mask to match scores dtype\n",
    "        causal_mask = tf.cast(causal_mask, scores.dtype)\n",
    "        combined_mask = causal_mask * token_mask  # broadcast -> (B, H, L, L)\n",
    "\n",
    "        # 6. Apply mask (replace disallowed with -1e9) - use same dtype as scores\n",
    "        mask_value = tf.constant(-1e9, dtype=scores.dtype)\n",
    "        scores = tf.where(combined_mask > 0, scores, mask_value)\n",
    "\n",
    "        # 7. Scaled dot-product attention - ensure dtype consistency\n",
    "        scale_factor = tf.sqrt(tf.cast(self.d_head, scores.dtype))\n",
    "        attention_weights = tf.nn.softmax(scores / scale_factor, axis=-1)\n",
    "        \n",
    "        context = attention_weights @ V   #(batch, heads, seq_len, seq_len) × (batch, heads, seq_len, d_head) → (batch, heads, seq_len, d_head)\n",
    "        concat_context = tf.reshape(context, (batch_size, seq_len, self.attention_heads * self.d_head))  # type: ignore\n",
    "\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads,})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40a50e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 13, 4, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], shape=(16, 13, 4, 3), dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = np.random.rand(3, 13, 64)  # batch, seq_len, d_model\n",
    "q = np.ones((64, 64))          # d_model × d_model\n",
    "\n",
    "a = tf.constant(a, dtype=tf.float32)\n",
    "q = tf.constant(q, dtype=tf.float32)\n",
    "\n",
    "s = a @ q   # type: ignore # [3, 13, 64]\n",
    "d_head = 16\n",
    "num_heads = 64 // d_head # 4\n",
    "\n",
    "# split into heads\n",
    "s = tf.reshape(s, (3, num_heads, 13, d_head))  # [3, 4, 13, 16]\n",
    "f = tf.constant(np.ones_like(s))\n",
    "\n",
    "tf.transpose(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "873ee087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.95257413, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.04742587, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,9,4,5],[4,5,6,7,8]])\n",
    "keras.activations.softmax(tf.cast(arr,dtype = tf.float32),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb91875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.04742587, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.95257413, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,3,4,5],[4,5,6,7,8]])\n",
    "arr = tf.cast(arr,dtype = tf.float32)\n",
    "\n",
    "tf.nn.softmax(arr,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac7cc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000000.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0da4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7a994b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, seq_len, input_dim)\n",
    "        output = tf.matmul(inputs, self.kernel) + self.bias  # shape: (batch, seq_len, units))  # shape: (batch, seq_len, units)\n",
    "        return output + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch, seq_len, input_dim)\n",
    "        return (input_shape, input_shape[1], self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb7c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2,3,4,5,6)\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea053e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd143548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_to_id_dict= tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt']) # Size 94\n",
    "\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids,attention_mask = tokenize_and_build_token_id_sp(sp,batch_text,CONTEXT_LEN) # type: ignore # Unpacking Values\n",
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d639c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 256), dtype=int32, numpy=\n",
       " array([[  58, 7974,    0, ...,    0,    0,    0],\n",
       "        [7970,    1,  306, ...,    0,    0,    0],\n",
       "        [7970,    1,  254, ...,    0,    0,    0],\n",
       "        [7970,    1, 7971, ...,    0,    0,    0]],\n",
       "       shape=(4, 256), dtype=int32)>,\n",
       " <tf.Tensor: shape=(4, 256), dtype=int32, numpy=\n",
       " array([[1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], shape=(4, 256), dtype=int32)>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_ids,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d45b6a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">591,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">591,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">591,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">591,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,080,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m3,072,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m589,824\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (\u001b[38;5;33mDropout\u001b[0m)          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1536\u001b[0m)         │       \u001b[38;5;34m591,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m590,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m589,824\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1536\u001b[0m)         │       \u001b[38;5;34m591,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m590,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m589,824\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1536\u001b[0m)         │       \u001b[38;5;34m591,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m590,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m589,824\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1536\u001b[0m)         │       \u001b[38;5;34m591,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │       \u001b[38;5;34m590,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m8000\u001b[0m)         │     \u001b[38;5;34m3,080,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,244,480</span> (50.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,244,480\u001b[0m (50.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,244,480</span> (50.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,244,480\u001b[0m (50.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build once to get .summary()\n",
    "DECODER_BLOCKS = 4\n",
    "ATTENTION_HEADS = 2\n",
    "\n",
    "GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,0.1)\n",
    "_ = GPT_model((token_ids, attention_mask))\n",
    "GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# training (stable): use logits\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-4)\n",
    "GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# inference probs (when you actually need them)\n",
    "logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "probs = keras.ops.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc4eb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 256, 8000), dtype=float32, numpy=\n",
       "array([[[1.29824592e-04, 1.06779167e-04, 1.22794649e-04, ...,\n",
       "         8.11613718e-05, 7.20704920e-05, 9.71354893e-05],\n",
       "        [1.22811238e-04, 1.06585168e-04, 1.20838362e-04, ...,\n",
       "         8.64683680e-05, 7.22751356e-05, 9.42848346e-05],\n",
       "        [1.19839431e-04, 1.06091757e-04, 1.19593788e-04, ...,\n",
       "         8.83806570e-05, 7.26225990e-05, 9.43358682e-05],\n",
       "        ...,\n",
       "        [1.22395693e-04, 8.10345809e-05, 1.05884603e-04, ...,\n",
       "         1.28185435e-04, 1.39951590e-04, 9.54497882e-05],\n",
       "        [1.20639328e-04, 8.24767703e-05, 1.01917642e-04, ...,\n",
       "         1.26679108e-04, 1.39628042e-04, 9.58719102e-05],\n",
       "        [1.24011160e-04, 8.44897950e-05, 9.77056116e-05, ...,\n",
       "         1.28249449e-04, 1.35947645e-04, 9.46443688e-05]],\n",
       "\n",
       "       [[1.23397520e-04, 1.07271248e-04, 1.24283964e-04, ...,\n",
       "         8.07737160e-05, 7.13818954e-05, 9.82879938e-05],\n",
       "        [1.07669744e-04, 1.07843814e-04, 1.14868097e-04, ...,\n",
       "         9.18637088e-05, 6.59815050e-05, 8.93592223e-05],\n",
       "        [9.98333853e-05, 1.03553131e-04, 1.06034611e-04, ...,\n",
       "         9.53605413e-05, 6.32677620e-05, 8.51373043e-05],\n",
       "        ...,\n",
       "        [1.37762167e-04, 1.03001825e-04, 1.01239479e-04, ...,\n",
       "         1.51022468e-04, 1.27190215e-04, 1.07475884e-04],\n",
       "        [1.36649120e-04, 1.08461958e-04, 9.68381573e-05, ...,\n",
       "         1.51126253e-04, 1.25288294e-04, 1.10233472e-04],\n",
       "        [1.42528501e-04, 1.13558555e-04, 9.29268499e-05, ...,\n",
       "         1.54451831e-04, 1.22103651e-04, 1.09751483e-04]],\n",
       "\n",
       "       [[1.23923324e-04, 1.07541746e-04, 1.24071288e-04, ...,\n",
       "         8.07893157e-05, 7.11767862e-05, 9.86551531e-05],\n",
       "        [1.08104286e-04, 1.06086081e-04, 1.14208618e-04, ...,\n",
       "         9.18784426e-05, 6.55918338e-05, 8.96056154e-05],\n",
       "        [9.94388320e-05, 1.00867961e-04, 1.03898281e-04, ...,\n",
       "         9.46849177e-05, 6.32443698e-05, 8.57118212e-05],\n",
       "        ...,\n",
       "        [1.31274966e-04, 1.04329207e-04, 1.02353028e-04, ...,\n",
       "         1.48454856e-04, 1.29557622e-04, 1.02257683e-04],\n",
       "        [1.30023298e-04, 1.09764187e-04, 9.74913783e-05, ...,\n",
       "         1.48417312e-04, 1.27915948e-04, 1.04310777e-04],\n",
       "        [1.33804555e-04, 1.13932947e-04, 9.28996742e-05, ...,\n",
       "         1.51698638e-04, 1.25300518e-04, 1.02636841e-04]],\n",
       "\n",
       "       [[1.25996201e-04, 1.08154978e-04, 1.23486228e-04, ...,\n",
       "         8.22803631e-05, 7.14207999e-05, 9.85267470e-05],\n",
       "        [1.12614376e-04, 1.10727829e-04, 1.16952455e-04, ...,\n",
       "         9.24637279e-05, 6.79411460e-05, 9.19502272e-05],\n",
       "        [1.10002802e-04, 1.09261113e-04, 1.15623021e-04, ...,\n",
       "         9.53716517e-05, 6.74425319e-05, 9.04015396e-05],\n",
       "        ...,\n",
       "        [1.15090399e-04, 8.02263239e-05, 1.00485515e-04, ...,\n",
       "         1.33580499e-04, 1.34618007e-04, 9.33602205e-05],\n",
       "        [1.13468712e-04, 8.09171543e-05, 9.57090597e-05, ...,\n",
       "         1.32542365e-04, 1.33751018e-04, 9.43047999e-05],\n",
       "        [1.16696829e-04, 8.25667157e-05, 9.11816169e-05, ...,\n",
       "         1.34541697e-04, 1.29695429e-04, 9.32182593e-05]]],\n",
       "      shape=(4, 256, 8000), dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81260874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 256, 8000), dtype=float32, numpy=\n",
       "array([[[1.29824592e-04, 1.06779167e-04, 1.22794649e-04, ...,\n",
       "         8.11613718e-05, 7.20704920e-05, 9.71354893e-05],\n",
       "        [1.22811238e-04, 1.06585168e-04, 1.20838362e-04, ...,\n",
       "         8.64683680e-05, 7.22751356e-05, 9.42848346e-05],\n",
       "        [1.19839431e-04, 1.06091757e-04, 1.19593788e-04, ...,\n",
       "         8.83806570e-05, 7.26225990e-05, 9.43358682e-05],\n",
       "        ...,\n",
       "        [1.22395693e-04, 8.10345809e-05, 1.05884603e-04, ...,\n",
       "         1.28185435e-04, 1.39951590e-04, 9.54497882e-05],\n",
       "        [1.20639328e-04, 8.24767703e-05, 1.01917642e-04, ...,\n",
       "         1.26679108e-04, 1.39628042e-04, 9.58719102e-05],\n",
       "        [1.24011160e-04, 8.44897950e-05, 9.77056116e-05, ...,\n",
       "         1.28249449e-04, 1.35947645e-04, 9.46443688e-05]],\n",
       "\n",
       "       [[1.23397520e-04, 1.07271248e-04, 1.24283964e-04, ...,\n",
       "         8.07737160e-05, 7.13818954e-05, 9.82879938e-05],\n",
       "        [1.07669744e-04, 1.07843814e-04, 1.14868097e-04, ...,\n",
       "         9.18637088e-05, 6.59815050e-05, 8.93592223e-05],\n",
       "        [9.98333853e-05, 1.03553131e-04, 1.06034611e-04, ...,\n",
       "         9.53605413e-05, 6.32677620e-05, 8.51373043e-05],\n",
       "        ...,\n",
       "        [1.37762167e-04, 1.03001825e-04, 1.01239479e-04, ...,\n",
       "         1.51022468e-04, 1.27190215e-04, 1.07475884e-04],\n",
       "        [1.36649120e-04, 1.08461958e-04, 9.68381573e-05, ...,\n",
       "         1.51126253e-04, 1.25288294e-04, 1.10233472e-04],\n",
       "        [1.42528501e-04, 1.13558555e-04, 9.29268499e-05, ...,\n",
       "         1.54451831e-04, 1.22103651e-04, 1.09751483e-04]],\n",
       "\n",
       "       [[1.23923324e-04, 1.07541746e-04, 1.24071288e-04, ...,\n",
       "         8.07893157e-05, 7.11767862e-05, 9.86551531e-05],\n",
       "        [1.08104286e-04, 1.06086081e-04, 1.14208618e-04, ...,\n",
       "         9.18784426e-05, 6.55918338e-05, 8.96056154e-05],\n",
       "        [9.94388320e-05, 1.00867961e-04, 1.03898281e-04, ...,\n",
       "         9.46849177e-05, 6.32443698e-05, 8.57118212e-05],\n",
       "        ...,\n",
       "        [1.31274966e-04, 1.04329207e-04, 1.02353028e-04, ...,\n",
       "         1.48454856e-04, 1.29557622e-04, 1.02257683e-04],\n",
       "        [1.30023298e-04, 1.09764187e-04, 9.74913783e-05, ...,\n",
       "         1.48417312e-04, 1.27915948e-04, 1.04310777e-04],\n",
       "        [1.33804555e-04, 1.13932947e-04, 9.28996742e-05, ...,\n",
       "         1.51698638e-04, 1.25300518e-04, 1.02636841e-04]],\n",
       "\n",
       "       [[1.25996201e-04, 1.08154978e-04, 1.23486228e-04, ...,\n",
       "         8.22803631e-05, 7.14207999e-05, 9.85267470e-05],\n",
       "        [1.12614376e-04, 1.10727829e-04, 1.16952455e-04, ...,\n",
       "         9.24637279e-05, 6.79411460e-05, 9.19502272e-05],\n",
       "        [1.10002802e-04, 1.09261113e-04, 1.15623021e-04, ...,\n",
       "         9.53716517e-05, 6.74425319e-05, 9.04015396e-05],\n",
       "        ...,\n",
       "        [1.15090399e-04, 8.02263239e-05, 1.00485515e-04, ...,\n",
       "         1.33580499e-04, 1.34618007e-04, 9.33602205e-05],\n",
       "        [1.13468712e-04, 8.09171543e-05, 9.57090597e-05, ...,\n",
       "         1.32542365e-04, 1.33751018e-04, 9.43047999e-05],\n",
       "        [1.16696829e-04, 8.25667157e-05, 9.11816169e-05, ...,\n",
       "         1.34541697e-04, 1.29695429e-04, 9.32182593e-05]]],\n",
       "      shape=(4, 256, 8000), dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = GPT_model((token_ids, attention_mask))\n",
    "probs = tf.nn.softmax(outputs, axis=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72ebf9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,080,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m3,072,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │     \u001b[38;5;34m1,772,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m384\u001b[0m)          │           \u001b[38;5;34m768\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m8000\u001b[0m)         │     \u001b[38;5;34m3,080,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,244,480</span> (50.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,244,480\u001b[0m (50.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,244,480</span> (50.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,244,480\u001b[0m (50.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80e575a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diagram to gpt_model.png\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "try:\n",
    "    plot_model(\n",
    "        GPT_model,\n",
    "        to_file=\"gpt_model.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        expand_nested=True,\n",
    "        dpi=160\n",
    "    )\n",
    "    print(\"Saved diagram to gpt_model.png\")\n",
    "except Exception as e:\n",
    "    print(\"plot_model failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fdd9c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InitializePositionalEmbeddings name=init_embeddings, built=True>,\n",
       " <DecoderBlock name=decoder_block_0, built=True>,\n",
       " <DecoderBlock name=decoder_block_1, built=True>,\n",
       " <DecoderBlock name=decoder_block_2, built=True>,\n",
       " <DecoderBlock name=decoder_block_3, built=True>,\n",
       " <LayerNormalization name=layer_normalization_8, built=True>,\n",
       " <Dense name=Model_head, built=True>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe5fecf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13244480"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7809a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               GPT MODEL SUMMARY                                \n",
      "================================================================================\n",
      "Total parameters:      13,244,480\n",
      "Total layers:          7\n",
      "Trainable weights:     53\n",
      "Final output shape(s): ['(unavailable)']\n",
      "--------------------------------------------------------------------------------\n",
      "Idx | Layer Type               | Layer Name              | Weight Name                  | Shape           |   Params\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "000 | InitializePositionalEmbeddings | init_embeddings         | embedding_matrix             | (8000, 384)     | 3,072,000\n",
      "001 | DecoderBlock             | decoder_block_0         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | glorot_uniform               | (384, 384)      |  147,456\n",
      "    |                          |                         | Key_Vector_for_projection    | (384, 384)      |  147,456\n",
      "    |                          |                         | Value_Vector_for_projection  | (384, 384)      |  147,456\n",
      "    |                          |                         | Output_projection            | (384, 384)      |  147,456\n",
      "    |                          |                         | kernel                       | (384, 1536)     |  589,824\n",
      "    |                          |                         | bias                         | (1536,)         |    1,536\n",
      "    |                          |                         | kernel                       | (1536, 384)     |  589,824\n",
      "    |                          |                         | bias                         | (384,)          |      384\n",
      "    |                          |                         |                              |                 |         \n",
      "002 | DecoderBlock             | decoder_block_1         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | glorot_uniform               | (384, 384)      |  147,456\n",
      "    |                          |                         | Key_Vector_for_projection    | (384, 384)      |  147,456\n",
      "    |                          |                         | Value_Vector_for_projection  | (384, 384)      |  147,456\n",
      "    |                          |                         | Output_projection            | (384, 384)      |  147,456\n",
      "    |                          |                         | kernel                       | (384, 1536)     |  589,824\n",
      "    |                          |                         | bias                         | (1536,)         |    1,536\n",
      "    |                          |                         | kernel                       | (1536, 384)     |  589,824\n",
      "    |                          |                         | bias                         | (384,)          |      384\n",
      "    |                          |                         |                              |                 |         \n",
      "003 | DecoderBlock             | decoder_block_2         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | glorot_uniform               | (384, 384)      |  147,456\n",
      "    |                          |                         | Key_Vector_for_projection    | (384, 384)      |  147,456\n",
      "    |                          |                         | Value_Vector_for_projection  | (384, 384)      |  147,456\n",
      "    |                          |                         | Output_projection            | (384, 384)      |  147,456\n",
      "    |                          |                         | kernel                       | (384, 1536)     |  589,824\n",
      "    |                          |                         | bias                         | (1536,)         |    1,536\n",
      "    |                          |                         | kernel                       | (1536, 384)     |  589,824\n",
      "    |                          |                         | bias                         | (384,)          |      384\n",
      "    |                          |                         |                              |                 |         \n",
      "004 | DecoderBlock             | decoder_block_3         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         | glorot_uniform               | (384, 384)      |  147,456\n",
      "    |                          |                         | Key_Vector_for_projection    | (384, 384)      |  147,456\n",
      "    |                          |                         | Value_Vector_for_projection  | (384, 384)      |  147,456\n",
      "    |                          |                         | Output_projection            | (384, 384)      |  147,456\n",
      "    |                          |                         | kernel                       | (384, 1536)     |  589,824\n",
      "    |                          |                         | bias                         | (1536,)         |    1,536\n",
      "    |                          |                         | kernel                       | (1536, 384)     |  589,824\n",
      "    |                          |                         | bias                         | (384,)          |      384\n",
      "    |                          |                         |                              |                 |         \n",
      "005 | LayerNormalization       | layer_normalization_8   | alpha                        | (384,)          |      384\n",
      "    |                          |                         | beta                         | (384,)          |      384\n",
      "    |                          |                         |                              |                 |         \n",
      "006 | Dense                    | Model_head              | kernel                       | (384, 8000)     | 3,072,000\n",
      "    |                          |                         | bias                         | (8000,)         |    8,000\n",
      "    |                          |                         |                              |                 |         \n",
      "================================================================================\n",
      "Note: Only trainable weights are listed above. Output shapes may be unavailable\n",
      "for subclassed models or models not built symbolically.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_model_summary(model):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'GPT MODEL SUMMARY':^80}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    total_layers = len(model.layers)\n",
    "    total_weights = sum(len(layer.trainable_weights) for layer in model.layers)\n",
    "    try:\n",
    "        output_shapes = [tuple(out.shape) for out in model.outputs]\n",
    "    except Exception:\n",
    "        output_shapes = [\"(unavailable)\"]\n",
    "    \n",
    "    print(f\"{'Total parameters:':<22} {total_params:,}\")\n",
    "    print(f\"{'Total layers:':<22} {total_layers}\")\n",
    "    print(f\"{'Trainable weights:':<22} {total_weights}\")\n",
    "    print(f\"{'Final output shape(s):':<22} {output_shapes if output_shapes else '(N/A)'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    header = f\"{'Idx':>3} | {'Layer Type':<24} | {'Layer Name':<23} | {'Weight Name':<28} | {'Shape':<15} | {'Params':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_type = layer.__class__.__name__\n",
    "        layer_name = layer.name\n",
    "        weights = layer.trainable_weights\n",
    "        layer_weight_count = len(weights)\n",
    "\n",
    "        if layer_weight_count == 0:\n",
    "            print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {'-':<28} | {'-':<15} | {'0':>8}\")\n",
    "        else:\n",
    "            for j, w in enumerate(weights):\n",
    "                n = int(np.prod(w.shape)) if hasattr(w, \"shape\") else \"?\"\n",
    "                shape_str = str(tuple(w.shape))\n",
    "                weight_name = w.name\n",
    "                if j == 0:\n",
    "                    print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "                else:\n",
    "                    print(f\"    | {'':<24} | {'':<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "        if layer_weight_count > 1:\n",
    "            print(f\"    | {'':<24} | {'':<23} | {'':<28} | {'':<15} | {'':>8}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Note: Only trainable weights are listed above. Output shapes may be unavailable\\n\"\n",
    "          \"for subclassed models or models not built symbolically.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usage:\n",
    "format_model_summary(GPT_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ab03dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_book_training_data(book_text: str, \n",
    "#                              token_to_id_dict: Dict[str, int], \n",
    "#                              context_length: int = 512,\n",
    "#                              pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Prepare training data from a Gutenberg book\n",
    "#     \"\"\"\n",
    "#     # 1. Tokenize the entire book\n",
    "#     token_ids = [token_to_id_dict.get(c, pad_value) for c in book_text]\n",
    "    \n",
    "#     # 2. Create sliding windows\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "    \n",
    "#     # Slide window across the entire book\n",
    "#     for i in range(0, len(token_ids) - context_length, context_length):\n",
    "#         # Extract window of context_length + 1 tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "        \n",
    "#         if len(window) < context_length + 1:\n",
    "#             break  # Skip incomplete windows at the end\n",
    "        \n",
    "#         # Create input-target pair\n",
    "#         input_seq = window[:-1]   # [t1, t2, ..., t512]\n",
    "#         target_seq = window[1:]   # [t2, t3, ..., t513]\n",
    "        \n",
    "#         inputs.append(input_seq)\n",
    "#         targets.append(target_seq)\n",
    "    \n",
    "#     # 3. Convert to numpy arrays\n",
    "#     inputs = np.array(inputs, dtype=np.int32)\n",
    "#     targets = np.array(targets, dtype=np.int32)\n",
    "    \n",
    "#     # 4. Create padding masks (all 1s since we're using full context)\n",
    "#     masks = np.ones_like(inputs, dtype=np.int32)\n",
    "    \n",
    "#     return inputs, targets, masks\n",
    "\n",
    "# # Usage:\n",
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r', encoding='utf-8') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea8b1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# print(f\"Total characters: {len(book_text)}\")\n",
    "# print(f\"Expected examples (rough): {len(book_text) // 512}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fc7c7",
   "metadata": {},
   "source": [
    "''' Stop '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "649d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to TensorFlow tensors\n",
    "# input_ids = tf.constant(inputs, dtype=tf.int32)\n",
    "# target_ids = tf.constant(targets, dtype=tf.int32)\n",
    "# attention_masks = tf.constant(masks, dtype=tf.int32)\n",
    "\n",
    "# model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,8,0.00001,4,0.1,sinusoidal_lookup_table)\n",
    "\n",
    "# # Compile your model\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Train with .fit()\n",
    "# history = model.fit(\n",
    "#     x=[input_ids, ,  # Your model expects (token_ids, attention_mask)\n",
    "#     y=target_ids,\n",
    "#     batch_size=16,  # Start small since 677 examples isn't huge\n",
    "#     epochs=50,\n",
    "#     validation_split=0.1,  # Use 10% for validation\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),\n",
    "#         keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e775cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_text, token_to_id, id_to_token, max_len=100, temperature=1.0):\n",
    "#     # Encode start text\n",
    "#     token_ids, attention_mask = tokenize_and_build_token_id(\n",
    "#         token_to_id, [start_text], max_seq_len=512\n",
    "#     )\n",
    "    \n",
    "#     generated = list(token_ids[0].numpy())  # flatten out\n",
    "#     mask = list(attention_mask[0].numpy())\n",
    "    \n",
    "#     for _ in range(max_len):\n",
    "#         # Trim to last 512 tokens\n",
    "#         x_tokens = np.array([generated[-512:]])\n",
    "#         x_mask   = np.array([mask[-512:]])\n",
    "\n",
    "#         # Forward pass with both inputs\n",
    "#         logits = model((x_tokens, x_mask), training=False)\n",
    "\n",
    "#         # Take last position logits\n",
    "#         next_logits = logits[0, -1] / temperature\n",
    "#         probs = tf.nn.softmax(next_logits).numpy()\n",
    "\n",
    "#         # Sample next token\n",
    "#         next_id = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "#         # Append\n",
    "#         generated.append(next_id)\n",
    "#         mask.append(1)  # mark as valid token\n",
    "\n",
    "#     # Decode\n",
    "#     return ''.join(id_to_token[i] for i in generated if i in id_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d50c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_token_dict = {v: k for k, v in token_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43978b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = \"Akshat Khatri\"\n",
    "# result = generate_text(model, start, token_to_id_dict, id_to_token_dict, max_len=100, temperature=0.8)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94cf8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-v1')# Concatenate train + validation + test\n",
    "# all_texts = []\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     all_texts.extend(dataset[split][\"text\"])\n",
    "\n",
    "# # Remove empty lines\n",
    "# all_texts = [t.strip() for t in all_texts if t.strip() != \"\"]\n",
    "\n",
    "# # Join into one giant string\n",
    "# big_text = \"\\n\".join(all_texts)\n",
    "\n",
    "# # Write to file\n",
    "# with open(\"wikitext_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(big_text)\n",
    "\n",
    "# print(\"Saved dataset to wikitext_full.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c759f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5f51f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming you already have:\n",
    "# # - prepare_book_training_data()\n",
    "# # - token_to_id_dict\n",
    "\n",
    "# file_path = r\"/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\"\n",
    "\n",
    "# inputs_list, targets_list, masks_list = [], [], []\n",
    "\n",
    "# buffer = \"\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         buffer += line.strip() + \" \"\n",
    "#         # Process every ~5000 chars to avoid memory spike\n",
    "#         if len(buffer) > 5000:\n",
    "#             inp, tgt, msk = prepare_book_training_data(\n",
    "#                 buffer, token_to_id_dict, context_length=512\n",
    "#             )\n",
    "#             inputs_list.append(inp)\n",
    "#             targets_list.append(tgt)\n",
    "#             masks_list.append(msk)\n",
    "#             buffer = \"\"  # reset buffer\n",
    "\n",
    "# # Process any leftover buffer\n",
    "# if buffer.strip():\n",
    "#     inp, tgt, msk = prepare_book_training_data(\n",
    "#         buffer, token_to_id_dict, context_length=512\n",
    "#     )\n",
    "#     inputs_list.append(inp)\n",
    "#     targets_list.append(tgt)\n",
    "#     masks_list.append(msk)\n",
    "\n",
    "# # Concatenate all batches into final arrays\n",
    "# inputs = np.concatenate(inputs_list, axis=0)\n",
    "# targets = np.concatenate(targets_list, axis=0)\n",
    "# masks = np.concatenate(masks_list, axis=0)\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")\n",
    "# print(f\"Masks shape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20b06fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from typing import Dict, List\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "#     \"\"\"\n",
    "#     Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "#     Args:\n",
    "#         input_ids: Token sequence for model input\n",
    "#         target_ids: Token sequence for model targets (shifted by 1)\n",
    "#         attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "#     Returns:\n",
    "#         Serialized TFRecord example\n",
    "#     \"\"\"\n",
    "#     feature = {\n",
    "#         'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "#         'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "#         'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "#     }\n",
    "    \n",
    "#     example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#     return example.SerializeToString()\n",
    "\n",
    "\n",
    "# def convert_text_to_tfrecord(\n",
    "#     text_file_path: str,\n",
    "#     token_to_id_dict: Dict[str, int],\n",
    "#     output_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     records_per_file: int = 1000,\n",
    "#     pad_value: int = 0\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Convert text file to TFRecord files for GPT training.\n",
    "    \n",
    "#     Process:\n",
    "#     1. Read and tokenize entire text file\n",
    "#     2. Create sliding windows of context_length + 1 tokens\n",
    "#     3. Split each window into input/target pairs (shifted by 1)\n",
    "#     4. Save as TFRecord files with specified number of records per file\n",
    "    \n",
    "#     Args:\n",
    "#         text_file_path: Path to your text file (e.g., WikiText-103)\n",
    "#         token_to_id_dict: Character-to-ID mapping dictionary\n",
    "#         output_dir: Directory to save TFRecord files\n",
    "#         context_length: Sequence length for training\n",
    "#         records_per_file: Number of examples per TFRecord file\n",
    "#         pad_value: Token ID used for unknown characters\n",
    "        \n",
    "#     Returns:\n",
    "#         Path to output directory containing TFRecord files\n",
    "#     \"\"\"\n",
    "#     # Setup\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     print(f\"Reading text file: {text_file_path}\")\n",
    "    \n",
    "#     # Step 1: Load and tokenize text\n",
    "#     with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "#         text = f.read()\n",
    "    \n",
    "#     print(f\"Text length: {len(text):,} characters\")\n",
    "    \n",
    "#     # Convert each character to token ID\n",
    "#     print(\"Tokenizing text...\")\n",
    "#     token_ids = [token_to_id_dict.get(char, pad_value) for char in text]\n",
    "#     print(f\"Token length: {len(token_ids):,} tokens\")\n",
    "    \n",
    "#     # Step 2: Calculate output size\n",
    "#     num_examples = (len(token_ids) - context_length) // context_length\n",
    "#     print(f\"Will create {num_examples:,} training examples\")\n",
    "    \n",
    "#     # Step 3: Process sliding windows and write TFRecord files\n",
    "#     file_count = 0\n",
    "#     examples_in_current_file = 0\n",
    "#     writer = None\n",
    "    \n",
    "#     print(\"Creating TFRecord files...\")\n",
    "    \n",
    "#     # Slide window across token sequence\n",
    "#     for i in tqdm(range(0, len(token_ids) - context_length, context_length)):\n",
    "        \n",
    "#         # Extract window of tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "#         if len(window) < context_length + 1:\n",
    "#             break\n",
    "            \n",
    "#         # Create input-target pair (GPT training format)\n",
    "#         input_ids = window[:-1]    # First 512 tokens: [t1, t2, ..., t512]\n",
    "#         target_ids = window[1:]    # Shifted by 1: [t2, t3, ..., t513]\n",
    "#         attention_mask = [1] * context_length  # All valid tokens (no padding)\n",
    "        \n",
    "#         # Start new TFRecord file if needed\n",
    "#         if writer is None or examples_in_current_file >= records_per_file:\n",
    "#             if writer is not None:\n",
    "#                 writer.close()\n",
    "            \n",
    "#             tfrecord_filename = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#             writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "#             file_count += 1\n",
    "#             examples_in_current_file = 0\n",
    "        \n",
    "#         # Write training example to current TFRecord file\n",
    "#         tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#         writer.write(tf_example)\n",
    "#         examples_in_current_file += 1\n",
    "    \n",
    "#     # Cleanup\n",
    "#     if writer is not None:\n",
    "#         writer.close()\n",
    "    \n",
    "#     # Step 4: Save summary information\n",
    "#     print(f\"\\nConversion complete!\")\n",
    "#     print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "#     print(f\"Total examples: {num_examples:,}\")\n",
    "    \n",
    "#     # Write metadata file\n",
    "#     metadata = {\n",
    "#         'context_length': context_length,\n",
    "#         'vocab_size': len(token_to_id_dict),\n",
    "#         'num_examples': num_examples,\n",
    "#         'num_files': file_count,\n",
    "#         'records_per_file': records_per_file\n",
    "#     }\n",
    "    \n",
    "#     metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         for key, value in metadata.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "#     print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "#     return output_dir\n",
    "\n",
    "\n",
    "# def create_tf_data_pipeline(\n",
    "#     tfrecord_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     batch_size: int = 32,\n",
    "#     shuffle_buffer: int = 1000,\n",
    "#     prefetch_buffer: int = tf.data.AUTOTUNE\n",
    "# ) -> tf.data.Dataset:\n",
    "#     \"\"\"\n",
    "#     Create tf.data pipeline from TFRecord files for training.\n",
    "    \n",
    "#     Process:\n",
    "#     1. Find all TFRecord files in directory\n",
    "#     2. Create dataset that reads and parses TFRecord examples\n",
    "#     3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "#     Args:\n",
    "#         tfrecord_dir: Directory containing TFRecord files\n",
    "#         context_length: Expected sequence length in records\n",
    "#         batch_size: Number of examples per training batch\n",
    "#         shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "#         prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "#     Returns:\n",
    "#         tf.data.Dataset ready for model.fit()\n",
    "#     \"\"\"\n",
    "#     # Step 1: Find TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     # Step 2: Define how to parse each TFRecord example\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_tfrecord_example(example_proto):\n",
    "#         \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "#         # Parse the serialized example\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         # Convert to correct data types\n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "#         model_inputs = (input_ids, attention_mask)\n",
    "#         model_targets = target_ids\n",
    "        \n",
    "#         return model_inputs, model_targets\n",
    "    \n",
    "#     # Step 3: Create and configure dataset pipeline\n",
    "#     dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "#     dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(shuffle_buffer)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# # Example usage (commented out)\n",
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Example usage for WikiText-103 or similar large text files\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "#     # example_vocab = your_token_to_id_dict\n",
    "    \n",
    "#     # # Step 2: Convert text to TFRecord format (run once)\n",
    "#     # tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     #     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     #     token_to_id_dict=example_vocab,\n",
    "#     #     output_dir='./tfrecords',\n",
    "#     #     context_length=512,\n",
    "#     #     records_per_file=1000\n",
    "#     # )\n",
    "    \n",
    "#     # # Step 3: Create training pipeline (use for training)\n",
    "#     # train_dataset = create_tf_data_pipeline(\n",
    "#     #     tfrecord_dir=tfrecord_dir,\n",
    "#     #     context_length=512,\n",
    "#     #     batch_size=16\n",
    "#     # )\n",
    "    \n",
    "#     # print(\"TFRecord pipeline ready for training!\")\n",
    "#     # # Now you can use train_dataset with your model's .fit() method\n",
    "    \n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6ab8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "    \"\"\"\n",
    "    Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token sequence for model input\n",
    "        target_ids: Token sequence for model targets (shifted by 1)\n",
    "        attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "    Returns:\n",
    "        Serialized TFRecord example\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "        'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "        'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_text_to_tfrecord_sp(\n",
    "    text_file_path: str,\n",
    "    sp_model_path: str,\n",
    "    output_dir: str,\n",
    "    context_length: int = 512,\n",
    "    records_per_file: int = 1000,\n",
    "    overlap_size: int = 64,\n",
    "    chunk_size: int = 100_000  # Number of characters read at a time\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Streaming version of text to TFRecord conversion with SentencePiece.\n",
    "    Reads and tokenizes file incrementally to limit memory usage.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_size = os.path.getsize(text_file_path)\n",
    "    print(f\"Reading and tokenizing text in chunks from: {text_file_path}\")\n",
    "    \n",
    "    # Load SentencePiece processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(sp_model_path)\n",
    "    \n",
    "    print(f\"Loaded SentencePiece model from: {sp_model_path}\")\n",
    "    print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "\n",
    "    buffer_tokens = []\n",
    "    step_size = context_length - overlap_size\n",
    "    file_count = 0\n",
    "    examples_in_current_file = 0\n",
    "    writer = None\n",
    "    \n",
    "    with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc='Processing text') as pbar:\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                buffer_tokens.extend(sp.encode_as_ids(chunk))\n",
    "                pbar.update(len(chunk.encode('utf-8')))  # update by bytes read\n",
    "                \n",
    "                # Process windows to create examples\n",
    "                while len(buffer_tokens) >= context_length + 1:\n",
    "                    window = buffer_tokens[:context_length + 1]\n",
    "                    input_ids = window[:-1]\n",
    "                    target_ids = window[1:]\n",
    "                    attention_mask = [1] * context_length\n",
    "                    \n",
    "                    if writer is None or examples_in_current_file >= records_per_file:\n",
    "                        if writer:\n",
    "                            writer.close()\n",
    "                        tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "                        writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "                        file_count += 1\n",
    "                        examples_in_current_file = 0\n",
    "                    \n",
    "                    tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "                    writer.write(tf_example)\n",
    "                    examples_in_current_file += 1\n",
    "                    \n",
    "                    # Slide window forward by step_size tokens\n",
    "                    buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "    # Process any remaining tokens\n",
    "    while len(buffer_tokens) >= context_length + 1:\n",
    "        window = buffer_tokens[:context_length + 1]\n",
    "        input_ids = window[:-1]\n",
    "        target_ids = window[1:]\n",
    "        attention_mask = [1] * context_length\n",
    "        \n",
    "        if writer is None or examples_in_current_file >= records_per_file:\n",
    "            if writer:\n",
    "                writer.close()\n",
    "            tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "            writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "            file_count += 1\n",
    "            examples_in_current_file = 0\n",
    "\n",
    "        tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "        writer.write(tf_example)\n",
    "        examples_in_current_file += 1\n",
    "        buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    num_examples = file_count * records_per_file  # Approximate count\n",
    "\n",
    "    print(f\"\\nConversion complete!\")\n",
    "    print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "    print(f\"Approximate total examples: {num_examples}\")\n",
    "\n",
    "    metadata = {\n",
    "        'context_length': context_length,\n",
    "        'vocab_size': sp.get_piece_size(),\n",
    "        'num_examples': num_examples,\n",
    "        'num_files': file_count,\n",
    "        'records_per_file': records_per_file,\n",
    "        'overlap_size': overlap_size,\n",
    "        'sp_model_path': sp_model_path,\n",
    "        'tokenization': 'SentencePiece'\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "\n",
    "def create_tf_data_pipeline_sp(\n",
    "    tfrecord_dir: str,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    shuffle_buffer: int = 1000,\n",
    "    prefetch_buffer: int = tf.data.AUTOTUNE\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create tf.data pipeline from TFRecord files for training (SentencePiece version).\n",
    "    \n",
    "    Process:\n",
    "    1. Find all TFRecord files in directory\n",
    "    2. Create dataset that reads and parses TFRecord examples\n",
    "    3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_dir: Directory containing TFRecord files\n",
    "        context_length: Expected sequence length in records\n",
    "        batch_size: Number of examples per training batch\n",
    "        shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "        prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "    Returns:\n",
    "        tf.data.Dataset ready for model.fit()\n",
    "    \"\"\"\n",
    "    # Step 1: Find TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    if not tfrecord_files:\n",
    "        raise FileNotFoundError(f\"No TFRecord files found in {tfrecord_dir}\")\n",
    "    \n",
    "    # Step 2: Define how to parse each TFRecord example\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_tfrecord_example(example_proto):\n",
    "        \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "        # Parse the serialized example\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Convert to correct data types\n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "        model_inputs = (input_ids, attention_mask)\n",
    "        model_targets = target_ids\n",
    "        \n",
    "        return model_inputs, model_targets\n",
    "    \n",
    "    # Step 3: Create and configure dataset pipeline\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Example usage assuming you have a pre-trained SentencePiece model\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Your pre-trained SentencePiece model path\n",
    "#     sp_model_path = '/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # You provide this\n",
    "    \n",
    "#     # Step 1: Convert text to TFRecords using your pre-trained model\n",
    "#     tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "#         text_file_path=r'/home/akshat/GPT_from_scratch/text_data/BookCorpus3_cleaned.txt',\n",
    "#         sp_model_path=sp_model_path,  # Your trained model\n",
    "#         output_dir='./tfrecords',\n",
    "#         context_length=CONTEXT_LEN,  # Match your CONTEXT_LEN\n",
    "#         records_per_file=1000,\n",
    "#         overlap_size = CONTEXT_LEN // 2,\n",
    "#         chunk_size = 150000\n",
    "#     )\n",
    "    \n",
    "#     # # Step 2: Create training pipeline\n",
    "#     # train_dataset = create_tf_data_pipeline_sp(\n",
    "#     #     tfrecord_dir=tfrecord_dir,\n",
    "#     #     context_length=128,  # Match your CONTEXT_LEN\n",
    "#     #     batch_size=16\n",
    "#     # )\n",
    "    \n",
    "#     print(\"SentencePiece TFRecord pipeline ready for training!\")\n",
    "    \n",
    "#     # Step 3: Load your SentencePiece model for vocab size and generation\n",
    "#     sp = spm.SentencePieceProcessor()\n",
    "#     sp.load(sp_model_path)\n",
    "#     VOCAB_SIZE = sp.get_piece_size()\n",
    "    \n",
    "#     print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "    # Now you can use:\n",
    "    # - sp for tokenization in generation\n",
    "    # - train_dataset for model.fit()\n",
    "    # - VOCAB_SIZE for your model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "253d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "# example_vocab = token_to_id_dict\n",
    "\n",
    "# # Step 2: Convert text to TFRecord format (run once)\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=example_vocab,\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=128,\n",
    "#     records_per_file=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9caffa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Create training and validation pipeline (use for training)\n",
    "# def create_train_val_datasets(tfrecord_dir: str, \n",
    "#                              context_length: int,\n",
    "#                              batch_size: int = 16,\n",
    "#                              val_split: float = 0.1):\n",
    "#     \"\"\"\n",
    "#     Create training and validation datasets from TFRecord files\n",
    "#     \"\"\"\n",
    "#     # Find all TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     # Split files for train/val\n",
    "#     num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "#     val_files = tfrecord_files[:num_val_files]\n",
    "#     train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "#     print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "#     # Feature description\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_function(example_proto):\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "#         return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "#     # Create training dataset\n",
    "#     train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "#     train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "#     train_dataset = train_dataset.batch(batch_size)\n",
    "#     train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     # Create validation dataset\n",
    "#     val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "#     val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     val_dataset = val_dataset.batch(batch_size)\n",
    "#     val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fb97b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_datasets_sp(tfrecord_dir: str, \n",
    "                                context_length: int,\n",
    "                                batch_size: int = 16,\n",
    "                                val_split: float = 0.1):\n",
    "    \"\"\"\n",
    "    Create training and validation datasets from TFRecord files (SentencePiece version)\n",
    "    \"\"\"\n",
    "    # Find all TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Split files for train/val\n",
    "    num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "    val_files = tfrecord_files[:num_val_files]\n",
    "    train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "    print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "    # Feature description\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_function(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "        return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "    train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "    val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.repeat()\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "def prepare_tfrecords_sp(\n",
    "    text_file_path: str,\n",
    "    sp_model_path: str,\n",
    "    context_length: int = 128,\n",
    "    records_per_file: int = 1000,\n",
    "    output_base_dir: str = './tfrecords',\n",
    "    version_name: str = None,\n",
    "    batch_size: int = 16,\n",
    "    val_split: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create TFRecords from text using SentencePiece and return train/val datasets.\n",
    "    Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "    Args:\n",
    "        text_file_path: Path to input text file.\n",
    "        sp_model_path: Path to trained SentencePiece model (.model file).\n",
    "        context_length: Sequence length for training.\n",
    "        records_per_file: Number of examples per TFRecord file.\n",
    "        output_base_dir: Base folder to store TFRecords.\n",
    "        version_name: Optional unique folder name. If None, uses context_length.\n",
    "        batch_size: Batch size for dataset.\n",
    "        val_split: Fraction of data to use as validation.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, steps_per_epoch, vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    # Load SentencePiece model to get vocab size\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(sp_model_path)\n",
    "    vocab_size = sp.get_piece_size()\n",
    "\n",
    "    # Determine output folder\n",
    "    if version_name is None:\n",
    "        version_name = f\"sp_context_{context_length}_bs{batch_size}_vocab{vocab_size}\"\n",
    "    output_dir = os.path.join(output_base_dir, version_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert text to TFRecords using SentencePiece\n",
    "    tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "        text_file_path=text_file_path,\n",
    "        sp_model_path=sp_model_path,\n",
    "        output_dir=output_dir,\n",
    "        context_length=context_length,\n",
    "        records_per_file=records_per_file\n",
    "    )\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset, val_dataset = create_train_val_datasets_sp(\n",
    "        tfrecord_dir=tfrecord_dir,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        val_split=val_split\n",
    "    )\n",
    "\n",
    "    # Calculate steps per epoch\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    total_examples = len(tfrecord_files) * records_per_file\n",
    "    train_examples = int(total_examples * (1 - val_split))\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "    print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Train examples: {train_examples}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    return train_dataset, val_dataset, steps_per_epoch, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64a8c3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4522 TFRecord files\n",
      "Using 4070 files for training, 452 for validation\n",
      "Found 4522 TFRecord files\n",
      "Using 4070 files for training, 452 for validation\n",
      "train done\n",
      "steps per epoch are 134246\n"
     ]
    }
   ],
   "source": [
    "# Usage with SentencePiece\n",
    "sp_model_path = '/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # Your pre-trained SentencePiece model\n",
    "tfrecord_dir = './tfrecords'\n",
    "# train_ds_16, val_ds_16, steps_16, VOCAB_SIZE = prepare_tfrecords_sp(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt',\n",
    "#     sp_model_path=sp_model_path,  # Your trained SentencePiece model\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16\n",
    "# )\n",
    "train_ds, val_ds = create_train_val_datasets_sp(\n",
    "    tfrecord_dir=r'/home/akshat/GPT_from_scratch/notebooks/tfrecords',\n",
    "    context_length=CONTEXT_LEN,\n",
    "    batch_size= 32\n",
    ")\n",
    "\n",
    "train_ds_64 , val_ds_64 = create_train_val_datasets_sp(\n",
    "    tfrecord_dir=r'/home/akshat/GPT_from_scratch/notebooks/tfrecords',\n",
    "    context_length=CONTEXT_LEN,\n",
    "    batch_size= 64\n",
    ")\n",
    "\n",
    "print('train done')\n",
    "# Calculate steps per epoch\n",
    "tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "total_examples = len(tfrecord_files) * 1000 # max_records per file = 1000\n",
    "train_examples = int(total_examples * (1 - 0.05)) # Validation - split = 0.05 -- 5 % \n",
    "steps_per_epoch = train_examples // 32 # batch size = 32\n",
    "print(f'steps per epoch are {steps_per_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ba0fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DATASETS\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"TFRecord pipeline ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4819661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef29291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Remove existing tfrecords directory\n",
    "# if os.path.exists('./tfrecords'):\n",
    "#     shutil.rmtree('./tfrecords')\n",
    "#     print(\"Removed existing tfrecords directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9252ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SOLUTION 1: Recreate TFRecords with correct context length\n",
    "# # Delete the existing tfrecords directory and recreate with CONTEXT_LEN\n",
    "\n",
    "\n",
    "# # Recreate with correct context length\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,  # Make sure this variable is defined\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # Use your actual context length (128)\n",
    "#     records_per_file=1000\n",
    "# )\n",
    "\n",
    "# # Now create datasets with matching context length\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # This will now match\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "# # Calculate steps per epoch\n",
    "# records_per_file = 1000  \n",
    "# tfrecord_files = tf.io.gfile.glob(os.path.join(\"./tfrecords\", \"*.tfrecord\"))\n",
    "# total_examples = len(tfrecord_files) * records_per_file\n",
    "# train_examples = int(total_examples * 0.9)\n",
    "# steps_per_epoch = train_examples // 16\n",
    "\n",
    "# print(f\"Files: {len(tfrecord_files)}\")\n",
    "# print(f\"Total examples: {total_examples}\")\n",
    "# print(f\"Steps per epoch: {steps_per_epoch}\") # When combined with batch_size sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a189899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def prepare_tfrecords(\n",
    "#     text_file_path: str,\n",
    "#     token_to_id_dict: dict,\n",
    "#     context_length: int = 128,\n",
    "#     records_per_file: int = 1000,\n",
    "#     output_base_dir: str = './tfrecords',\n",
    "#     version_name: str = None,\n",
    "#     batch_size: int = 16,\n",
    "#     val_split: float = 0.1\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create TFRecords from text and return train/val datasets.\n",
    "#     Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "#     Args:\n",
    "#         text_file_path: Path to input text file.\n",
    "#         token_to_id_dict: Character-to-id dictionary.\n",
    "#         context_length: Sequence length for training.\n",
    "#         records_per_file: Number of examples per TFRecord file.\n",
    "#         output_base_dir: Base folder to store TFRecords.\n",
    "#         version_name: Optional unique folder name. If None, uses context_length.\n",
    "#         batch_size: Batch size for dataset.\n",
    "#         val_split: Fraction of data to use as validation.\n",
    "\n",
    "#     Returns:\n",
    "#         train_dataset, val_dataset, steps_per_epoch\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Determine output folder\n",
    "#     if version_name is None:\n",
    "#         version_name = f\"context_{context_length}_bs{batch_size}\"\n",
    "#     output_dir = os.path.join(output_base_dir, version_name)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Convert text to TFRecords\n",
    "#     tfrecord_dir = convert_text_to_tfrecord(\n",
    "#         text_file_path=text_file_path,\n",
    "#         token_to_id_dict=token_to_id_dict,\n",
    "#         output_dir=output_dir,\n",
    "#         context_length=context_length,\n",
    "#         records_per_file=records_per_file\n",
    "#     )\n",
    "\n",
    "#     # Create train/val datasets\n",
    "#     train_dataset, val_dataset = create_train_val_datasets(\n",
    "#         tfrecord_dir=tfrecord_dir,\n",
    "#         context_length=context_length,\n",
    "#         batch_size=batch_size,\n",
    "#         val_split=val_split\n",
    "#     )\n",
    "\n",
    "#     # Calculate steps per epoch\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     total_examples = len(tfrecord_files) * records_per_file\n",
    "#     train_examples = int(total_examples * (1 - val_split))\n",
    "#     steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "#     print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "#     print(f\"Total examples: {total_examples}\")\n",
    "#     print(f\"Train examples: {train_examples}\")\n",
    "#     print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "#     return train_dataset, val_dataset, steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9722d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VOCAB_SIZE now comes from SentencePiece (e.g., 20000 - > 2000 instead of 94)\n",
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "\n",
    "# Create model with new vocab size\n",
    "model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, ATTENTION_HEADS, 0.00001, DECODER_BLOCKS, 0.2)\n",
    "\n",
    "# Compile your model (same as before)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    "    run_eagerly=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02ca0b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 13:06:21.877377: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-27 13:06:39.465250: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_892', 536 bytes spill stores, 536 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_896', 536 bytes spill stores, 536 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_898', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4999/134246\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:42:07\u001b[0m 465ms/step - accuracy: 0.9579 - loss: 0.2385\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_005000.keras\n",
      "\u001b[1m  9999/134246\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:42:35\u001b[0m 484ms/step - accuracy: 0.9582 - loss: 0.2368\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_010000.keras\n",
      "\u001b[1m 14999/134246\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:11:23\u001b[0m 398ms/step - accuracy: 0.9587 - loss: 0.2330\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_015000.keras\n",
      "\u001b[1m 19999/134246\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:12:10\u001b[0m 321ms/step - accuracy: 0.9593 - loss: 0.2293\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_020000.keras\n",
      "\u001b[1m 24999/134246\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:21:46\u001b[0m 276ms/step - accuracy: 0.9599 - loss: 0.2258\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_025000.keras\n",
      "\u001b[1m 29999/134246\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:05:49\u001b[0m 245ms/step - accuracy: 0.9604 - loss: 0.2229\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_030000.keras\n",
      "\u001b[1m 34999/134246\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:09:19\u001b[0m 223ms/step - accuracy: 0.9608 - loss: 0.2205\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_035000.keras\n",
      "\u001b[1m 39999/134246\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:25:05\u001b[0m 207ms/step - accuracy: 0.9612 - loss: 0.2183\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_040000.keras\n",
      "\u001b[1m 44999/134246\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:48:49\u001b[0m 194ms/step - accuracy: 0.9615 - loss: 0.2164\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_045000.keras\n",
      "\u001b[1m 49999/134246\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4:18:15\u001b[0m 184ms/step - accuracy: 0.9618 - loss: 0.2147\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_050000.keras\n",
      "\u001b[1m 54999/134246\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3:51:53\u001b[0m 176ms/step - accuracy: 0.9621 - loss: 0.2132\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_055000.keras\n",
      "\u001b[1m 59999/134246\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3:28:59\u001b[0m 169ms/step - accuracy: 0.9623 - loss: 0.2119\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_060000.keras\n",
      "\u001b[1m 64999/134246\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:08:26\u001b[0m 163ms/step - accuracy: 0.9626 - loss: 0.2107\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_065000.keras\n",
      "\u001b[1m 69999/134246\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:49:44\u001b[0m 159ms/step - accuracy: 0.9628 - loss: 0.2096\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_070000.keras\n",
      "\u001b[1m 74999/134246\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2:32:29\u001b[0m 154ms/step - accuracy: 0.9630 - loss: 0.2086\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_075000.keras\n",
      "\u001b[1m 79999/134246\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2:16:22\u001b[0m 151ms/step - accuracy: 0.9631 - loss: 0.2076\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_080000.keras\n",
      "\u001b[1m 84999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2:01:15\u001b[0m 148ms/step - accuracy: 0.9633 - loss: 0.2067\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_085000.keras\n",
      "\u001b[1m 89999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:46:57\u001b[0m 145ms/step - accuracy: 0.9635 - loss: 0.2059\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_090000.keras\n",
      "\u001b[1m 94999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:33:17\u001b[0m 143ms/step - accuracy: 0.9636 - loss: 0.2052\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_095000.keras\n",
      "\u001b[1m 99999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:20:11\u001b[0m 140ms/step - accuracy: 0.9638 - loss: 0.2044\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_100000.keras\n",
      "\u001b[1m104999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:07:32\u001b[0m 139ms/step - accuracy: 0.9639 - loss: 0.2038\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_105000.keras\n",
      "\u001b[1m109999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m55:18\u001b[0m 137ms/step - accuracy: 0.9640 - loss: 0.2031\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_110000.keras\n",
      "\u001b[1m114999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m43:23\u001b[0m 135ms/step - accuracy: 0.9641 - loss: 0.2025\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_115000.keras\n",
      "\u001b[1m119999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m31:47\u001b[0m 134ms/step - accuracy: 0.9643 - loss: 0.2020\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_120000.keras\n",
      "\u001b[1m124999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20:26\u001b[0m 133ms/step - accuracy: 0.9644 - loss: 0.2014\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_125000.keras\n",
      "\u001b[1m127161/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15:35\u001b[0m 132ms/step - accuracy: 0.9644 - loss: 0.2012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 17:46:33.785460: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-27 17:46:36.018105: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:37.223742: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:37.349095: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_24', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:38.334368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10307', 752 bytes spill stores, 728 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:38.364556: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10232', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:38.628331: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 1292 bytes spill stores, 1260 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:38.790159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7991', 752 bytes spill stores, 728 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.077590: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10307', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.162868: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 712 bytes spill stores, 708 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.447456: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7991', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.447617: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_99', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.521529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.758168: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10338', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:39.883779: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5', 1224 bytes spill stores, 1176 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:40.068099: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 1292 bytes spill stores, 1260 bytes spill loads\n",
      "\n",
      "2025-08-27 17:46:57.545837: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_1066', 540 bytes spill stores, 540 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_1076', 540 bytes spill stores, 540 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_1078', 540 bytes spill stores, 540 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129999/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9:17\u001b[0m 131ms/step - accuracy: 0.9645 - loss: 0.2009\n",
      "Epoch 1: saving model to small_best_checkpoints/sp_model_epoch_130000.keras\n",
      "\u001b[1m134246/134246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9646 - loss: 0.2005"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 17:57:35.391762: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-27 18:02:37.176373: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: DATA_LOSS: corrupted record at 593614\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-08-27 18:02:37.176498: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4960750327556710173\n",
      "2025-08-27 18:02:37.176665: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7756871781465258070\n",
      "2025-08-27 18:02:37.176715: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12456303280770371262\n"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": "Graph execution error:\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_568712/1373718217.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_568712/1373718217.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n2 root error(s) found.\n  (0) DATA_LOSS:  corrupted record at 593614\n\t [[{{node IteratorGetNext}}]]\n\t [[IteratorGetNext/_6]]\n  (1) DATA_LOSS:  corrupted record at 593614\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_3033968]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataLossError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m model.compile(\n\u001b[32m     11\u001b[39m     optimizer=keras.optimizers.AdamW(learning_rate=\u001b[32m1e-4\u001b[39m),\n\u001b[32m     12\u001b[39m     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     13\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m iterations = \u001b[32m130000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Save every epoch\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmall_best_checkpoints/sp_model_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{batch:06d}\u001b[39;49;00m\u001b[33;43m.keras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Save best model based on val_loss\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msmall_best_checkpoints/sp_model_best.keras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Early stopping to prevent overfitting\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Reduce learning rate when plateauing\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# CSV logging\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCSVLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msp_training_log.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Simplified TensorBoard\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs_sp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhistogram_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprofile_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwrite_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mupdate_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mDataLossError\u001b[39m: Graph execution error:\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_568712/1373718217.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nDetected at node IteratorGetNext defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_568712/1373718217.py\", line 18, in <module>\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n2 root error(s) found.\n  (0) DATA_LOSS:  corrupted record at 593614\n\t [[{{node IteratorGetNext}}]]\n\t [[IteratorGetNext/_6]]\n  (1) DATA_LOSS:  corrupted record at 593614\n\t [[{{node IteratorGetNext}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_3033968]"
     ]
    }
   ],
   "source": [
    "# Solution 1: Build the model first, then load weights\n",
    "# Build the model by calling it on some sample data\n",
    "sample_batch = next(iter(train_ds))\n",
    "_ = model(sample_batch[0])  # This builds the model\n",
    "\n",
    "# Now load weights\n",
    "model.load_weights(\"/home/akshat/GPT_from_scratch/notebooks/small_checkpoints/sp_model_step_130000.keras\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=1e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "iterations = 130000\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=iterations // steps_per_epoch,\n",
    "    epochs=300,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[\n",
    "        # Save every epoch\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"small_best_checkpoints/sp_model_epoch_{batch:06d}.keras\",\n",
    "            save_freq=5000,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        \n",
    "        # Save best model based on val_loss\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"small_best_checkpoints/sp_model_best.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            verbose=1,\n",
    "        ),\n",
    "        \n",
    "        # Early stopping to prevent overfitting\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=10, \n",
    "            restore_best_weights=True, \n",
    "            verbose=1,\n",
    "            monitor=\"val_loss\"\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate when plateauing\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            verbose=1, \n",
    "            monitor=\"val_loss\", \n",
    "            mode=\"min\"\n",
    "        ),\n",
    "        \n",
    "        # CSV logging\n",
    "        keras.callbacks.CSVLogger(\"sp_training_log.csv\"),\n",
    "        \n",
    "        # Simplified TensorBoard\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=\"./logs_sp\", \n",
    "            histogram_freq=0,\n",
    "            profile_batch=0, \n",
    "            write_graph=False,\n",
    "            update_freq='epoch'\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 12:52:36.009202: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=300,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[\n",
    "        # Save every 5000 steps (mid-epoch snapshots)\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='small_checkpoints/sp_model_step_{batch:06d}.keras',\n",
    "            save_freq=5000,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Save best model based on val_loss\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='small_best_checkpoints/sp_model_best.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Early stopping to prevent overfitting\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=10, \n",
    "            restore_best_weights=True, \n",
    "            verbose=1\n",
    "        ),\n",
    "        # Reduce learning rate when plateauing\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            verbose=1,\n",
    "            monitor='val_loss',\n",
    "            mode='min'\n",
    "        ),\n",
    "        # CSV logging\n",
    "        keras.callbacks.CSVLogger('sp_training_log.csv'),\n",
    "        # TensorBoard\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=\"./logs_sp\",\n",
    "            histogram_freq=1,\n",
    "            profile_batch=0,\n",
    "            write_graph=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effe305",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (667066463.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSave every 5000 steps (mid-epoch snapshots)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset,val_dataset,steps_per_epoch = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d131044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_24, val_ds_24, steps_24 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=24\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_32, val_ds_32, steps_32 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_44, val_ds_44, steps_44 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=44\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9933902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_64, val_ds_64, steps_64 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTEXT_LEN = 128\n",
    "# D_model = 128\n",
    "# VOCAB_SIZE = len(token_to_id_dict) # 94 currently char level\n",
    "# sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)\n",
    "\n",
    "# model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "\n",
    "# # Compile your model (same as before)\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now training should work\n",
    "# history = model.fit(\n",
    "#     train_ds_64,\n",
    "#     validation_data=val_ds_64,\n",
    "#     epochs=2,\n",
    "#     steps_per_epoch=steps_64,\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint(filepath='model_epoch_{epoch:02d}.keras',save_freq='epoch'),  # saves with epoch number   save_freq='epoch',                        # save every epoch    save_best_only=False,                     # save all epochs    verbose=1)\n",
    "#         keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, verbose=1),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, verbose=1),\n",
    "#         keras.callbacks.CSVLogger('training_log.csv'),\n",
    "#         keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1, profile_batch=0)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45dbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Positional embeddings are working. Shape: (1, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "# pick a small dummy batch\n",
    "import tensorflow as tf\n",
    "\n",
    "dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)  # batch_size=1, length=CONTEXT_LEN\n",
    "dum_model.save('model_epoch_1.keras')\n",
    "# run the embeddings layer only\n",
    "pos_layer = dum_model.get_layer('init_embeddings')  # or however your layer is named\n",
    "try:\n",
    "    pos_emb = pos_layer(dummy_input)\n",
    "    print(\"✅ Positional embeddings are working. Shape:\", pos_emb.shape)\n",
    "except Exception as e:\n",
    "    print(\"❌ Embedding test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'final_gpt_model.keras'\n",
      "Training history saved as 'training_history.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the final model\n",
    "model.save('final_gpt_model_sentence_piece.keras')\n",
    "print(\"Model saved as 'final_gpt_model_sentence_piece.keras'\")\n",
    "\n",
    "# Optional: Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved as 'training_history_sentence_piece.pkl'\")\n",
    "\n",
    "# Step 5: Load model later (when needed)\n",
    "def load_trained_model():\n",
    "    \"\"\"Load your saved model\"\"\"\n",
    "    loaded_model = keras.models.load_model('best_model_sentence_piece.keras')  # or 'final_gpt_model.keras'\n",
    "    return loaded_model\n",
    "\n",
    "# Usage for inference later:\n",
    "# model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ce64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257939/2192788961.py:82: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:5714\n",
      "* Running on public URL: https://e2d87bd39408007117.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e2d87bd39408007117.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import keras\n",
    "\n",
    "# Load trained GPT model\n",
    "model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/best_checkpoints/sp_model_epoch_01.keras')\n",
    "\n",
    "# Load your pre-trained SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model')  # Path to your .model file\n",
    "\n",
    "CONTEXT_LEN = 128  # Should match your model's context length\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_k=10):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize prompt with SentencePiece\n",
    "    input_tokens = sp.encode_as_ids(prompt)\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Prepare inputs\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        logits = model((input_ids, attention_mask), training=False)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample next token with top-k\n",
    "        next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        \n",
    "        # Stop on padding token id or EOS token id (adjust if needed)\n",
    "        if next_token == sp.pad_id() or next_token == sp.eos_id():\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = sp.decode_ids(new_tokens)\n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k)\n",
    "    history.append((message, bot_response))\n",
    "    return \"\", history\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"My Generative Pre-trained transformer (GPT) Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Akshat's My GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with SentencePiece tokenizer. And NO i DON'T Use any API calls or local LLM's Akshat pre-trained me on wikipedia data from scratch , so be gentle :)\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=10, maximum=200, value=100, step=10, label=\"Max Length\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=10, step=1, label=\"Top-K Sampling\")\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6000,\n",
    "        show_error=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d11a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sinusoidal_lookup_table.shape)  # should be (CONTEXT_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67921403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive testing for YOUR custom model implementation...\n",
      "\n",
      "🔍 Testing sinusoidal lookup table creation...\n",
      "✅ Sinusoidal lookup table created successfully. Shape: (128, 128)\n",
      "   Table type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "   Table dtype: <dtype: 'float32'>\n",
      "\n",
      "🔍 Testing InitializePositionalEmbeddings layer...\n",
      "❌ Positional embeddings test failed: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing LayerNormalization layer...\n",
      "✅ LayerNormalization working. Input shape: (2, 128, 128), Output shape: (2, 128, 128)\n",
      "   Output mean (should be ~0): 0.000000\n",
      "   Output variance (should be ~1): 0.999990\n",
      "\n",
      "🔍 Testing YOUR SelfAttentionLayer...\n",
      "   Input embeddings shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "   Mask sample - seq 1: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "✅ SelfAttentionLayer working. Output shape: (2, 128, 128)\n",
      "   Output range: [-0.7532, 0.7810]\n",
      "   Output mean: 0.0010\n",
      "   Output std: 0.0782\n",
      "   Attention heads: 8\n",
      "   d_head: 16\n",
      "   d_model: 128\n",
      "\n",
      "🔍 Testing YOUR DecoderBlock...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 29, in test_positional_embeddings\n",
      "    pos_layer = InitializePositionalEmbeddings(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_206177/3965528913.py\", line 11, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Input shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "✅ DecoderBlock working (training=False). Output shape: (2, 128, 128)\n",
      "✅ DecoderBlock working (training=True). Output shape: (2, 128, 128)\n",
      "   Input mean: 0.0003, Output mean: -0.0041\n",
      "   Output range: [-4.3376, 4.7553]\n",
      "\n",
      "🔍 Testing YOUR complete GPT model...\n",
      "❌ Full model test failed: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing the specific embedding issue from your original code...\n",
      "❌ Original embedding test still fails: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n",
      "\n",
      "======================================================================\n",
      "🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\n",
      "======================================================================\n",
      "Sinusoidal Lookup Table........................... ✅ PASSED\n",
      "Positional Embeddings............................. ❌ FAILED\n",
      "Layer Normalization............................... ✅ PASSED\n",
      "YOUR Self Attention Layer......................... ✅ PASSED\n",
      "YOUR Decoder Block................................ ✅ PASSED\n",
      "YOUR Full Model Forward Pass...................... ❌ FAILED\n",
      "YOUR Original Embedding Issue..................... ❌ FAILED\n",
      "Model Compilation & Training...................... ❌ FAILED\n",
      "\n",
      "🏆 Overall: 4/8 tests passed\n",
      "⚠️  Some tests failed. Please fix the issues before training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 186, in test_your_full_model\n",
      "    model = GPT(\n",
      "            ^^^^\n",
      "  File \"/tmp/ipykernel_206177/1791215164.py\", line 64, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/models/model.py\", line 158, in __init__\n",
      "    Layer.__init__(self, *args, **kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 255, in test_specific_embedding_issue\n",
      "    dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "CONTEXT_LEN = 128\n",
    "D_MODEL = 128\n",
    "VOCAB_SIZE = 94\n",
    "\n",
    "def test_sinusoidal_lookup_table():\n",
    "    \"\"\"Test the sinusoidal lookup table creation\"\"\"\n",
    "    print(\"🔍 Testing sinusoidal lookup table creation...\")\n",
    "    try:\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        print(f\"✅ Sinusoidal lookup table created successfully. Shape: {sinusoidal_lookup_table.shape}\")\n",
    "        print(f\"   Table type: {type(sinusoidal_lookup_table)}\")\n",
    "        print(f\"   Table dtype: {sinusoidal_lookup_table.dtype}\")\n",
    "        return sinusoidal_lookup_table\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sinusoidal lookup table creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_positional_embeddings(sinusoidal_lookup_table):\n",
    "    \"\"\"Test the positional embeddings layer\"\"\"\n",
    "    print(\"\\n🔍 Testing InitializePositionalEmbeddings layer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create the layer\n",
    "        pos_layer = InitializePositionalEmbeddings(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_pos_embeddings\"\n",
    "        )\n",
    "        \n",
    "        # Test input\n",
    "        dummy_input = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Positional embeddings working. Output shape: {pos_emb.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {D_MODEL})\")\n",
    "        \n",
    "        # Check if embeddings are reasonable\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(pos_emb):.4f}\")\n",
    "        \n",
    "        return pos_layer, pos_emb\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Positional embeddings test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_layer_normalization():\n",
    "    \"\"\"Test LayerNormalization layer\"\"\"\n",
    "    print(\"\\n🔍 Testing LayerNormalization layer...\")\n",
    "    \n",
    "    try:\n",
    "        ln = LayerNormalization(eps=1e-5, name=\"test_ln\")\n",
    "        test_input = tf.random.normal((2, CONTEXT_LEN, D_MODEL))\n",
    "        \n",
    "        output = ln(test_input)\n",
    "        print(f\"✅ LayerNormalization working. Input shape: {test_input.shape}, Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check normalization properties\n",
    "        mean = tf.reduce_mean(output, axis=-1)\n",
    "        var = tf.reduce_mean(tf.square(output - tf.expand_dims(mean, -1)), axis=-1)\n",
    "        print(f\"   Output mean (should be ~0): {tf.reduce_mean(mean):.6f}\")\n",
    "        print(f\"   Output variance (should be ~1): {tf.reduce_mean(var):.6f}\")\n",
    "        \n",
    "        return ln, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LayerNormalization test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_self_attention():\n",
    "    \"\"\"Test YOUR custom SelfAttentionLayer\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR SelfAttentionLayer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your attention layer with correct parameter name\n",
    "        attn_layer = SelfAttentionLayer(attention_heads=8, name=\"test_attention\")\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        embeddings = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        # Make some positions masked (set to 0)\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 50:] = 0  # Mask second half of first sequence\n",
    "        attention_mask[1, 80:] = 0  # Mask last part of second sequence\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample - seq 1: {attention_mask[0, :10].numpy()} ... {attention_mask[0, -10:].numpy()}\")\n",
    "        \n",
    "        # Test with your layer's expected input format: (embeddings, mask)\n",
    "        output = attn_layer([embeddings, attention_mask])\n",
    "        print(f\"✅ SelfAttentionLayer working. Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(output):.4f}\")\n",
    "        print(f\"   Output std: {tf.math.reduce_std(output):.4f}\")\n",
    "        \n",
    "        # Verify attention heads are working\n",
    "        print(f\"   Attention heads: {attn_layer.attention_heads}\")\n",
    "        print(f\"   d_head: {attn_layer.d_head}\")\n",
    "        print(f\"   d_model: {attn_layer.d_model}\")\n",
    "        \n",
    "        return attn_layer, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ SelfAttentionLayer test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_decoder_block():\n",
    "    \"\"\"Test YOUR custom DecoderBlock\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR DecoderBlock...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your decoder block\n",
    "        decoder = DecoderBlock(\n",
    "            d_model=D_MODEL,\n",
    "            n_heads=8,\n",
    "            dropout_rate=0.1,\n",
    "            epsilon=1e-5,\n",
    "            name=\"test_decoder\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        test_input = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        \n",
    "        # Make some positions masked\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 60:] = 0\n",
    "        attention_mask[1, 90:] = 0\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input shape: {test_input.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Test training=False\n",
    "        output = decoder(test_input, attention_mask, training=False)\n",
    "        print(f\"✅ DecoderBlock working (training=False). Output shape: {output.shape}\")\n",
    "        \n",
    "        # Test training=True\n",
    "        output_train = decoder(test_input, attention_mask, training=True)\n",
    "        print(f\"✅ DecoderBlock working (training=True). Output shape: {output_train.shape}\")\n",
    "        \n",
    "        # Check residual connections work (output should be different from input)\n",
    "        input_mean = tf.reduce_mean(test_input)\n",
    "        output_mean = tf.reduce_mean(output)\n",
    "        print(f\"   Input mean: {input_mean:.4f}, Output mean: {output_mean:.4f}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        \n",
    "        return decoder, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ DecoderBlock test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_full_model(sinusoidal_lookup_table):\n",
    "    \"\"\"Test YOUR complete GPT model\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR complete GPT model...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you do\n",
    "        model = GPT(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            context_length=CONTEXT_LEN,\n",
    "            attention_heads=8,\n",
    "            epsilon=1e-5,\n",
    "            decoder_blocks=4,\n",
    "            dropout_rate=0.1,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_gpt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        token_ids = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        attention_mask = tf.ones((1, CONTEXT_LEN), dtype=tf.float32)\n",
    "        \n",
    "        # Set mask to 0 for padding tokens\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 5:] = 0  # Only first 5 tokens are real\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Token IDs shape: {token_ids.shape}\")\n",
    "        print(f\"   Token IDs sample: {token_ids[0, :10].numpy()}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample: {attention_mask[0, :10].numpy()}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        logits = model([token_ids, attention_mask], training=False)\n",
    "        print(f\"✅ Full model forward pass successful!\")\n",
    "        print(f\"   Output logits shape: {logits.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {VOCAB_SIZE})\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Logits range: [{tf.reduce_min(logits):.4f}, {tf.reduce_max(logits):.4f}]\")\n",
    "        print(f\"   Logits mean: {tf.reduce_mean(logits):.4f}\")\n",
    "        \n",
    "        # Test with different batch size\n",
    "        token_ids_batch = tf.constant([\n",
    "            [1, 2, 3] + [0] * (CONTEXT_LEN - 3),\n",
    "            [4, 5, 6, 7] + [0] * (CONTEXT_LEN - 4)\n",
    "        ], dtype=tf.int32)\n",
    "        attention_mask_batch = tf.ones((2, CONTEXT_LEN), dtype=tf.float32)\n",
    "        attention_mask_batch = attention_mask_batch.numpy()\n",
    "        attention_mask_batch[0, 3:] = 0  # First seq has 3 real tokens\n",
    "        attention_mask_batch[1, 4:] = 0  # Second seq has 4 real tokens\n",
    "        attention_mask_batch = tf.constant(attention_mask_batch)\n",
    "        \n",
    "        logits_batch = model([token_ids_batch, attention_mask_batch], training=False)\n",
    "        print(f\"✅ Batch processing successful! Output shape: {logits_batch.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        logits_train = model([token_ids, attention_mask], training=True)\n",
    "        print(f\"✅ Training mode successful! Output shape: {logits_train.shape}\")\n",
    "        \n",
    "        return model, logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Full model test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_specific_embedding_issue():\n",
    "    \"\"\"Test the specific embedding issue you encountered\"\"\"\n",
    "    print(\"\\n🔍 Testing the specific embedding issue from your original code...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you did\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
    "        \n",
    "        # Test exactly as you did\n",
    "        dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)\n",
    "        \n",
    "        # Get the embeddings layer\n",
    "        pos_layer = dum_model.get_layer('init_embeddings')\n",
    "        \n",
    "        # Run the embeddings layer\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Your original embedding test now works! Shape: {pos_emb.shape}\")\n",
    "        print(f\"   Input was all zeros: {dummy_input[0, :5].numpy()}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Original embedding test still fails: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_model_compilation_and_training(model):\n",
    "    \"\"\"Test model compilation and training capability\"\"\"\n",
    "    print(\"\\n🔍 Testing model compilation and training...\")\n",
    "    \n",
    "    try:\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.AdamW(learning_rate=1e-4),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"✅ Model compilation successful!\")\n",
    "        \n",
    "        # Create dummy training data\n",
    "        batch_size = 4\n",
    "        dummy_x = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        dummy_mask = tf.ones((batch_size, CONTEXT_LEN), dtype=tf.float32)\n",
    "        # Create some realistic masking\n",
    "        for i in range(batch_size):\n",
    "            seq_len = tf.random.uniform([], minval=10, maxval=CONTEXT_LEN, dtype=tf.int32)\n",
    "            dummy_mask = dummy_mask.numpy()\n",
    "            dummy_mask[i, seq_len:] = 0\n",
    "            dummy_mask = tf.constant(dummy_mask)\n",
    "        \n",
    "        dummy_y = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        \n",
    "        print(f\"   Training data shapes: X={dummy_x.shape}, mask={dummy_mask.shape}, Y={dummy_y.shape}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        predictions = model.predict([dummy_x, dummy_mask], verbose=0)\n",
    "        print(f\"✅ Model prediction successful! Predictions shape: {predictions.shape}\")\n",
    "        \n",
    "        # Test training step\n",
    "        loss = model.train_on_batch([dummy_x, dummy_mask], dummy_y)\n",
    "        print(f\"✅ Training step successful! Loss: {loss}\")\n",
    "        \n",
    "        # Test model summary\n",
    "        print(f\"\\n📊 Model has {model.count_params():,} parameters\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model compilation/training test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def run_all_tests_for_your_model():\n",
    "    \"\"\"Run all tests specifically for your model implementation\"\"\"\n",
    "    print(\"🚀 Starting comprehensive testing for YOUR custom model implementation...\\n\")\n",
    "    \n",
    "    # Test 1: Sinusoidal lookup table\n",
    "    sinusoidal_lookup_table = test_sinusoidal_lookup_table()\n",
    "    if sinusoidal_lookup_table is None:\n",
    "        print(\"❌ Cannot proceed without sinusoidal lookup table\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Positional embeddings\n",
    "    pos_layer, pos_emb = test_positional_embeddings(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 3: Layer normalization\n",
    "    ln_layer, ln_output = test_layer_normalization()\n",
    "    \n",
    "    # Test 4: Your self attention\n",
    "    attn_layer, attn_output = test_your_self_attention()\n",
    "    \n",
    "    # Test 5: Your decoder block\n",
    "    decoder_layer, decoder_output = test_your_decoder_block()\n",
    "    \n",
    "    # Test 6: Your full model\n",
    "    model, logits = test_your_full_model(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 7: Your specific embedding issue\n",
    "    embedding_issue_fixed = test_specific_embedding_issue()\n",
    "    \n",
    "    # Test 8: Model compilation and training\n",
    "    compilation_success = False\n",
    "    if model is not None:\n",
    "        compilation_success = test_model_compilation_and_training(model)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Sinusoidal Lookup Table\", sinusoidal_lookup_table is not None),\n",
    "        (\"Positional Embeddings\", pos_emb is not None),\n",
    "        (\"Layer Normalization\", ln_output is not None),\n",
    "        (\"YOUR Self Attention Layer\", attn_output is not None),\n",
    "        (\"YOUR Decoder Block\", decoder_output is not None),\n",
    "        (\"YOUR Full Model Forward Pass\", logits is not None),\n",
    "        (\"YOUR Original Embedding Issue\", embedding_issue_fixed),\n",
    "        (\"Model Compilation & Training\", compilation_success)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, result in tests if result)\n",
    "    total = len(tests)\n",
    "    \n",
    "    for test_name, result in tests:\n",
    "        status = \"✅ PASSED\" if result else \"❌ FAILED\"\n",
    "        print(f\"{test_name:.<50} {status}\")\n",
    "    \n",
    "    print(f\"\\n🏆 Overall: {passed}/{total} tests passed\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"🎉 All tests passed! Your model is working perfectly!\")\n",
    "        print(\"🚀 Your model is ready for training and inference!\")\n",
    "    elif passed >= total - 2:\n",
    "        print(\"🎊 Almost all tests passed! Your model is mostly working correctly!\")\n",
    "        print(\"🔧 Check the failed tests above for minor issues.\")\n",
    "    else:\n",
    "        print(\"⚠️  Some tests failed. Please fix the issues before training.\")\n",
    "    \n",
    "    return model if logits is not None else None\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    final_model = run_all_tests_for_your_model()\n",
    "    \n",
    "    if final_model is not None:\n",
    "        print(f\"\\n🎁 Model returned successfully!\")\n",
    "        print(f\"   Total parameters: {final_model.count_params():,}\")\n",
    "        print(f\"   Ready for: training, inference, and saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_bot(model, token_to_id_dict, prompt, max_length=100, temperature=0.7, context_len=128):\n",
    "    \"\"\"Generate text response from your GPT model\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_tokens = [token_to_id_dict.get(char, 0) for char in prompt]\n",
    "    \n",
    "    # Handle context length\n",
    "    if len(input_tokens) > context_len:\n",
    "        input_tokens = input_tokens[-context_len:]\n",
    "    \n",
    "    # Pad to context length\n",
    "    input_ids = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        input_ids[-len(input_tokens):] = input_tokens\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        attention_mask[-len(input_tokens):] = 1\n",
    "    \n",
    "    # Prepare for model\n",
    "    input_ids = np.expand_dims(input_ids, axis=0)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    \n",
    "    # Generate response token by token\n",
    "    generated_tokens = input_tokens.copy()\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        \n",
    "        # Get last token logits (find the last non-zero position in attention mask)\n",
    "        last_pos = np.sum(attention_mask[0]) - 1\n",
    "        if last_pos < 0:\n",
    "            last_pos = 0\n",
    "        next_token_logits = predictions[0, last_pos, :] / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "        \n",
    "        # Stop if we hit a stop token or newline\n",
    "        if next_token == 0 or (next_token in token_to_id_dict.values() and id_to_token[next_token] == '\\n'):\n",
    "            break\n",
    "            \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Update input for next iteration\n",
    "        if len(generated_tokens) > context_len:\n",
    "            generated_tokens = generated_tokens[-context_len:]\n",
    "        \n",
    "        # Create new input\n",
    "        new_input_ids = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        \n",
    "        new_attention_mask = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        input_ids = new_input_ids\n",
    "        attention_mask = new_attention_mask\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    response_tokens = generated_tokens[len(input_tokens):]  # Only the new tokens\n",
    "    response = ''.join([id_to_token.get(token, '') for token in response_tokens])\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b200e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"]8'Vi3A;#sFöXöxr3ö][xöM6!dlwx—$pb:Orxx1JkW0:pöyyö;94œ!ööGHQöG:‘::$fwrg3Rg!R!/gxrgg/PöJIYPlö6öJ%6RpLp\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_from_bot(model,token_to_id_dict,prompt = 'yoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
