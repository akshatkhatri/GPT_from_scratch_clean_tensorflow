{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04d5ab2",
   "metadata": {},
   "source": [
    "# GPT Experimental Development - Messy Prototype\n",
    "\n",
    "This is my experimental notebook where I'm figuring out how to build a GPT from scratch. \n",
    "Lots of trial and error, debugging prints, and quick hacks here.\n",
    "\n",
    "**TODO**: Clean this up later into proper modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c26d86",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Just importing everything I might need... will clean this up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me just import everything I might need\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Some imports I'm not sure I need but keeping just in case\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Quick path setup - hardcoded for now\n",
    "PROJECT_PATH = '/home/akshat/GPT_from_scratch'\n",
    "DATA_PATH = f'{PROJECT_PATH}/text_data'\n",
    "print(f\"Working from: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9453ca",
   "metadata": {},
   "source": [
    "## Messy Data Loading and Exploration\n",
    "\n",
    "Let me just load some data and see what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some text data - using Alice for now because it's small\n",
    "alice_path = f'{DATA_PATH}/alice_story.txt'\n",
    "\n",
    "try:\n",
    "    with open(alice_path, 'r', encoding='utf-8') as f:\n",
    "        text_data = f.read()\n",
    "    print(\"‚úì Loaded Alice story\")\n",
    "except:\n",
    "    print(\"‚ùå Failed to load Alice story\")\n",
    "    # Let me try a different file\n",
    "    alice_path = f'{DATA_PATH}/alice_extended.txt'\n",
    "    with open(alice_path, 'r', encoding='utf-8') as f:\n",
    "        text_data = f.read()\n",
    "    print(\"‚úì Loaded Alice extended\")\n",
    "\n",
    "print(f\"Text length: {len(text_data)} characters\")\n",
    "print(f\"First 200 chars: {text_data[:200]}\")\n",
    "print(f\"Last 200 chars: {text_data[-200:]}\")\n",
    "\n",
    "# Quick stats\n",
    "unique_chars = list(set(text_data))\n",
    "print(f\"\\nUnique characters: {len(unique_chars)}\")\n",
    "print(f\"Characters: {unique_chars[:20]}...\")  # Just show first 20\n",
    "\n",
    "# Word-level stats too\n",
    "words = text_data.split()\n",
    "print(f\"\\nTotal words: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")\n",
    "print(f\"First 10 words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374e94e",
   "metadata": {},
   "source": [
    "## Quick and Dirty Tokenizer Experiments\n",
    "\n",
    "Let me try different ways to tokenize this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5489668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenizer - simplest approach\n",
    "def char_tokenizer(text):\n",
    "    \"\"\"Dead simple character tokenizer\"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "    id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return char_to_id, id_to_char, chars\n",
    "\n",
    "char_to_id, id_to_char, vocab = char_tokenizer(text_data)\n",
    "print(f\"Character vocab size: {len(vocab)}\")\n",
    "print(f\"Vocab: {vocab}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello Alice!\"\n",
    "tokens = [char_to_id.get(ch, 0) for ch in test_text]  # 0 for unknown\n",
    "reconstructed = ''.join([id_to_char.get(tok, '?') for tok in tokens])\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Reconstructed: '{reconstructed}'\")\n",
    "\n",
    "# Let's also try a simple word tokenizer\n",
    "def simple_word_tokenizer(text):\n",
    "    \"\"\"Basic word tokenizer with some cleaning\"\"\"\n",
    "    # Simple cleaning\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    vocab = sorted(list(set(words)))\n",
    "    word_to_id = {word: i+1 for i, word in enumerate(vocab)}  # Start from 1, 0 for UNK\n",
    "    word_to_id['<UNK>'] = 0\n",
    "    id_to_word = {i: word for word, i in word_to_id.items()}\n",
    "    \n",
    "    return word_to_id, id_to_word, vocab\n",
    "\n",
    "word_to_id, id_to_word, word_vocab = simple_word_tokenizer(text_data)\n",
    "print(f\"\\nWord vocab size: {len(word_vocab)}\")\n",
    "print(f\"First 20 words: {word_vocab[:20]}\")\n",
    "\n",
    "# Test word tokenization\n",
    "test_words = \"hello alice how are you?\".split()\n",
    "word_tokens = [word_to_id.get(word, 0) for word in test_words]\n",
    "print(f\"\\nWord test: {test_words}\")\n",
    "print(f\"Word tokens: {word_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b04e9",
   "metadata": {},
   "source": [
    "## Rough Model Architecture Prototyping\n",
    "\n",
    "Let me build some basic transformer layers... this is gonna be messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me try to build a simple attention mechanism first\n",
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        print(f\"Creating attention with d_model={d_model}, heads={num_heads}, head_dim={self.head_dim}\")\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model) \n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        \n",
    "        # Get Q, K, V\n",
    "        q = self.wq(x)  # (batch, seq, d_model)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        q = tf.reshape(q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        k = tf.reshape(k, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        v = tf.reshape(v, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        \n",
    "        # Transpose to (batch, heads, seq, head_dim)\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        \n",
    "        # Apply causal mask for GPT\n",
    "        mask_val = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask_val = tf.where(mask_val == 0, -1e9, 0.0)\n",
    "        scores += mask_val\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, (batch_size, seq_len, self.d_model))\n",
    "        \n",
    "        # Final linear layer\n",
    "        return self.wo(out)\n",
    "\n",
    "# Test the attention layer\n",
    "print(\"Testing attention layer...\")\n",
    "attention = SimpleAttention(d_model=64, num_heads=4)\n",
    "test_input = tf.random.normal((2, 10, 64))  # (batch=2, seq=10, features=64)\n",
    "output = attention(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úì Attention layer works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8beedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let me build a simple transformer block\n",
    "class SimpleTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print(f\"Creating transformer block: d_model={d_model}, heads={num_heads}, ff_dim={ff_dim}\")\n",
    "        \n",
    "        self.attention = SimpleAttention(d_model, num_heads)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "        ])\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        # Self-attention + residual\n",
    "        attn_out = self.attention(x)\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        x1 = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward + residual  \n",
    "        ff_out = self.ff(x1)\n",
    "        ff_out = self.dropout2(ff_out, training=training)\n",
    "        return self.norm2(x1 + ff_out)\n",
    "\n",
    "# Test transformer block\n",
    "print(\"\\nTesting transformer block...\")\n",
    "block = SimpleTransformerBlock(d_model=64, num_heads=4, ff_dim=128)\n",
    "test_input = tf.random.normal((2, 10, 64))\n",
    "output = block(test_input, training=True)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úì Transformer block works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a08dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let me build a simple GPT model\n",
    "class SimpleGPT(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, ff_dim, max_len=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print(f\"Creating GPT: vocab={vocab_size}, d_model={d_model}, layers={num_layers}\")\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = [SimpleTransformerBlock(d_model, num_heads, ff_dim) \n",
    "                      for _ in range(num_layers)]\n",
    "        \n",
    "        # Output head\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.head = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = tf.range(seq_len)\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        \n",
    "        # Add embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x, training=training)\n",
    "            if i == 0:  # Debug print for first block\n",
    "                print(f\"After block {i}: shape {x.shape}, mean {tf.reduce_mean(x):.4f}\")\n",
    "        \n",
    "        # Final norm and head\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Test the full model\n",
    "print(\"\\nTesting full GPT model...\")\n",
    "vocab_size = len(char_to_id)  # Use our character vocab\n",
    "model = SimpleGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    ff_dim=128,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "# Test with some input\n",
    "test_sequence = [char_to_id.get(ch, 0) for ch in \"Hello Alice\"]\n",
    "test_input = tf.constant([test_sequence + [0] * (20 - len(test_sequence))])  # Pad to length 20\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "print(f\"Test input: {test_input}\")\n",
    "\n",
    "output = model(test_input, training=False)\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Output sample: {output[0, 0, :5]}\")\n",
    "print(\"‚úì Full model works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f23c00",
   "metadata": {},
   "source": [
    "## Hacky Training Loop with Print Debugging\n",
    "\n",
    "Let me write a quick training loop with lots of debug prints..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some training data quickly\n",
    "def prepare_training_data(text, tokenizer_dict, seq_len=32):\n",
    "    \"\"\"Quick and dirty data preparation\"\"\"\n",
    "    print(f\"Preparing training data with seq_len={seq_len}\")\n",
    "    \n",
    "    # Tokenize the entire text\n",
    "    tokens = [tokenizer_dict.get(ch, 0) for ch in text]\n",
    "    print(f\"Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    # Create input-target pairs\n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    for i in range(0, len(tokens) - seq_len, seq_len // 2):  # Overlapping windows\n",
    "        input_seq = tokens[i:i+seq_len]\n",
    "        target_seq = tokens[i+1:i+seq_len+1]\n",
    "        \n",
    "        if len(input_seq) == seq_len and len(target_seq) == seq_len:\n",
    "            inputs.append(input_seq)\n",
    "            targets.append(target_seq)\n",
    "    \n",
    "    print(f\"Created {len(inputs)} training examples\")\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# Prepare data\n",
    "seq_length = 32\n",
    "X_train, y_train = prepare_training_data(text_data[:5000], char_to_id, seq_length)  # Use only first 5k chars for speed\n",
    "print(f\"Training data shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Sample input: {X_train[0][:10]}\")\n",
    "print(f\"Sample target: {y_train[0][:10]}\")\n",
    "\n",
    "# Convert to tensorflow dataset\n",
    "batch_size = 8\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "print(f\"Created dataset with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small model for quick testing\n",
    "small_model = SimpleGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=32,  # Smaller for faster training\n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    ff_dim=64,\n",
    "    max_len=seq_length\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "print(\"Model compiled!\")\n",
    "\n",
    "# Manual training loop with lots of debugging\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = small_model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, small_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, small_model.trainable_variables))\n",
    "    \n",
    "    return loss, predictions\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "losses = []\n",
    "\n",
    "print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n=== EPOCH {epoch+1}/{epochs} ===\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for step, (batch_inputs, batch_targets) in enumerate(train_dataset):\n",
    "        loss, predictions = train_step(batch_inputs, batch_targets)\n",
    "        epoch_losses.append(loss.numpy())\n",
    "        \n",
    "        if step % 10 == 0:  # Print every 10 steps\n",
    "            print(f\"  Step {step:3d}: loss = {loss.numpy():.4f}\")\n",
    "            \n",
    "            # Quick sanity check - show a prediction\n",
    "            sample_pred = tf.nn.softmax(predictions[0, 0, :])  # First token of first example\n",
    "            top_pred = tf.argmax(sample_pred).numpy()\n",
    "            pred_char = id_to_char.get(top_pred, '?')\n",
    "            target_char = id_to_char.get(batch_targets[0, 0].numpy(), '?')\n",
    "            print(f\"    Predicted '{pred_char}' (confidence: {sample_pred[top_pred]:.3f}), Target: '{target_char}'\")\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"  Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b798b",
   "metadata": {},
   "source": [
    "## Model Testing with Manual Validation\n",
    "\n",
    "Let me test the model with some manual examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66908d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation\n",
    "def generate_text(model, start_text, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text from the model - quick and dirty version\"\"\"\n",
    "    print(f\"Generating text starting with: '{start_text}'\")\n",
    "    \n",
    "    # Convert start text to tokens\n",
    "    input_tokens = [char_to_id.get(ch, 0) for ch in start_text]\n",
    "    generated = input_tokens.copy()\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # Prepare input (last seq_length tokens)\n",
    "        current_seq = generated[-seq_length:]\n",
    "        if len(current_seq) < seq_length:\n",
    "            current_seq = [0] * (seq_length - len(current_seq)) + current_seq\n",
    "        \n",
    "        input_tensor = tf.constant([current_seq])\n",
    "        \n",
    "        # Get prediction\n",
    "        predictions = model(input_tensor, training=False)\n",
    "        next_token_logits = predictions[0, -1, :] / temperature\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        next_token = tf.random.categorical([next_token_logits], 1)[0, 0].numpy()\n",
    "        generated.append(next_token)\n",
    "        \n",
    "        # Convert to character and print progress\n",
    "        next_char = id_to_char.get(next_token, '?')\n",
    "        if i % 10 == 0:\n",
    "            current_text = ''.join([id_to_char.get(tok, '?') for tok in generated])\n",
    "            print(f\"  Step {i:2d}: '{current_text[-20:]}' (next: '{next_char}')\")\n",
    "    \n",
    "    # Convert back to text\n",
    "    generated_text = ''.join([id_to_char.get(tok, '?') for tok in generated])\n",
    "    return generated_text\n",
    "\n",
    "# Test generation\n",
    "test_prompts = [\"Alice\", \"The\", \"Once\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    generated = generate_text(small_model, prompt, max_length=30, temperature=0.8)\n",
    "    print(f\"\\nGenerated: '{generated}'\")\n",
    "    print(f\"Length: {len(generated)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick accuracy test\n",
    "def test_model_accuracy(model, test_inputs, test_targets, num_samples=50):\n",
    "    \"\"\"Quick accuracy calculation\"\"\"\n",
    "    print(f\"Testing model accuracy on {num_samples} samples...\")\n",
    "    \n",
    "    # Take a subset for quick testing\n",
    "    test_X = test_inputs[:num_samples]\n",
    "    test_y = test_targets[:num_samples]\n",
    "    \n",
    "    predictions = model(test_X, training=False)\n",
    "    predicted_tokens = tf.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Calculate token-level accuracy\n",
    "    correct = tf.cast(predicted_tokens == test_y, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    \n",
    "    print(f\"Token-level accuracy: {accuracy.numpy():.4f}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    for i in range(min(3, num_samples)):\n",
    "        input_text = ''.join([id_to_char.get(tok, '?') for tok in test_X[i]])\n",
    "        target_text = ''.join([id_to_char.get(tok, '?') for tok in test_y[i]])\n",
    "        pred_text = ''.join([id_to_char.get(tok, '?') for tok in predicted_tokens[i]])\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Input:  '{input_text[:30]}...'\")\n",
    "        print(f\"  Target: '{target_text[:30]}...'\")\n",
    "        print(f\"  Pred:   '{pred_text[:30]}...'\")\n",
    "    \n",
    "    return accuracy.numpy()\n",
    "\n",
    "# Test accuracy\n",
    "acc = test_model_accuracy(small_model, X_train, y_train, num_samples=50)\n",
    "print(f\"\\nFinal model accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49ff2e",
   "metadata": {},
   "source": [
    "## Performance Analysis with Ad-hoc Visualizations\n",
    "\n",
    "Let me make some quick plots to see how we did..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=2, marker='o')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "for i, loss in enumerate(losses):\n",
    "    plt.annotate(f'{loss:.3f}', (i, loss), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Token distribution in training data\n",
    "plt.subplot(2, 2, 2)\n",
    "token_counts = {}\n",
    "for ch in text_data[:1000]:  # Sample first 1000 chars\n",
    "    token_counts[ch] = token_counts.get(ch, 0) + 1\n",
    "\n",
    "chars = list(token_counts.keys())[:20]  # Top 20 chars\n",
    "counts = [token_counts[ch] for ch in chars]\n",
    "plt.bar(range(len(chars)), counts)\n",
    "plt.title('Top 20 Character Frequencies')\n",
    "plt.xlabel('Character')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(chars)), chars, rotation=45)\n",
    "\n",
    "# Model parameter count (rough estimate)\n",
    "plt.subplot(2, 2, 3)\n",
    "total_params = sum([tf.size(var).numpy() for var in small_model.trainable_variables])\n",
    "layer_sizes = [tf.size(var).numpy() for var in small_model.trainable_variables[:5]]  # First 5 layers\n",
    "layer_names = [f'Layer {i+1}' for i in range(len(layer_sizes))]\n",
    "\n",
    "plt.pie(layer_sizes, labels=layer_names, autopct='%1.1f%%')\n",
    "plt.title(f'Model Parameters Distribution\\nTotal: {total_params:,} params')\n",
    "\n",
    "# Generate some text and show character diversity\n",
    "plt.subplot(2, 2, 4)\n",
    "sample_generated = generate_text(small_model, \"Alice\", max_length=100, temperature=0.8)\n",
    "gen_char_counts = {}\n",
    "for ch in sample_generated:\n",
    "    gen_char_counts[ch] = gen_char_counts.get(ch, 0) + 1\n",
    "\n",
    "gen_chars = list(gen_char_counts.keys())[:15]\n",
    "gen_counts = [gen_char_counts[ch] for ch in gen_chars]\n",
    "plt.bar(range(len(gen_chars)), gen_counts, alpha=0.7, color='orange')\n",
    "plt.title('Generated Text Character Dist.')\n",
    "plt.xlabel('Character')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(gen_chars)), gen_chars, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"- Vocabulary size: {vocab_size}\")\n",
    "print(f\"- Total parameters: {total_params:,}\")\n",
    "print(f\"- Final training loss: {losses[-1]:.4f}\")\n",
    "print(f\"- Token accuracy: {acc:.4f}\")\n",
    "print(f\"- Training data size: {len(X_train)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7770faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me save this model quickly in case I want to use it later\n",
    "import os\n",
    "checkpoint_dir = f\"{PROJECT_PATH}/notebooks/messy_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_path = f\"{checkpoint_dir}/quick_gpt_prototype.keras\"\n",
    "small_model.save(model_path)\n",
    "print(f\"‚úì Saved model to: {model_path}\")\n",
    "\n",
    "# Also save the tokenizer\n",
    "tokenizer_path = f\"{checkpoint_dir}/char_tokenizer.json\"\n",
    "with open(tokenizer_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'char_to_id': char_to_id,\n",
    "        'id_to_char': id_to_char,\n",
    "        'vocab_size': vocab_size\n",
    "    }, f)\n",
    "print(f\"‚úì Saved tokenizer to: {tokenizer_path}\")\n",
    "\n",
    "# Quick notes for later\n",
    "notes = f\"\"\"\n",
    "Quick GPT Prototype - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "==================================================\n",
    "Model specs:\n",
    "- d_model: 32\n",
    "- num_heads: 2  \n",
    "- num_layers: 2\n",
    "- vocab_size: {vocab_size}\n",
    "- sequence_length: {seq_length}\n",
    "\n",
    "Training:\n",
    "- epochs: {epochs}\n",
    "- batch_size: {batch_size}\n",
    "- final_loss: {losses[-1]:.4f}\n",
    "- token_accuracy: {acc:.4f}\n",
    "\n",
    "TODO:\n",
    "- Clean up the architecture\n",
    "- Better data preprocessing\n",
    "- More training epochs\n",
    "- Proper evaluation metrics\n",
    "- Hyperparameter tuning\n",
    "\"\"\"\n",
    "\n",
    "notes_path = f\"{checkpoint_dir}/experiment_notes.txt\"\n",
    "with open(notes_path, 'w') as f:\n",
    "    f.write(notes)\n",
    "print(f\"‚úì Saved notes to: {notes_path}\")\n",
    "\n",
    "print(\"\\nüéâ Experiment complete! Time to clean this up into proper modules...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
