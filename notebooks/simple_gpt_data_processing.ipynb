{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4cffc",
   "metadata": {},
   "source": [
    "# Simple Data Processing for Character-level GPT\n",
    "This notebook demonstrates clean and simple steps for preparing data for a character-level GPT model:\n",
    "- Build vocabulary from text\n",
    "- Tokenize and pad batches\n",
    "- Prepare input-target pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dce33f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93d459a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 38\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from text file\n",
    "def build_vocab(text_path):\n",
    "    with open(text_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    vocab = sorted(set(text))\n",
    "    token_to_id = {ch: i for i, ch in enumerate(vocab)}\n",
    "    id_to_token = {i: ch for ch, i in token_to_id.items()}\n",
    "    return token_to_id, id_to_token, vocab\n",
    "text_path = '/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'\n",
    "token_to_id, id_to_token, vocab = build_vocab(text_path)\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b82358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [[ 0 16 23 23 26  0  0  0  0  0]\n",
      " [ 0 26 29 23 15  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "Attention Mask: [[0 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad batches of text\n",
    "def tokenize_and_pad(text_batch, token_to_id, max_seq_len, pad_value=0):\n",
    "    batch_token_ids = []\n",
    "    for text in text_batch:\n",
    "        ids = [token_to_id.get(c, pad_value) for c in text]\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        batch_token_ids.append(ids)\n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    return token_ids, attention_mask\n",
    "batch_text = ['Hello', 'World!', 'GPT']\n",
    "max_seq_len = 10\n",
    "token_ids, attention_mask = tokenize_and_pad(batch_text, token_to_id, max_seq_len)\n",
    "print('Token IDs:', token_ids)\n",
    "print('Attention Mask:', attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de9aa40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[ 0 16 23 23 26]\n",
      " [16 23 23 26  1]\n",
      " [23 23 26  1  0]\n",
      " [23 26  1  0  0]\n",
      " [26  1  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  0  1  0]\n",
      " [ 0  0  1  0 26]\n",
      " [ 0  1  0 26 29]\n",
      " [ 1  0 26 29 23]\n",
      " [ 0 26 29 23 15]]\n",
      "Targets: [[16 23 23 26  1]\n",
      " [23 23 26  1  0]\n",
      " [23 26  1  0  0]\n",
      " [26  1  0  0  0]\n",
      " [ 1  0  0  0  1]\n",
      " [ 0  0  0  1  0]\n",
      " [ 0  0  1  0 26]\n",
      " [ 0  1  0 26 29]\n",
      " [ 1  0 26 29 23]\n",
      " [ 0 26 29 23 15]\n",
      " [26 29 23 15  2]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare input-target pairs for training with sliding window\n",
    "def prepare_training_data(text, token_to_id, context_length=10, pad_value=0):\n",
    "    token_ids = [token_to_id.get(c, pad_value) for c in text]\n",
    "    inputs, targets = [], []\n",
    "    for i in range(0, len(token_ids) - context_length):\n",
    "        input_seq = token_ids[i:i+context_length]\n",
    "        target_seq = token_ids[i+1:i+context_length+1]\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    inputs = np.array(inputs, dtype=np.int32)\n",
    "    targets = np.array(targets, dtype=np.int32)\n",
    "    return inputs, targets\n",
    "inputs, targets = prepare_training_data('Hello GPT World!', token_to_id, context_length=5)\n",
    "print('Inputs:', inputs)\n",
    "print('Targets:', targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49ca9956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (limited): 10000\n",
      "Word vocab size: 10000\n",
      "Word Token IDs: [[   0  416    0    0    0    0]\n",
      " [   0   23   92    0    0    0]\n",
      " [   0 8602    0    0    0    0]]\n",
      "Word Attention Mask: [[0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Word-level vocabulary and tokenization (with max_words limit)\n",
    "from collections import Counter\n",
    "def build_word_vocab(text_path, max_words=10000):\n",
    "    with open(text_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common = word_counts.most_common(max_words)\n",
    "    vocab = [w for w, _ in most_common]\n",
    "    token_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_token = {i: word for word, i in token_to_id.items()}\n",
    "    print(f'Vocabulary size (limited): {len(vocab)}')\n",
    "    return token_to_id, id_to_token, vocab\n",
    "word_token_to_id, word_id_to_token, word_vocab = build_word_vocab(text_path, max_words=10000)\n",
    "print(f'Word vocab size: {len(word_vocab)}')\n",
    "\n",
    "def tokenize_and_pad_words(text_batch, token_to_id, max_seq_len, pad_value=0):\n",
    "    batch_token_ids = []\n",
    "    for text in text_batch:\n",
    "        words = text.split()\n",
    "        ids = [token_to_id.get(w, pad_value) for w in words]\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        batch_token_ids.append(ids)\n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    return token_ids, attention_mask\n",
    "batch_text_words = ['Hello world', 'GPT is great', 'Word level']\n",
    "max_seq_len_words = 6\n",
    "word_token_ids, word_attention_mask = tokenize_and_pad_words(batch_text_words, word_token_to_id, max_seq_len_words)\n",
    "print('Word Token IDs:', word_token_ids)\n",
    "print('Word Attention Mask:', word_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f67c32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Hello world this is a word-level GPT example\n",
      "Split words: ['Hello', 'world', 'this', 'is', 'a', 'word-level', 'GPT', 'example']\n",
      "Word IDs: [0, 416, 42, 23, 4, 0, 0, 3357]\n",
      "Window 0:\n",
      "  Input IDs: [0, 416, 42, 23]\n",
      "  Target IDs: [416, 42, 23, 4]\n",
      "Window 1:\n",
      "  Input IDs: [416, 42, 23, 4]\n",
      "  Target IDs: [42, 23, 4, 0]\n",
      "Window 2:\n",
      "  Input IDs: [42, 23, 4, 0]\n",
      "  Target IDs: [23, 4, 0, 0]\n",
      "Window 3:\n",
      "  Input IDs: [23, 4, 0, 0]\n",
      "  Target IDs: [4, 0, 0, 3357]\n",
      "Final input array: [[  0 416  42  23]\n",
      " [416  42  23   4]\n",
      " [ 42  23   4   0]\n",
      " [ 23   4   0   0]]\n",
      "Final target array: [[ 416   42   23    4]\n",
      " [  42   23    4    0]\n",
      " [  23    4    0    0]\n",
      " [   4    0    0 3357]]\n",
      "Word Inputs: [[  0 416  42  23]\n",
      " [416  42  23   4]\n",
      " [ 42  23   4   0]\n",
      " [ 23   4   0   0]]\n",
      "Word Targets: [[ 416   42   23    4]\n",
      " [  42   23    4    0]\n",
      " [  23    4    0    0]\n",
      " [   4    0    0 3357]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare word-level input-target pairs for training with sliding window (verbose for debugging)\n",
    "def prepare_word_training_data(text, token_to_id, context_length=6, pad_value=0):\n",
    "    print(f'Input text: {text}')\n",
    "    words = text.split()\n",
    "    print(f'Split words: {words}')\n",
    "    word_ids = [token_to_id.get(w, pad_value) for w in words]\n",
    "    print(f'Word IDs: {word_ids}')\n",
    "    inputs, targets = [], []\n",
    "    for i in range(0, len(word_ids) - context_length):\n",
    "        input_seq = word_ids[i:i+context_length]\n",
    "        target_seq = word_ids[i+1:i+context_length+1]\n",
    "        print(f'Window {i}:')\n",
    "        print(f'  Input IDs: {input_seq}')\n",
    "        print(f'  Target IDs: {target_seq}')\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    inputs = np.array(inputs, dtype=np.int32)\n",
    "    targets = np.array(targets, dtype=np.int32)\n",
    "    print('Final input array:', inputs)\n",
    "    print('Final target array:', targets)\n",
    "    return inputs, targets\n",
    "word_inputs, word_targets = prepare_word_training_data('Hello world this is a word-level GPT example', word_token_to_id, context_length=4)\n",
    "print('Word Inputs:', word_inputs)\n",
    "print('Word Targets:', word_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6494064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9619cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt\n",
      "Raw text sample:  sir walter elliot, of kellynch hall, in somersetshire, was a man who,\n",
      "for his own amusement, never \n",
      "Vocabulary: ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Token to ID mapping: {'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n",
      "ID to Token mapping: {0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: ':', 10: ';', 11: '?', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n",
      "Vocab size: 38\n",
      "Batch to tokenize: ['Hello', 'World!', 'GPT']\n",
      "Text: Hello -> IDs: [0, 16, 23, 23, 26]\n",
      "Padded IDs: [0, 16, 23, 23, 26, 0, 0, 0, 0, 0]\n",
      "Text: World! -> IDs: [0, 26, 29, 23, 15, 2]\n",
      "Padded IDs: [0, 26, 29, 23, 15, 2, 0, 0, 0, 0]\n",
      "Text: GPT -> IDs: [0, 0, 0]\n",
      "Padded IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs array: [[ 0 16 23 23 26  0  0  0  0  0]\n",
      " [ 0 26 29 23 15  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "Attention mask array: [[0 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "Raw text sample:  sir walter elliot, of kellynch hall, in somersetshire, was a man who,\n",
      "for his own amusement, never \n",
      "Vocabulary: ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Token to ID mapping: {'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n",
      "ID to Token mapping: {0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: ':', 10: ';', 11: '?', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n",
      "Vocab size: 38\n",
      "Batch to tokenize: ['Hello', 'World!', 'GPT']\n",
      "Text: Hello -> IDs: [0, 16, 23, 23, 26]\n",
      "Padded IDs: [0, 16, 23, 23, 26, 0, 0, 0, 0, 0]\n",
      "Text: World! -> IDs: [0, 26, 29, 23, 15, 2]\n",
      "Padded IDs: [0, 26, 29, 23, 15, 2, 0, 0, 0, 0]\n",
      "Text: GPT -> IDs: [0, 0, 0]\n",
      "Padded IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token IDs array: [[ 0 16 23 23 26  0  0  0  0  0]\n",
      " [ 0 26 29 23 15  2  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "Attention mask array: [[0 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Show every step for character-level processing\n",
    "def build_vocab_debug(text_path):\n",
    "    print(f'Reading file: {text_path}')\n",
    "    with open(text_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print('Raw text sample:', text[:100])\n",
    "    vocab = sorted(set(text))\n",
    "    print('Vocabulary:', vocab)\n",
    "    token_to_id = {ch: i for i, ch in enumerate(vocab)}\n",
    "    id_to_token = {i: ch for ch, i in token_to_id.items()}\n",
    "    print('Token to ID mapping:', token_to_id)\n",
    "    print('ID to Token mapping:', id_to_token)\n",
    "    print(f'Vocab size: {len(vocab)}')\n",
    "    return token_to_id, id_to_token, vocab\n",
    "token_to_id, id_to_token, vocab = build_vocab_debug(text_path)\n",
    "\n",
    "def tokenize_and_pad_debug(text_batch, token_to_id, max_seq_len, pad_value=0):\n",
    "    print('Batch to tokenize:', text_batch)\n",
    "    batch_token_ids = []\n",
    "    for text in text_batch:\n",
    "        ids = [token_to_id.get(c, pad_value) for c in text]\n",
    "        print(f'Text: {text} -> IDs: {ids}')\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "            print(f'Truncated IDs: {ids}')\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "            print(f'Padded IDs: {ids}')\n",
    "        batch_token_ids.append(ids)\n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    print('Token IDs array:', token_ids)\n",
    "    print('Attention mask array:', attention_mask)\n",
    "    return token_ids, attention_mask\n",
    "batch_text = ['Hello', 'World!', 'GPT']\n",
    "max_seq_len = 10\n",
    "token_ids, attention_mask = tokenize_and_pad_debug(batch_text, token_to_id, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5728f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20bcf2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt\n",
      "Raw text sample:  sir walter elliot, of kellynch hall, in somersetshire, was a man who,\n",
      "for his own amusement, never \n",
      "Total words: 779561\n",
      "Most common words: ['the', 'to', 'and', 'of', 'a', 'i', 'her', 'in', 'was', 'she', 'not', 'that', 'it', 'be', 'as', 'had', 'he', 'you', 'for', 'his']\n",
      "Vocabulary (limited to 10000): ['the', 'to', 'and', 'of', 'a', 'i', 'her', 'in', 'was', 'she', 'not', 'that', 'it', 'be', 'as', 'had', 'he', 'you', 'for', 'his'] ...\n",
      "Token to ID mapping (sample): {'the': 0, 'to': 1, 'and': 2, 'of': 3, 'a': 4, 'i': 5, 'her': 6, 'in': 7, 'was': 8, 'she': 9, 'not': 10, 'that': 11, 'it': 12, 'be': 13, 'as': 14, 'had': 15, 'he': 16, 'you': 17, 'for': 18, 'his': 19}\n",
      "ID to Token mapping (sample): {0: 'the', 1: 'to', 2: 'and', 3: 'of', 4: 'a', 5: 'i', 6: 'her', 7: 'in', 8: 'was', 9: 'she', 10: 'not', 11: 'that', 12: 'it', 13: 'be', 14: 'as', 15: 'had', 16: 'he', 17: 'you', 18: 'for', 19: 'his'}\n",
      "Vocabulary size (limited): 10000\n",
      "Batch to tokenize: ['Hello world', 'GPT is great', 'Word level']\n",
      "Text: Hello world -> Words: ['Hello', 'world']\n",
      "Word IDs: [0, 416]\n",
      "Padded IDs: [0, 416, 0, 0, 0, 0]\n",
      "Text: GPT is great -> Words: ['GPT', 'is', 'great']\n",
      "Word IDs: [0, 23, 92]\n",
      "Padded IDs: [0, 23, 92, 0, 0, 0]\n",
      "Text: Word level -> Words: ['Word', 'level']\n",
      "Word IDs: [0, 8602]\n",
      "Padded IDs: [0, 8602, 0, 0, 0, 0]\n",
      "Token IDs array: [[   0  416    0    0    0    0]\n",
      " [   0   23   92    0    0    0]\n",
      " [   0 8602    0    0    0    0]]\n",
      "Attention mask array: [[0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Show every step for word-level processing\n",
    "def build_word_vocab_debug(text_path, max_words=10000):\n",
    "    print(f'Reading file: {text_path}')\n",
    "    with open(text_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print('Raw text sample:', text[:100])\n",
    "    words = text.split()\n",
    "    print(f'Total words: {len(words)}')\n",
    "    word_counts = Counter(words)\n",
    "    most_common = word_counts.most_common(max_words)\n",
    "    print(f'Most common words: {[w for w, _ in most_common[:20]]}')\n",
    "    vocab = [w for w, _ in most_common]\n",
    "    print(f'Vocabulary (limited to {max_words}):', vocab[:20], '...')\n",
    "    token_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_token = {i: word for word, i in token_to_id.items()}\n",
    "    print('Token to ID mapping (sample):', dict(list(token_to_id.items())[:20]))\n",
    "    print('ID to Token mapping (sample):', dict(list(id_to_token.items())[:20]))\n",
    "    print(f'Vocabulary size (limited): {len(vocab)}')\n",
    "    return token_to_id, id_to_token, vocab\n",
    "word_token_to_id, word_id_to_token, word_vocab = build_word_vocab_debug(text_path, max_words=10000)\n",
    "\n",
    "def tokenize_and_pad_words_debug(text_batch, token_to_id, max_seq_len, pad_value=0):\n",
    "    print('Batch to tokenize:', text_batch)\n",
    "    batch_token_ids = []\n",
    "    for text in text_batch:\n",
    "        words = text.split()\n",
    "        print(f'Text: {text} -> Words: {words}')\n",
    "        ids = [token_to_id.get(w, pad_value) for w in words]\n",
    "        print(f'Word IDs: {ids}')\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "            print(f'Truncated IDs: {ids}')\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "            print(f'Padded IDs: {ids}')\n",
    "        batch_token_ids.append(ids)\n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    print('Token IDs array:', token_ids)\n",
    "    print('Attention mask array:', attention_mask)\n",
    "    return token_ids, attention_mask\n",
    "batch_text_words = ['Hello world', 'GPT is great', 'Word level']\n",
    "max_seq_len_words = 6\n",
    "word_token_ids, word_attention_mask = tokenize_and_pad_words_debug(batch_text_words, word_token_to_id, max_seq_len_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
