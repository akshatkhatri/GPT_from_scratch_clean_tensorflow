{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d1c0f8",
   "metadata": {},
   "source": [
    "# Training Experiments - Quick and Dirty\n",
    "\n",
    "Trying different training approaches, lots of trial and error here!\n",
    "This notebook is where I'm figuring out training loops, hyperparameters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab553ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More messy imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "# Disable warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = '/home/akshat/GPT_from_scratch'\n",
    "NOTEBOOK_DIR = f'{PROJECT_ROOT}/notebooks'\n",
    "print(f\"Working from: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a140e",
   "metadata": {},
   "source": [
    "## Load Previous Experiment Data\n",
    "\n",
    "Loading tokenizers and data from previous experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8dfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from previous experiment\n",
    "tokenizer_path = f\"{NOTEBOOK_DIR}/tokenizer_experiments/char_tokenizer_v1.json\"\n",
    "try:\n",
    "    with open(tokenizer_path, 'r') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    char_to_id = tokenizer_data['char_to_id']\n",
    "    id_to_char = tokenizer_data['id_to_char']\n",
    "    vocab = tokenizer_data['vocab']\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    print(f\"✓ Loaded tokenizer: {vocab_size} tokens\")\n",
    "    print(f\"Sample vocab: {vocab[:10]}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Tokenizer not found, creating a quick one...\")\n",
    "    # Quick fallback tokenizer\n",
    "    text_path = f'{PROJECT_ROOT}/text_data/alice_story.txt'\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab = ['<PAD>', '<UNK>'] + chars\n",
    "    char_to_id = {ch: i for i, ch in enumerate(vocab)}\n",
    "    id_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"✓ Created fallback tokenizer: {vocab_size} tokens\")\n",
    "\n",
    "# Load some text data\n",
    "alice_path = f'{PROJECT_ROOT}/text_data/alice_story.txt'\n",
    "with open(alice_path, 'r') as f:\n",
    "    training_text = f.read()\n",
    "\n",
    "print(f\"\\nTraining text: {len(training_text):,} characters\")\n",
    "print(f\"Preview: {training_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a6b61",
   "metadata": {},
   "source": [
    "## Quick Model Definitions\n",
    "\n",
    "Copy-pasting and modifying model code for quick experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of attention layer with some modifications\n",
    "class QuickAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, x, mask=None, training=None):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        q = self.wq(x)\n",
    "        k = self.wk(x)\n",
    "        v = self.wv(x)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(self.depth, tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # Causal mask for GPT\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = tf.where(mask == 0, -1e9, 0.0)\n",
    "        scaled_attention_logits += mask\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights, training=training)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "\n",
    "# Quick transformer block\n",
    "class QuickTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.att = QuickAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        attn_output = self.att(x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "print(\"✓ Defined quick model components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick GPT model for experiments\n",
    "class ExperimentalGPT(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dff, maximum_position_encoding, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = tf.keras.layers.Embedding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [QuickTransformerBlock(d_model, num_heads, dff, dropout) \n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Embeddings + positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        pos = tf.range(seq_len)\n",
    "        x += self.pos_encoding(pos)\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, training=training)\n",
    "        \n",
    "        return self.final_layer(x)\n",
    "\n",
    "print(\"✓ Defined experimental GPT model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284f47f",
   "metadata": {},
   "source": [
    "## Training Data Preparation\n",
    "\n",
    "Quick data prep for experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data preparation function\n",
    "def prepare_data_quick(text, tokenizer, seq_length=64, batch_size=32):\n",
    "    \"\"\"Quick data preparation for experiments\"\"\"\n",
    "    print(f\"Preparing data: seq_len={seq_length}, batch_size={batch_size}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = [tokenizer.get(ch, tokenizer.get('<UNK>', 0)) for ch in text]\n",
    "    print(f\"Tokenized to {len(tokens)} tokens\")\n",
    "    \n",
    "    # Create sequences\n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    # Use sliding window\n",
    "    step_size = seq_length // 4  # Overlap windows\n",
    "    for i in range(0, len(tokens) - seq_length, step_size):\n",
    "        input_seq = tokens[i:i+seq_length]\n",
    "        target_seq = tokens[i+1:i+seq_length+1]\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "    \n",
    "    print(f\"Created {len(inputs)} sequences\")\n",
    "    \n",
    "    # Convert to numpy\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "    \n",
    "    return dataset, len(inputs)\n",
    "\n",
    "# Prepare training data\n",
    "SEQ_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Use only part of text for quick experiments\n",
    "sample_text = training_text[:10000]  # First 10k chars\n",
    "train_dataset, num_examples = prepare_data_quick(sample_text, char_to_id, SEQ_LENGTH, BATCH_SIZE)\n",
    "steps_per_epoch = num_examples // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nDataset ready: {num_examples} examples, {steps_per_epoch} steps/epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e96aad",
   "metadata": {},
   "source": [
    "## Model Training Experiments\n",
    "\n",
    "Trying different model sizes and hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Small model\n",
    "print(\"=== Experiment 1: Small Model ===\")\n",
    "\n",
    "model_small = ExperimentalGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    dff=128,\n",
    "    maximum_position_encoding=SEQ_LENGTH,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Compile\n",
    "optimizer_small = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Quick test\n",
    "test_input = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.int32)  # Dummy input\n",
    "test_output = model_small(test_input)\n",
    "print(f\"Small model output shape: {test_output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum([tf.size(var).numpy() for var in model_small.trainable_variables])\n",
    "print(f\"Small model parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4abdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop with debugging\n",
    "def train_model_experimental(model, dataset, optimizer, epochs=3, model_name=\"model\"):\n",
    "    \"\"\"Quick training function with lots of debugging\"\"\"\n",
    "    print(f\"\\nTraining {model_name} for {epochs} epochs...\")\n",
    "    \n",
    "    # Track metrics\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(targets, predictions)\n",
    "        \n",
    "        return loss, predictions\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        # Progress tracking\n",
    "        step_losses = []\n",
    "        \n",
    "        for step, (inputs, targets) in enumerate(dataset):\n",
    "            loss, predictions = train_step(inputs, targets)\n",
    "            step_losses.append(loss.numpy())\n",
    "            \n",
    "            # Print progress every 20 steps\n",
    "            if step % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1}, Step {step:3d}: loss={loss.numpy():.4f}, acc={train_accuracy.result():.4f}\")\n",
    "                \n",
    "                # Show sample prediction\n",
    "                if step == 0:  # Only on first step\n",
    "                    sample_pred = tf.nn.softmax(predictions[0, 0, :])  # First token of first example\n",
    "                    top_3_preds = tf.nn.top_k(sample_pred, 3)\n",
    "                    pred_chars = [id_to_char[str(idx.numpy())] for idx in top_3_preds.indices]\n",
    "                    target_char = id_to_char[str(targets[0, 0].numpy())]\n",
    "                    print(f\"    Sample prediction: {pred_chars} (target: '{target_char}')\")\n",
    "        \n",
    "        # Epoch summary\n",
    "        epoch_time = time.time() - start\n",
    "        final_loss = train_loss.result().numpy()\n",
    "        final_acc = train_accuracy.result().numpy()\n",
    "        \n",
    "        history['loss'].append(final_loss)\n",
    "        history['accuracy'].append(final_acc)\n",
    "        \n",
    "        print(f\"\\n  Epoch {epoch+1} Summary:\")\n",
    "        print(f\"    Loss: {final_loss:.4f}\")\n",
    "        print(f\"    Accuracy: {final_acc:.4f}\")\n",
    "        print(f\"    Time: {epoch_time:.2f}s\")\n",
    "        print(f\"    Loss trend: {step_losses[0]:.4f} -> {step_losses[-1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train small model\n",
    "history_small = train_model_experimental(model_small, train_dataset, optimizer_small, epochs=3, model_name=\"Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d58ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Medium model\n",
    "print(\"\\n=== Experiment 2: Medium Model ===\")\n",
    "\n",
    "model_medium = ExperimentalGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    dff=256,\n",
    "    maximum_position_encoding=SEQ_LENGTH,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "optimizer_medium = tf.keras.optimizers.Adam(learning_rate=0.0005)  # Lower LR for bigger model\n",
    "\n",
    "# Test model\n",
    "test_output = model_medium(test_input)\n",
    "print(f\"Medium model output shape: {test_output.shape}\")\n",
    "\n",
    "total_params = sum([tf.size(var).numpy() for var in model_medium.trainable_variables])\n",
    "print(f\"Medium model parameters: {total_params:,}\")\n",
    "\n",
    "# Train medium model (fewer epochs due to size)\n",
    "history_medium = train_model_experimental(model_medium, train_dataset, optimizer_medium, epochs=2, model_name=\"Medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae02723",
   "metadata": {},
   "source": [
    "## Text Generation Testing\n",
    "\n",
    "Quick tests of text generation from trained models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick text generation function\n",
    "def generate_quick(model, prompt, max_length=50, temperature=0.8):\n",
    "    \"\"\"Generate text quickly for testing\"\"\"\n",
    "    print(f\"Generating from prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = [char_to_id.get(ch, char_to_id.get('<UNK>', 0)) for ch in prompt]\n",
    "    \n",
    "    # Generate\n",
    "    for i in range(max_length):\n",
    "        # Prepare input (last seq_length tokens)\n",
    "        current_input = input_ids[-SEQ_LENGTH:]\n",
    "        if len(current_input) < SEQ_LENGTH:\n",
    "            current_input = [0] * (SEQ_LENGTH - len(current_input)) + current_input\n",
    "        \n",
    "        # Get prediction\n",
    "        input_tensor = tf.constant([current_input])\n",
    "        predictions = model(input_tensor, training=False)\n",
    "        \n",
    "        # Sample next token\n",
    "        logits = predictions[0, -1, :] / temperature\n",
    "        predicted_id = tf.random.categorical([logits], 1)[0, 0].numpy()\n",
    "        \n",
    "        input_ids.append(predicted_id)\n",
    "        \n",
    "        # Convert to text for progress\n",
    "        if i % 10 == 0 or i < 5:\n",
    "            current_text = ''.join([id_to_char[str(id)] for id in input_ids])\n",
    "            print(f\"  Step {i:2d}: {current_text[-20:]}\")\n",
    "    \n",
    "    # Final result\n",
    "    generated_text = ''.join([id_to_char[str(id)] for id in input_ids])\n",
    "    return generated_text\n",
    "\n",
    "# Test generation with small model\n",
    "print(\"=== Small Model Generation ===\")\n",
    "prompts = [\"Alice\", \"The cat\", \"Once upon\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n--- Prompt: '{prompt}' ---\")\n",
    "    try:\n",
    "        generated = generate_quick(model_small, prompt, max_length=30, temperature=0.8)\n",
    "        print(f\"Generated: '{generated}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b47087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models side by side\n",
    "print(\"=== Model Comparison ===\")\n",
    "test_prompt = \"Alice was\"\n",
    "\n",
    "print(f\"\\nPrompt: '{test_prompt}'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    print(\"Small model:\")\n",
    "    small_gen = generate_quick(model_small, test_prompt, max_length=40, temperature=0.7)\n",
    "    print(f\"  Result: '{small_gen}'\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    print(\"Medium model:\")\n",
    "    medium_gen = generate_quick(model_medium, test_prompt, max_length=40, temperature=0.7)\n",
    "    print(f\"  Result: '{medium_gen}'\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace7dbc",
   "metadata": {},
   "source": [
    "## Quick Performance Analysis\n",
    "\n",
    "Visualizing training results and model comparison..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(history_small['loss'], 'b-o', label='Small Model', linewidth=2)\n",
    "if len(history_medium['loss']) > 0:\n",
    "    axes[0, 0].plot(history_medium['loss'], 'r-s', label='Medium Model', linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 1].plot(history_small['accuracy'], 'b-o', label='Small Model', linewidth=2)\n",
    "if len(history_medium['accuracy']) > 0:\n",
    "    axes[0, 1].plot(history_medium['accuracy'], 'r-s', label='Medium Model', linewidth=2)\n",
    "axes[0, 1].set_title('Training Accuracy Comparison')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count comparison\n",
    "small_params = sum([tf.size(var).numpy() for var in model_small.trainable_variables])\n",
    "medium_params = sum([tf.size(var).numpy() for var in model_medium.trainable_variables])\n",
    "\n",
    "models = ['Small', 'Medium']\n",
    "param_counts = [small_params, medium_params]\n",
    "\n",
    "bars = axes[1, 0].bar(models, param_counts, color=['skyblue', 'lightcoral'], alpha=0.7)\n",
    "axes[1, 0].set_title('Model Size Comparison')\n",
    "axes[1, 0].set_ylabel('Parameters')\n",
    "\n",
    "# Add parameter count labels on bars\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Final performance summary\n",
    "final_metrics = {\n",
    "    'Small Model': {\n",
    "        'Final Loss': history_small['loss'][-1] if history_small['loss'] else 0,\n",
    "        'Final Accuracy': history_small['accuracy'][-1] if history_small['accuracy'] else 0,\n",
    "        'Parameters': small_params\n",
    "    },\n",
    "    'Medium Model': {\n",
    "        'Final Loss': history_medium['loss'][-1] if history_medium['loss'] else 0,\n",
    "        'Final Accuracy': history_medium['accuracy'][-1] if history_medium['accuracy'] else 0,\n",
    "        'Parameters': medium_params\n",
    "    }\n",
    "}\n",
    "\n",
    "# Table-like visualization\n",
    "table_data = []\n",
    "for model_name, metrics in final_metrics.items():\n",
    "    table_data.append([\n",
    "        model_name,\n",
    "        f\"{metrics['Final Loss']:.4f}\",\n",
    "        f\"{metrics['Final Accuracy']:.4f}\",\n",
    "        f\"{metrics['Parameters']:,}\"\n",
    "    ])\n",
    "\n",
    "axes[1, 1].axis('tight')\n",
    "axes[1, 1].axis('off')\n",
    "table = axes[1, 1].table(cellText=table_data,\n",
    "                        colLabels=['Model', 'Final Loss', 'Final Acc', 'Parameters'],\n",
    "                        cellLoc='center',\n",
    "                        loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1, 1].set_title('Final Metrics Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Training Experiments Summary:\")\n",
    "print(f\"- Small Model: {small_params:,} params, final loss: {history_small['loss'][-1]:.4f}\")\n",
    "print(f\"- Medium Model: {medium_params:,} params, final loss: {history_medium['loss'][-1]:.4f}\")\n",
    "print(f\"- Training data: {num_examples} examples, {steps_per_epoch} steps/epoch\")\n",
    "print(f\"- Sequence length: {SEQ_LENGTH}, Vocabulary: {vocab_size} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results for later use\n",
    "experiment_dir = f\"{NOTEBOOK_DIR}/training_experiments\"\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "model_small.save_weights(f\"{experiment_dir}/small_model_weights\")\n",
    "model_medium.save_weights(f\"{experiment_dir}/medium_model_weights\")\n",
    "\n",
    "# Save training history\n",
    "with open(f\"{experiment_dir}/training_history.pkl\", 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'small_history': history_small,\n",
    "        'medium_history': history_medium,\n",
    "        'model_configs': {\n",
    "            'small': {'d_model': 64, 'num_heads': 4, 'num_layers': 2, 'dff': 128},\n",
    "            'medium': {'d_model': 128, 'num_heads': 8, 'num_layers': 4, 'dff': 256}\n",
    "        },\n",
    "        'training_config': {\n",
    "            'seq_length': SEQ_LENGTH,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'vocab_size': vocab_size,\n",
    "            'num_examples': num_examples\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "# Quick experiment log\n",
    "experiment_log = f\"\"\"\n",
    "Training Experiments Log - {datetime.now()}\n",
    "==========================================\n",
    "\n",
    "Models Tested:\n",
    "1. Small Model: 64d, 4h, 2l -> {small_params:,} params\n",
    "   - Final Loss: {history_small['loss'][-1]:.4f}\n",
    "   - Final Accuracy: {history_small['accuracy'][-1]:.4f}\n",
    "\n",
    "2. Medium Model: 128d, 8h, 4l -> {medium_params:,} params\n",
    "   - Final Loss: {history_medium['loss'][-1]:.4f}\n",
    "   - Final Accuracy: {history_medium['accuracy'][-1]:.4f}\n",
    "\n",
    "Training Setup:\n",
    "- Data: Alice story (first 10k chars)\n",
    "- Sequence Length: {SEQ_LENGTH}\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Vocabulary: {vocab_size} character tokens\n",
    "- Examples: {num_examples}\n",
    "\n",
    "Observations:\n",
    "- Both models converged quickly on this small dataset\n",
    "- Medium model shows better final metrics but much larger\n",
    "- Text generation works but needs more training data\n",
    "- Character-level tokenization works well for this dataset size\n",
    "\n",
    "Next Steps:\n",
    "- Scale up training data\n",
    "- Try different learning rate schedules\n",
    "- Implement better text generation\n",
    "- Add proper evaluation metrics\n",
    "- Try different tokenization approaches\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{experiment_dir}/experiment_log.txt\", 'w') as f:\n",
    "    f.write(experiment_log)\n",
    "\n",
    "print(f\"✓ Saved experiment results to {experiment_dir}\")\n",
    "print(\"\\n🎉 Training experiments complete!\")\n",
    "print(\"Ready to implement clean training pipeline in organized modules...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
