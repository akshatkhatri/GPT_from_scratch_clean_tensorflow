{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2184491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 07:24:54.687256: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-30 07:24:54.850616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-30 07:24:56.434467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e83566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_vocabulary_tf(\n",
    "    file_path_list: List[str],\n",
    "    existing_vocab: Dict[str,int]|None = None\n",
    ") -> Dict[str,int]:\n",
    "    \"\"\"\n",
    "    Build a word-level vocabulary from text files.\n",
    "    Returns token_to_id mapping each word (or <UNK>, <PAD>) to an integer ID.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]  # type: ignore\n",
    "    if existing_vocab is None:\n",
    "        existing_vocab = {'<PAD>':0,'<UNK>':1}\n",
    "    vocab = set(existing_vocab.keys())\n",
    "\n",
    "    for fn in file_path_list:\n",
    "        if os.path.isdir(fn):\n",
    "            raise IsADirectoryError(fn)\n",
    "        if not os.path.isfile(fn):\n",
    "            raise FileNotFoundError(fn)\n",
    "        text = open(fn, encoding='utf-8').read().lower()\n",
    "        # simple word tokenizer; keeps only alphanumeric words\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "        vocab.update(words)\n",
    "\n",
    "    sorted_words = sorted(w for w in vocab if w not in ('<PAD>','<UNK>'))\n",
    "    token_to_id = {'<PAD>':0,'<UNK>':1}\n",
    "    for idx, w in enumerate(sorted_words, start=2):\n",
    "        token_to_id[w] = idx\n",
    "    return token_to_id\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id(\n",
    "    token_to_id_dict: Dict[str,int],\n",
    "    text_batch: List[str],\n",
    "    max_seq_len: int,\n",
    "    pad_value: int = 0\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    '''\n",
    "    Tokenize batch of strings into word-IDs, pad/truncate, and build attention masks.\n",
    "    '''\n",
    "    batch_ids = []\n",
    "    for text in text_batch:\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "        ids = [token_to_id_dict.get(w, token_to_id_dict['<UNK>']) for w in words]\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    token_ids = np.array(batch_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "\n",
    "    return tf.constant(token_ids), tf.constant(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0806ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def train_sentencepiece_tokenizer(file_path_list: List[str], \n",
    "                                vocab_size: int = 2000,\n",
    "                                model_prefix: str = 'spm_gpt') -> spm.SentencePieceProcessor:\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizer from text files (replaces tokenize_and_build_vocabulary_tf).\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths containing the text corpus.\n",
    "        vocab_size: Size of the subword vocabulary (default: 2000).\n",
    "        model_prefix: Prefix for output model files.\n",
    "    \n",
    "    Returns:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]\n",
    "    \n",
    "    # Validate files (same as your original)\n",
    "    for file_name in file_path_list:\n",
    "        if os.path.isdir(file_name):\n",
    "            raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "    \n",
    "    # Combine all files into one input (or use comma-separated list)\n",
    "    input_files = ','.join(file_path_list)\n",
    "    \n",
    "    # Train SentencePiece model\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_files,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe',\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "    )\n",
    "    \n",
    "    # Load and return processor\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'{model_prefix}.model')\n",
    "    return sp\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id_sp(sp: spm.SentencePieceProcessor, \n",
    "                                 text_batch: List[str], \n",
    "                                 max_seq_len: int, \n",
    "                                 pad_value: int = 0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Tokenize batch of text using SentencePiece (replaces tokenize_and_build_token_id).\n",
    "    \n",
    "    Args:\n",
    "        sp: Trained SentencePieceProcessor object.\n",
    "        text_batch: List of text strings to tokenize.\n",
    "        max_seq_len: Maximum sequence length after padding/truncation.\n",
    "        pad_value: Integer ID used for padding tokens (should match sp.pad_id()).\n",
    "    \n",
    "    Returns:\n",
    "        token_ids: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "        attention_mask: tf.Tensor of shape (batch_size, max_seq_len), dtype tf.int32.\n",
    "    \"\"\"\n",
    "    batch_token_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        # Encode text to subword IDs\n",
    "        ids = sp.encode_as_ids(text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[-max_seq_len:]  # Keep the end (recent context)\n",
    "        else:\n",
    "            # Pad to max_seq_len\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_token_ids.append(ids)\n",
    "    \n",
    "    token_ids = np.array(batch_token_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "    \n",
    "    return tf.constant(token_ids), tf.constant(attention_mask) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ec0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        max_seq_len: int = 512,\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, max_seq_len)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ae6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self,attention_heads = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape): # Two tuples -> first tuple is (Batch Shape , Max_seq_length_in_batch,d_model) , Second tuple is (batch , max_seq_len)\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        self.Query_projection = self.add_weight(\n",
    "            name = 'Query_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name = 'Key_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name = 'Value_Vector_for_projection',\n",
    "            initializer = 'random_normal',\n",
    "            shape = (self.d_model,self.d_model),\n",
    "            trainable = True \n",
    "        )\n",
    "\n",
    "        self.output_projection = self.add_weight(\n",
    "        name=\"Output_projection\",\n",
    "        initializer=\"random_normal\",\n",
    "        shape=(self.d_model, self.d_model),\n",
    "        trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads # type: ignore\n",
    "        \n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self,inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0] # type: ignore\n",
    "        seq_len = tf.shape(embeddings)[1] # type: ignore\n",
    "\n",
    "        Q = embeddings @ self.Query_projection # (seq_len , d_model)\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape and transpose for multi-head Attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        scores = tf.matmul(Q,K, transpose_b=True) # (batch , heads , seq_len,seq_len)\n",
    "        scores = scores / tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        # 5a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        causal_mask = tf.reshape(causal_mask, (1, 1, seq_len, seq_len))  # (1,1,L,L)\n",
    "\n",
    "        # 5b. Token mask (B,L) -> (B,1,1,L)\n",
    "        token_mask = tf.cast(token_masks[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "\n",
    "        # 5c. Combine masks\n",
    "        combined_mask = causal_mask * token_mask  # broadcast -> (B, H, L, L)\n",
    "\n",
    "        # 6. Apply mask (replace disallowed with -1e9)\n",
    "        scores = tf.where(combined_mask > 0, scores, tf.constant(-1e9, dtype = scores.dtype))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        context = attention_weights @ V   #(batch, heads, seq_len, seq_len) × (batch, heads, seq_len, d_head) → (batch, heads, seq_len, d_head)\n",
    "        concat_context = tf.reshape(context, (batch_size,seq_len,self.attention_heads * self.d_head))  # type: ignore\n",
    "\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads,})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8edee72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e118009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class CosineDecayWithWarmup(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, \n",
    "                 warmup_steps: int,\n",
    "                 total_steps: int,\n",
    "                 peak_learning_rate: float = 1e-4,\n",
    "                 min_learning_rate: float = 1e-6,\n",
    "                 name: str = \"cosine_decay_with_warmup\"):\n",
    "        super().__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase: linear increase from 0 to peak_learning_rate\n",
    "        warmup_lr = self.peak_learning_rate * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        cosine_decay_lr = self.min_learning_rate + 0.5 * (\n",
    "            self.peak_learning_rate - self.min_learning_rate\n",
    "        ) * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
    "        \n",
    "        return tf.where(step < warmup_steps, warmup_lr, cosine_decay_lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"peak_learning_rate\": self.peak_learning_rate,\n",
    "            \"min_learning_rate\": self.min_learning_rate,\n",
    "            \"name\": self.name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfcab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171a899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14801"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id_dict = tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'])\n",
    "id_to_token_dict = {id_val: token for token, id_val in token_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f95965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Could not load best_model.keras: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/best_model.keras. Please ensure the file is an accessible `.keras` zip file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:745: UserWarning: Model 'gpt_4' had a build config, but the model cannot be built automatically in `build_from_config(config)`. You should implement `def build_from_config(self, config)`, and you might also want to implement the method  that generates the config at saving time, `def get_build_config(self)`. The method `build_from_config()` is meant to create the state of the model (i.e. its variables) upon deserialization.\n",
      "  instance.build_from_config(build_config)\n",
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 3 variables whereas the saved optimizer has 107 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded second best model\n",
      "Vocabulary size: 14801\n",
      "Model type: <class '__main__.GPT'>\n",
      "Model vocab size: 14801\n",
      "Model context length: 64\n",
      "Model d_model: 256\n",
      "Model attention heads: 4\n",
      "Model decoder blocks: 4\n",
      "Vocab size matches model: True\n",
      "Sample words in vocab: ['<PAD>', '<UNK>', 'a', 'abandoned', 'abashed', 'abate', 'abatement', 'abating', 'abbey', 'abbeyland', 'abbeys', 'abbots', 'abbott', 'abbreviation', 'abdication', 'abdy', 'aberdeen', 'abhor', 'abhorred', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abilities', 'ability', 'abjectly', 'abjuring', 'able', 'ablest', 'ablution']\n",
      "Common words present: [True, True, True, True]\n",
      "Testing word-level model:\n",
      "\n",
      "Testing with: 'the'\n",
      "Top 5 predictions:\n",
      "  1. 'superiorities' (ID: 12725) - 0.0001\n",
      "  2. 'prenticed' (ID: 9910) - 0.0001\n",
      "  3. 'manner' (ID: 8021) - 0.0001\n",
      "  4. 'criticisms' (ID: 3034) - 0.0001\n",
      "  5. 'guiltless' (ID: 6018) - 0.0001\n",
      "\n",
      "Testing with: 'elizabeth'\n",
      "Top 5 predictions:\n",
      "  1. 'superiorities' (ID: 12725) - 0.0001\n",
      "  2. 'prenticed' (ID: 9910) - 0.0001\n",
      "  3. 'manner' (ID: 8021) - 0.0001\n",
      "  4. 'criticisms' (ID: 3034) - 0.0001\n",
      "  5. 'scarce' (ID: 11413) - 0.0001\n",
      "\n",
      "Testing with: 'it is a truth'\n",
      "Top 5 predictions:\n",
      "  1. 'study' (ID: 12572) - 0.0001\n",
      "  2. 'dost' (ID: 4028) - 0.0001\n",
      "  3. 'prenticed' (ID: 9910) - 0.0001\n",
      "  4. 'prophesy' (ID: 10139) - 0.0001\n",
      "  5. 'larolles' (ID: 7576) - 0.0001\n",
      "\n",
      "Testing with longer Jane Austen context:\n",
      "Context: 'it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a'\n",
      "Context length: 22 tokens\n",
      "Top 5 predictions after long context:\n",
      "  1. 'guiltless' (ID: 6018) - 0.0001\n",
      "  2. 'prenticed' (ID: 9910) - 0.0001\n",
      "  3. 'buckingham' (ID: 1684) - 0.0001\n",
      "  4. 'dost' (ID: 4028) - 0.0001\n",
      "  5. 'literally' (ID: 7798) - 0.0001\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot find empty port in range: 6021-6021. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 357\u001b[39m\n\u001b[32m    354\u001b[39m     clear_btn.click(\u001b[38;5;28;01mlambda\u001b[39;00m: [], \u001b[38;5;28;01mNone\u001b[39;00m, chatbot)\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mdemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshare\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Generate public share link\u001b[39;49;00m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6021\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/gradio/blocks.py:2794\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[32m   2789\u001b[39m     (\n\u001b[32m   2790\u001b[39m         server_name,\n\u001b[32m   2791\u001b[39m         server_port,\n\u001b[32m   2792\u001b[39m         local_url,\n\u001b[32m   2793\u001b[39m         server,\n\u001b[32m-> \u001b[39m\u001b[32m2794\u001b[39m     ) = \u001b[43mhttp_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2801\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2802\u001b[39m \u001b[38;5;28mself\u001b[39m.server_name = server_name\n\u001b[32m   2803\u001b[39m \u001b[38;5;28mself\u001b[39m.local_url = local_url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/gradio/http_server.py:156\u001b[39m, in \u001b[36mstart_server\u001b[39m\u001b[34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[39m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot find empty port in range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(server_ports)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ssl_keyfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    161\u001b[39m     path_to_local_server = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_host_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Cannot find empty port in range: 6021-6021. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import re\n",
    "\n",
    "# Try to load the best available model - check for later epochs first\n",
    "model_paths = [\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/best_model.keras', \"best_model.keras\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_76_val_loss_1.1698.keras', \"second best model\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_17_val_loss_1.4634.keras', \"epoch 17 model (val_loss 1.4634)\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_15_val_loss_1.5230.keras', \"epoch 15 model (val_loss 1.5230)\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_13_val_loss_1.6033.keras', \"epoch 13 model (val_loss 1.6033)\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_12_val_loss_1.7153.keras', \"epoch 12 model (val_loss 1.7153)\"),\n",
    "    (r'/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_07_val_loss_2.0682.keras', \"epoch 07 model (val_loss 2.0682)\"),\n",
    "]\n",
    "\n",
    "model = None\n",
    "for model_path, description in model_paths:\n",
    "    try:\n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(f\"✅ Loaded {description}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load {description}: {e}\")\n",
    "        continue\n",
    "\n",
    "if model is None:\n",
    "    raise Exception(\"Could not load any model!\")\n",
    "\n",
    "CONTEXT_LEN = model._context_length  # Use the model's actual context length\n",
    "\n",
    "# Debug: Print vocabulary info\n",
    "print(f\"Vocabulary size: {len(token_to_id_dict)}\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Get model info from your custom GPT model\n",
    "try:\n",
    "    print(f\"Model vocab size: {model._vocab_size}\")\n",
    "    print(f\"Model context length: {model._context_length}\")\n",
    "    print(f\"Model d_model: {model._d_model}\")\n",
    "    print(f\"Model attention heads: {model._attention_heads}\")\n",
    "    print(f\"Model decoder blocks: {model._decoder_blocks}\")\n",
    "    print(f\"Vocab size matches model: {model._vocab_size == len(token_to_id_dict)}\")\n",
    "    \n",
    "    if model._vocab_size != len(token_to_id_dict):\n",
    "        print(f\"⚠️  VOCAB SIZE MISMATCH! Model expects {model._vocab_size}, got {len(token_to_id_dict)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting model info: {e}\")\n",
    "\n",
    "print(f\"Sample words in vocab: {list(token_to_id_dict.keys())[:30]}\")\n",
    "print(f\"Common words present: {['the' in token_to_id_dict, 'and' in token_to_id_dict, 'is' in token_to_id_dict, 'a' in token_to_id_dict]}\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Simple word tokenization - split on whitespace and punctuation\"\"\"\n",
    "    # Convert to lowercase and handle punctuation\n",
    "    text = text.lower()\n",
    "    # Split on whitespace and common punctuation, keeping punctuation as separate tokens\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "def encode_text(text, token_to_id_dict):\n",
    "    \"\"\"Encode text to token IDs using word-level tokenizer\"\"\"\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in token_to_id_dict:\n",
    "            token_ids.append(token_to_id_dict[token])\n",
    "        else:\n",
    "            # Handle unknown words - use <UNK> token if available, otherwise skip\n",
    "            unk_id = token_to_id_dict.get('<UNK>', None)\n",
    "            if unk_id is not None:\n",
    "                token_ids.append(unk_id)\n",
    "                print(f\"Warning: '{token}' not in vocabulary, using <UNK>\")\n",
    "            else:\n",
    "                print(f\"Warning: '{token}' not in vocabulary, skipping (no <UNK> token)\")\n",
    "                continue\n",
    "    return token_ids\n",
    "\n",
    "def decode_ids(token_ids, id_to_token_dict):\n",
    "    \"\"\"Decode token IDs back to text using word-level tokenizer\"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        if token_id in id_to_token_dict:\n",
    "            token = id_to_token_dict[token_id]\n",
    "            tokens.append(token)\n",
    "        else:\n",
    "            print(f\"Warning: token ID {token_id} not in vocabulary\")\n",
    "            tokens.append(f\"<UNK_{token_id}>\")\n",
    "    \n",
    "    # Join tokens with spaces, but handle punctuation properly\n",
    "    text = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            text = token\n",
    "        elif token in \".,!?;:\":  # Punctuation - no space before\n",
    "            text += token\n",
    "        elif tokens[i-1] in \".,!?;:\":  # After punctuation - add space\n",
    "            text += \" \" + token\n",
    "        else:  # Regular word - add space\n",
    "            text += \" \" + token\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_special_token_ids():\n",
    "    \"\"\"Get special token IDs for word-level tokenizer\"\"\"\n",
    "    pad_id = token_to_id_dict.get('<PAD>', None)\n",
    "    eos_id = token_to_id_dict.get('<EOS>', None)\n",
    "    # For word-level, common stopping tokens might be period, newline, or special EOS\n",
    "    if eos_id is None:\n",
    "        eos_id = token_to_id_dict.get('.', None)\n",
    "    \n",
    "    print(f\"Special tokens - PAD: {pad_id}, EOS: {eos_id}\")\n",
    "    return pad_id, eos_id\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    # Ensure we don't sample more than available tokens\n",
    "    k = min(k, len(logits))\n",
    "    \n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    \n",
    "    # Add small epsilon to avoid numerical issues\n",
    "    probs = probs + 1e-10\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=50, temperature=0.7, top_k=10, use_argmax=False):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"\\n--- Generation Debug ---\")\n",
    "    print(f\"Input prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize prompt with word-level tokenizer\n",
    "    input_tokens = encode_text(prompt, token_to_id_dict)\n",
    "    print(f\"Input tokens: {input_tokens}\")\n",
    "    print(f\"Input tokens decoded back: '{decode_ids(input_tokens, id_to_token_dict)}'\")\n",
    "    \n",
    "    if not input_tokens:\n",
    "        return \"Error: Could not tokenize input\"\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    pad_id, eos_id = get_special_token_ids()\n",
    "    \n",
    "    print(f\"Starting generation with {len(input_tokens)} input tokens...\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Prepare inputs - pad from left to maintain most recent context\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        \n",
    "        # Place tokens at the end of the context window\n",
    "        current_len = min(len(generated_tokens), CONTEXT_LEN)\n",
    "        start_idx = CONTEXT_LEN - current_len\n",
    "        input_ids[0, start_idx:] = generated_tokens[-current_len:]\n",
    "        attention_mask[0, start_idx:] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        try:\n",
    "            logits = model((input_ids, attention_mask), training=False)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Check raw logit statistics\n",
    "            logit_mean = float(tf.reduce_mean(next_token_logits))\n",
    "            logit_std = float(tf.math.reduce_std(next_token_logits))\n",
    "            logit_max = float(tf.reduce_max(next_token_logits))\n",
    "            logit_min = float(tf.reduce_min(next_token_logits))\n",
    "            \n",
    "            if step == 0:\n",
    "                print(f\"Raw logits stats - Mean: {logit_mean:.4f}, Std: {logit_std:.4f}, Max: {logit_max:.4f}, Min: {logit_min:.4f}\")\n",
    "                if logit_std < 0.1:\n",
    "                    print(\"⚠️  WARNING: Very low logit variance suggests model may not be well trained!\")\n",
    "            \n",
    "            # Apply temperature\n",
    "            if not use_argmax:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "        except Exception as e:\n",
    "            print(f\"Model forward pass error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Sample next token\n",
    "        try:\n",
    "            if use_argmax:\n",
    "                # Use argmax (greedy) sampling for testing\n",
    "                next_token = int(np.argmax(next_token_logits))\n",
    "            else:\n",
    "                # Use top-k sampling\n",
    "                next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Debug: Print first few tokens with more detail\n",
    "        if step < 5:  # Reduced to 5 for cleaner output\n",
    "            sampled_word = id_to_token_dict.get(next_token, f\"<UNK:{next_token}>\")\n",
    "            prob = float(tf.nn.softmax(next_token_logits)[next_token])\n",
    "            max_prob = float(tf.nn.softmax(next_token_logits).numpy().max())\n",
    "            print(f\"Step {step}: Token {next_token} -> '{sampled_word}' (prob: {prob:.6f}, max_prob: {max_prob:.6f})\")\n",
    "            \n",
    "            # Check if probabilities are too uniform (potential training issue)\n",
    "            if step == 0 and max_prob < 0.01:\n",
    "                print(\"⚠️  WARNING: Very low max probability suggests potential training issues!\")\n",
    "        \n",
    "        # Check if token is valid\n",
    "        if next_token >= len(id_to_token_dict):\n",
    "            print(f\"Warning: Invalid token {next_token}, vocab size is {len(id_to_token_dict)}\")\n",
    "            break\n",
    "        \n",
    "        # Stop on special tokens\n",
    "        if pad_id is not None and next_token == pad_id:\n",
    "            print(f\"Stopping at step {step}: hit PAD token\")\n",
    "            break\n",
    "        if eos_id is not None and next_token == eos_id and step > 5:  # Don't stop too early for words\n",
    "            print(f\"Stopping at step {step}: hit EOS token\")\n",
    "            break\n",
    "        \n",
    "        # Stop on repeated patterns (indication of poor generation)\n",
    "        if len(generated_tokens) >= 3 and generated_tokens[-1] == generated_tokens[-2] == generated_tokens[-3]:\n",
    "            print(f\"Stopping at step {step}: detected repetition\")\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        # Maintain sliding window\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = decode_ids(new_tokens, id_to_token_dict)\n",
    "    print(f\"Generated {len(new_tokens)} new tokens: {new_tokens[:10]}...\")  # Show first 10\n",
    "    print(f\"Generated response: '{response}'\")\n",
    "    print(f\"--- End Debug ---\\n\")\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k, use_argmax):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k, use_argmax=use_argmax)\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    return \"\", history\n",
    "\n",
    "# Quick test with the word-level model\n",
    "print(\"Testing word-level model:\")\n",
    "test_cases = [\"the\", \"elizabeth\", \"it is a truth\"]\n",
    "\n",
    "for prompt in test_cases:\n",
    "    print(f\"\\nTesting with: '{prompt}'\")\n",
    "    tokens = encode_text(prompt, token_to_id_dict)\n",
    "    \n",
    "    if not tokens:\n",
    "        print(\"No valid tokens found!\")\n",
    "        continue\n",
    "    \n",
    "    # Create model input\n",
    "    input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    \n",
    "    # Place tokens at end of context window\n",
    "    start_idx = CONTEXT_LEN - len(tokens)\n",
    "    input_ids[0, start_idx:] = tokens\n",
    "    attention_mask[0, start_idx:] = 1\n",
    "    \n",
    "    # Get model predictions\n",
    "    try:\n",
    "        logits = model((input_ids, attention_mask), training=False)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Show top 5 predictions\n",
    "        top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "        print(\"Top 5 predictions:\")\n",
    "        for i in range(5):\n",
    "            token_id = int(top_indices[i])\n",
    "            prob = float(top_probs[i])\n",
    "            word = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "            print(f\"  {i+1}. '{word}' (ID: {token_id}) - {prob:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "\n",
    "# Test with longer context\n",
    "print(f\"\\nTesting with longer Jane Austen context:\")\n",
    "long_prompt = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a\"\n",
    "tokens = encode_text(long_prompt, token_to_id_dict)\n",
    "print(f\"Context: '{long_prompt}'\")\n",
    "print(f\"Context length: {len(tokens)} tokens\")\n",
    "\n",
    "if tokens:\n",
    "    # Use reasonable context length\n",
    "    context_tokens = tokens[-50:] if len(tokens) > 50 else tokens  # Reduced for word-level\n",
    "\n",
    "    input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    \n",
    "    start_idx = CONTEXT_LEN - len(context_tokens)\n",
    "    input_ids[0, start_idx:] = context_tokens\n",
    "    attention_mask[0, start_idx:] = 1\n",
    "\n",
    "    try:\n",
    "        logits = model((input_ids, attention_mask), training=False)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "\n",
    "        top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "        print(\"Top 5 predictions after long context:\")\n",
    "        for i in range(5):\n",
    "            token_id = int(top_indices[i])\n",
    "            prob = float(top_probs[i])\n",
    "            word = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "            print(f\"  {i+1}. '{word}' (ID: {token_id}) - {prob:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during long context prediction: {e}\")\n",
    "\n",
    "with gr.Blocks(title=\"My Word-Level GPT Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Akshat's Word-Level GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with word-level tokenization on Jane Austen data. I understand whole words now!\")\n",
    "    \n",
    "    # Add vocab info\n",
    "    gr.Markdown(f\"**Model Info:** Vocabulary size: {len(token_to_id_dict)} words\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True, type='messages')\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.7, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=5, maximum=100, value=25, step=5, label=\"Max Length (words)\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=10, step=1, label=\"Top-K Sampling\")\n",
    "        use_argmax = gr.Checkbox(label=\"Use Argmax (Greedy) - for testing\", value=False)\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6021,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250acbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 'it is a'\n",
      "Tokens: [30, 41, 1, 30, 40, 1, 22]\n",
      "Top 10 predictions:\n",
      "  1. 'f' (ID: 27) - 0.3225\n",
      "  2. 'c' (ID: 24) - 0.0669\n",
      "  3. ',' (ID: 6) - 0.0521\n",
      "  4. ' ' (ID: 1) - 0.0432\n",
      "  5. '-' (ID: 7) - 0.0400\n",
      "  6. 't' (ID: 41) - 0.0397\n",
      "  7. '9' (ID: 18) - 0.0386\n",
      "  8. 'p' (ID: 37) - 0.0377\n",
      "  9. 'j' (ID: 31) - 0.0376\n",
      "  10. 's' (ID: 40) - 0.0365\n"
     ]
    }
   ],
   "source": [
    "# Test with a very simple prompt\n",
    "test_prompt = \"it is a\"\n",
    "print(f\"Testing with: '{test_prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = encode_text(test_prompt, token_to_id_dict)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Create model input\n",
    "input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "input_ids[0, -len(tokens):] = tokens\n",
    "attention_mask[0, -len(tokens):] = 1\n",
    "\n",
    "# Get model predictions\n",
    "logits = model((input_ids, attention_mask), training=False)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Show top 10 predictions\n",
    "top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=10)\n",
    "print(\"Top 10 predictions:\")\n",
    "for i in range(10):\n",
    "    token_id = int(top_indices[i])\n",
    "    prob = float(top_probs[i])\n",
    "    char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "    print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c030eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent characters:\n",
      "' ': 716875\n",
      "'e': 433350\n",
      "'t': 296790\n",
      "'a': 268929\n",
      "'o': 264312\n",
      "'n': 244375\n",
      "'i': 234267\n",
      "'s': 212583\n",
      "'h': 212370\n",
      "'r': 209492\n"
     ]
    }
   ],
   "source": [
    "# Check character frequency in your data\n",
    "with open('/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt', 'r') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "char_counts = {}\n",
    "for char in text:\n",
    "    char_counts[char] = char_counts.get(char, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most frequent characters:\")\n",
    "for char, count in sorted_chars[:10]:\n",
    "    print(f\"'{char}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06f0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Hello' -> Tokens: [26, 33, 33, 36] -> Decoded: 'ello'\n"
     ]
    }
   ],
   "source": [
    "# Test your tokenizer\n",
    "test_text = \"Hello\"\n",
    "tokens = encode_text(test_text, token_to_id_dict)\n",
    "decoded = decode_ids(tokens, id_to_token_dict)\n",
    "print(f\"Original: '{test_text}' -> Tokens: {tokens} -> Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248e0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
