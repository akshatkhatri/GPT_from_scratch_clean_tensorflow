{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62adc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 05:31:06.917820: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-14 05:31:07.557079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-14 05:31:09.828724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007f70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str] = [r\"/home/akshat/GPT_from_scratch/text_data/pg76702.txt\"],\n",
    "#     existing_vocab: Dict[str, int] = None # type: ignore\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TensorFlow lookup tables.\n",
    "    \n",
    "#     Returns:\n",
    "#         token_to_id_table: tf.lookup.StaticHashTable mapping char -> int\n",
    "#         id_to_token_table: tf.lookup.StaticHashTable mapping int -> char\n",
    "#     \"\"\"\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "\n",
    "#     # Collect characters from all files\n",
    "#     for file_name in file_path_list:\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     # Sort for consistency\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "\n",
    "#     # Assign IDs (keep existing IDs if possible)\n",
    "#     token_to_id = {token: i for i, token in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "#     # Convert dicts to tensors\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()))\n",
    "#     token_values = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_values = tf.constant(list(id_to_token.values()))\n",
    "\n",
    "#     # Create TensorFlow lookup tables\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_values),\n",
    "#         default_value=-1  # unknown token\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_values),\n",
    "#         default_value=\"\"  # unknown ID\n",
    "#     )\n",
    "\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9856e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import List, Dict, Tuple\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str],\n",
    "#     existing_vocab: Dict[str, int] | None = None\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TF lookup tables:\n",
    "#       token_to_id: char -> int\n",
    "#       id_to_token: int -> char\n",
    "#     \"\"\"\n",
    "#     if isinstance(file_path_list, (str, bytes)):\n",
    "#         file_path_list = [file_path_list] # type: ignore\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "#     for file_name in file_path_list:\n",
    "#         if os.path.isdir(file_name):\n",
    "#             raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "#         if not os.path.isfile(file_name):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "#     token_to_id = {tok: i for i, tok in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()), dtype=tf.string)\n",
    "#     token_vals = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_vals = tf.constant(list(id_to_token.values()), dtype=tf.string)\n",
    "\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_vals),\n",
    "#         default_value=-1\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_vals),\n",
    "#         default_value=tf.constant(\"\", dtype=tf.string)\n",
    "#     )\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_token_id(token_to_id_table: tf.lookup.StaticHashTable,\n",
    "#                                 text_batch: tf.Tensor,\n",
    "#                                 pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Tokenize a batch of strings character by character, pad sequences,\n",
    "#     and return attention masks.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_table: TF lookup table mapping char -> int\n",
    "#         text_batch: tf.Tensor of shape [batch_size], dtype=tf.string\n",
    "#         pad_value: int, ID to use for padding\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: tf.Tensor [batch_size, max_seq_len]\n",
    "#         attention_mask: tf.Tensor [batch_size, max_seq_len]\n",
    "#     \"\"\"\n",
    "#     token_ids_list = []\n",
    "\n",
    "#     for text in text_batch.numpy():  # type: ignore\n",
    "#         # Convert bytes to TF string\n",
    "\n",
    "#         # Split into characters\n",
    "#         char_tensor = tf.strings.bytes_split(text)\n",
    "\n",
    "#         # Lookup token IDs\n",
    "#         token_ids = token_to_id_table.lookup(char_tensor)\n",
    "\n",
    "#         token_ids_list.append(token_ids)\n",
    "\n",
    "#     # Pad all sequences to the same length\n",
    "#     token_ids_padded = tf.ragged.stack(token_ids_list).to_tensor(default_value=pad_value) # type: ignore\n",
    "#     # Create attention mask: 1 for real tokens, 0 for padding\n",
    "#     attention_mask = tf.cast(token_ids_padded != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids_padded, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import Tuple\n",
    "\n",
    "\n",
    "# def tokenize_and_build_token_id(\n",
    "#     token_to_id_dict: dict,\n",
    "#     text_batch: list[str],\n",
    "#     max_seq_len: int,\n",
    "#     pad_value: int = 0,\n",
    "#     unk_value: int = None\n",
    "# ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "#     \"\"\"\n",
    "#     TensorFlow-compatible tokenization converting batch of strings to char token IDs,\n",
    "#     padded/truncated to max_seq_len, along with attention mask.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_dict: dict mapping character str -> int ID\n",
    "#         text_batch: list of strings to tokenize\n",
    "#         max_seq_len: max length to pad/truncate sequences\n",
    "#         pad_value: int ID for padding tokens\n",
    "#         unk_value: int ID for unknown tokens; if None, uses pad_value\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: (batch_size, max_seq_len) tf.int32 tensor of token IDs\n",
    "#         attention_mask: (batch_size, max_seq_len) tf.int32 tensor (1 for tokens, 0 for padding)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if unk_value is None:\n",
    "#         unk_value = pad_value\n",
    "\n",
    "#     # Create lookup table from token_to_id_dict\n",
    "#     keys = tf.constant(list(token_to_id_dict.keys()))\n",
    "#     values = tf.constant(list(token_to_id_dict.values()), dtype=tf.int32)\n",
    "#     table = tf.lookup.StaticHashTable(\n",
    "#         tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "#         default_value=unk_value\n",
    "#     )\n",
    "\n",
    "#     # Convert text batch to a RaggedTensor of chars\n",
    "#     rt_chars = tf.strings.unicode_split(text_batch, 'UTF-8')  # shape: [batch_size, (seq_len)]\n",
    "\n",
    "#     # Lookup token IDs for each char\n",
    "#     token_ids = table.lookup(rt_chars)\n",
    "\n",
    "#     # Pad or truncate sequences to max_seq_len\n",
    "#     token_ids = token_ids.to_tensor(default_value=pad_value, shape=[None, max_seq_len])\n",
    "#     token_ids = token_ids[:, :max_seq_len]  # truncate if longer\n",
    "\n",
    "#     # Construct attention mask: 1 where not pad_value, else 0\n",
    "#     attention_mask = tf.cast(token_ids != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e3a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_vocabulary_tf(\n",
    "    file_path_list: List[str],\n",
    "    vocab_size: int = 10000,\n",
    "    existing_vocab: Dict[str,int]|None = None\n",
    ") -> Dict[str,int]:\n",
    "    \"\"\"\n",
    "    Build a word-level vocabulary from text files with size control.\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths to process\n",
    "        vocab_size: Maximum vocabulary size (includes special tokens)\n",
    "        existing_vocab: Optional existing vocabulary to extend\n",
    "        \n",
    "    Returns:\n",
    "        token_to_id mapping each word (or <UNK>, <PAD>) to an integer ID.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]  # type: ignore\n",
    "    \n",
    "    if existing_vocab is None:\n",
    "        existing_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Count word frequencies across all files\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for fn in file_path_list:\n",
    "        if os.path.isdir(fn):\n",
    "            raise IsADirectoryError(fn)\n",
    "        if not os.path.isfile(fn):\n",
    "            raise FileNotFoundError(fn)\n",
    "        \n",
    "        with open(fn, encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "        \n",
    "        # Simple word tokenizer; keeps only alphanumeric words\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Remove special tokens from word counts if they exist\n",
    "    for special_token in ['<PAD>', '<UNK>']:\n",
    "        if special_token in word_counts:\n",
    "            del word_counts[special_token]\n",
    "    \n",
    "    # Get the most common words (excluding space for special tokens)\n",
    "    num_special_tokens = len(existing_vocab)\n",
    "    max_regular_words = vocab_size - num_special_tokens\n",
    "    \n",
    "    most_common_words = [word for word, _ in word_counts.most_common(max_regular_words)]\n",
    "    \n",
    "    # Build the final vocabulary\n",
    "    token_to_id = existing_vocab.copy()\n",
    "    \n",
    "    for idx, word in enumerate(most_common_words, start=num_special_tokens):\n",
    "        token_to_id[word] = idx\n",
    "    \n",
    "    print(f\"Built vocabulary with {len(token_to_id)} tokens \"\n",
    "          f\"({num_special_tokens} special + {len(most_common_words)} regular)\")\n",
    "    \n",
    "    return token_to_id\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id(\n",
    "    token_to_id_dict: Dict[str,int],\n",
    "    text_batch: List[str],\n",
    "    max_seq_len: int,\n",
    "    pad_value: int = 0\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    '''\n",
    "    Tokenize batch of strings into word-IDs, pad/truncate, and build attention masks.\n",
    "    \n",
    "    Args:\n",
    "        token_to_id_dict: Dictionary mapping tokens to IDs\n",
    "        text_batch: List of text strings to tokenize\n",
    "        max_seq_len: Maximum sequence length (pad/truncate to this length)\n",
    "        pad_value: Value to use for padding (should match <PAD> token ID)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (token_ids, attention_mask) as TensorFlow tensors\n",
    "    '''\n",
    "    batch_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "        ids = [token_to_id_dict.get(w, token_to_id_dict['<UNK>']) for w in words]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        # Pad if too short\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    token_ids = np.array(batch_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "\n",
    "    return tf.constant(token_ids), tf.constant(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4783fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocabulary with 5000 tokens (2 special + 4998 regular)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'the': 2,\n",
       " 'to': 3,\n",
       " 'and': 4,\n",
       " 'of': 5,\n",
       " 'a': 6,\n",
       " 'her': 7,\n",
       " 'i': 8,\n",
       " 'in': 9,\n",
       " 'was': 10,\n",
       " 'it': 11,\n",
       " 'she': 12,\n",
       " 'not': 13,\n",
       " 'that': 14,\n",
       " 'be': 15,\n",
       " 'you': 16,\n",
       " 'he': 17,\n",
       " 'had': 18,\n",
       " 'as': 19,\n",
       " 'for': 20,\n",
       " 'with': 21,\n",
       " 'his': 22,\n",
       " 'but': 23,\n",
       " 'have': 24,\n",
       " 'is': 25,\n",
       " 'at': 26,\n",
       " 's': 27,\n",
       " 'so': 28,\n",
       " 'my': 29,\n",
       " 'all': 30,\n",
       " 'on': 31,\n",
       " 'very': 32,\n",
       " 'by': 33,\n",
       " 'him': 34,\n",
       " 'could': 35,\n",
       " 'which': 36,\n",
       " 'been': 37,\n",
       " 'would': 38,\n",
       " 'no': 39,\n",
       " 'were': 40,\n",
       " 'they': 41,\n",
       " 'mr': 42,\n",
       " 'from': 43,\n",
       " 'this': 44,\n",
       " 'me': 45,\n",
       " 'what': 46,\n",
       " 'mrs': 47,\n",
       " 'will': 48,\n",
       " 'do': 49,\n",
       " 'or': 50,\n",
       " 'an': 51,\n",
       " 'them': 52,\n",
       " 'more': 53,\n",
       " 'there': 54,\n",
       " 'such': 55,\n",
       " 'their': 56,\n",
       " 'must': 57,\n",
       " 'your': 58,\n",
       " 'any': 59,\n",
       " 'said': 60,\n",
       " 'if': 61,\n",
       " 'one': 62,\n",
       " 'much': 63,\n",
       " 'when': 64,\n",
       " 'than': 65,\n",
       " 'are': 66,\n",
       " 'am': 67,\n",
       " 'miss': 68,\n",
       " 'we': 69,\n",
       " 'only': 70,\n",
       " 'should': 71,\n",
       " 'who': 72,\n",
       " 'well': 73,\n",
       " 'did': 74,\n",
       " 'every': 75,\n",
       " 'being': 76,\n",
       " 'now': 77,\n",
       " 'how': 78,\n",
       " 'think': 79,\n",
       " 'own': 80,\n",
       " 'never': 81,\n",
       " 'know': 82,\n",
       " 'good': 83,\n",
       " 'might': 84,\n",
       " 'time': 85,\n",
       " 'herself': 86,\n",
       " 'some': 87,\n",
       " 'little': 88,\n",
       " 'can': 89,\n",
       " 'before': 90,\n",
       " 'has': 91,\n",
       " 'nothing': 92,\n",
       " 'other': 93,\n",
       " 'too': 94,\n",
       " 'most': 95,\n",
       " 'though': 96,\n",
       " 'soon': 97,\n",
       " 'may': 98,\n",
       " 'say': 99,\n",
       " 'without': 100,\n",
       " 'first': 101,\n",
       " 'great': 102,\n",
       " 'lady': 103,\n",
       " 'see': 104,\n",
       " 'after': 105,\n",
       " 'again': 106,\n",
       " 'out': 107,\n",
       " 'about': 108,\n",
       " 'fanny': 109,\n",
       " 'dear': 110,\n",
       " 'two': 111,\n",
       " 'shall': 112,\n",
       " 'ever': 113,\n",
       " 'man': 114,\n",
       " 'sir': 115,\n",
       " 'into': 116,\n",
       " 'sister': 117,\n",
       " 'quite': 118,\n",
       " 'made': 119,\n",
       " 'then': 120,\n",
       " 'always': 121,\n",
       " 'day': 122,\n",
       " 'emma': 123,\n",
       " 'make': 124,\n",
       " 'thought': 125,\n",
       " 'up': 126,\n",
       " 'room': 127,\n",
       " 'young': 128,\n",
       " 'thing': 129,\n",
       " 'mother': 130,\n",
       " 'father': 131,\n",
       " 'however': 132,\n",
       " 'sure': 133,\n",
       " 'like': 134,\n",
       " 'house': 135,\n",
       " 'elizabeth': 136,\n",
       " 'long': 137,\n",
       " 'himself': 138,\n",
       " 'give': 139,\n",
       " 'its': 140,\n",
       " 'away': 141,\n",
       " 'go': 142,\n",
       " 'indeed': 143,\n",
       " 'having': 144,\n",
       " 'elinor': 145,\n",
       " 'last': 146,\n",
       " 'many': 147,\n",
       " 'even': 148,\n",
       " 'better': 149,\n",
       " 'us': 150,\n",
       " 'way': 151,\n",
       " 'cannot': 152,\n",
       " 'upon': 153,\n",
       " 'friend': 154,\n",
       " 'hope': 155,\n",
       " 'over': 156,\n",
       " 'enough': 157,\n",
       " 'here': 158,\n",
       " 'come': 159,\n",
       " 'felt': 160,\n",
       " 'catherine': 161,\n",
       " 'moment': 162,\n",
       " 'jane': 163,\n",
       " 'family': 164,\n",
       " 'oh': 165,\n",
       " 'our': 166,\n",
       " 'crawford': 167,\n",
       " 'just': 168,\n",
       " 'mind': 169,\n",
       " 'done': 170,\n",
       " 'wish': 171,\n",
       " 'marianne': 172,\n",
       " 'while': 173,\n",
       " 'still': 174,\n",
       " 'seemed': 175,\n",
       " 'feelings': 176,\n",
       " 'yet': 177,\n",
       " 'home': 178,\n",
       " 'love': 179,\n",
       " 'same': 180,\n",
       " 'brother': 181,\n",
       " 'happy': 182,\n",
       " 'something': 183,\n",
       " 'saw': 184,\n",
       " 'came': 185,\n",
       " 'myself': 186,\n",
       " 'letter': 187,\n",
       " 'place': 188,\n",
       " 'few': 189,\n",
       " 'perhaps': 190,\n",
       " 'look': 191,\n",
       " 'half': 192,\n",
       " 'really': 193,\n",
       " 'till': 194,\n",
       " 'anne': 195,\n",
       " 'heart': 196,\n",
       " 'where': 197,\n",
       " 'harriet': 198,\n",
       " 'yes': 199,\n",
       " 'found': 200,\n",
       " 'take': 201,\n",
       " 'woman': 202,\n",
       " 'almost': 203,\n",
       " 'another': 204,\n",
       " 'going': 205,\n",
       " 'pleasure': 206,\n",
       " 'heard': 207,\n",
       " 'rather': 208,\n",
       " 'morning': 209,\n",
       " 'believe': 210,\n",
       " 'down': 211,\n",
       " 'present': 212,\n",
       " 'does': 213,\n",
       " 'poor': 214,\n",
       " 'those': 215,\n",
       " 'world': 216,\n",
       " 'general': 217,\n",
       " 'off': 218,\n",
       " 'left': 219,\n",
       " 'subject': 220,\n",
       " 'certainly': 221,\n",
       " 'less': 222,\n",
       " 'replied': 223,\n",
       " 'weston': 224,\n",
       " 'together': 225,\n",
       " 'whom': 226,\n",
       " 'part': 227,\n",
       " 'feel': 228,\n",
       " 'both': 229,\n",
       " 'least': 230,\n",
       " 'manner': 231,\n",
       " 'tell': 232,\n",
       " 'darcy': 233,\n",
       " 'cried': 234,\n",
       " 'evening': 235,\n",
       " 'speak': 236,\n",
       " 'hear': 237,\n",
       " 'between': 238,\n",
       " 'once': 239,\n",
       " 'nor': 240,\n",
       " 'looked': 241,\n",
       " 'edmund': 242,\n",
       " 'these': 243,\n",
       " 'three': 244,\n",
       " 'anything': 245,\n",
       " 'kind': 246,\n",
       " 'seen': 247,\n",
       " 'often': 248,\n",
       " 'therefore': 249,\n",
       " 'happiness': 250,\n",
       " 'life': 251,\n",
       " 'people': 252,\n",
       " 'knew': 253,\n",
       " 'whole': 254,\n",
       " 'since': 255,\n",
       " 'knightley': 256,\n",
       " 'elton': 257,\n",
       " 'far': 258,\n",
       " 'party': 259,\n",
       " 'told': 260,\n",
       " 'gone': 261,\n",
       " 'suppose': 262,\n",
       " 'find': 263,\n",
       " 'opinion': 264,\n",
       " 'each': 265,\n",
       " 'ought': 266,\n",
       " 'given': 267,\n",
       " 'immediately': 268,\n",
       " 'right': 269,\n",
       " 'hour': 270,\n",
       " 'back': 271,\n",
       " 'side': 272,\n",
       " 'spirits': 273,\n",
       " 'let': 274,\n",
       " 'want': 275,\n",
       " 'best': 276,\n",
       " 'acquaintance': 277,\n",
       " 'possible': 278,\n",
       " 'others': 279,\n",
       " 'daughter': 280,\n",
       " 'next': 281,\n",
       " 'captain': 282,\n",
       " 'thomas': 283,\n",
       " 'known': 284,\n",
       " 'ill': 285,\n",
       " 'short': 286,\n",
       " 'leave': 287,\n",
       " 'get': 288,\n",
       " 'edward': 289,\n",
       " 'hardly': 290,\n",
       " 'began': 291,\n",
       " 'friends': 292,\n",
       " 'towards': 293,\n",
       " 'able': 294,\n",
       " 'eyes': 295,\n",
       " 'idea': 296,\n",
       " 'bennet': 297,\n",
       " 'years': 298,\n",
       " 'visit': 299,\n",
       " 'passed': 300,\n",
       " 'either': 301,\n",
       " 'against': 302,\n",
       " 'deal': 303,\n",
       " 'affection': 304,\n",
       " 'under': 305,\n",
       " 'everything': 306,\n",
       " 'attention': 307,\n",
       " 'old': 308,\n",
       " 'colonel': 309,\n",
       " 'went': 310,\n",
       " 'woodhouse': 311,\n",
       " 'aunt': 312,\n",
       " 'henry': 313,\n",
       " 'obliged': 314,\n",
       " 'seeing': 315,\n",
       " 'word': 316,\n",
       " 'return': 317,\n",
       " 'longer': 318,\n",
       " 'bingley': 319,\n",
       " 'gave': 320,\n",
       " 'coming': 321,\n",
       " 'john': 322,\n",
       " 'else': 323,\n",
       " 'looking': 324,\n",
       " 'sort': 325,\n",
       " 'character': 326,\n",
       " 'person': 327,\n",
       " 'through': 328,\n",
       " 'chapter': 329,\n",
       " 'doubt': 330,\n",
       " 'comfort': 331,\n",
       " 'wife': 332,\n",
       " 'answer': 333,\n",
       " 'because': 334,\n",
       " 'whether': 335,\n",
       " 'brought': 336,\n",
       " 'rest': 337,\n",
       " 'perfectly': 338,\n",
       " 'end': 339,\n",
       " 'why': 340,\n",
       " 'course': 341,\n",
       " 'elliot': 342,\n",
       " 'account': 343,\n",
       " 'town': 344,\n",
       " 'took': 345,\n",
       " 'put': 346,\n",
       " 'beyond': 347,\n",
       " 'received': 348,\n",
       " 'minutes': 349,\n",
       " 'whose': 350,\n",
       " 'glad': 351,\n",
       " 'days': 352,\n",
       " 'within': 353,\n",
       " 'bertram': 354,\n",
       " 'set': 355,\n",
       " 'means': 356,\n",
       " 'situation': 357,\n",
       " 'body': 358,\n",
       " 'door': 359,\n",
       " 'walk': 360,\n",
       " 'london': 361,\n",
       " 'reason': 362,\n",
       " 'point': 363,\n",
       " 'wished': 364,\n",
       " 'conversation': 365,\n",
       " 'mary': 366,\n",
       " 'girl': 367,\n",
       " 'hand': 368,\n",
       " 'business': 369,\n",
       " 'dashwood': 370,\n",
       " 'yourself': 371,\n",
       " 'pretty': 372,\n",
       " 'feeling': 373,\n",
       " 'continued': 374,\n",
       " 'name': 375,\n",
       " 'uncle': 376,\n",
       " 'agreeable': 377,\n",
       " 'added': 378,\n",
       " 'talked': 379,\n",
       " 'carriage': 380,\n",
       " 'things': 381,\n",
       " 'talk': 382,\n",
       " 'marriage': 383,\n",
       " 'taken': 384,\n",
       " 'married': 385,\n",
       " 'returned': 386,\n",
       " 'determined': 387,\n",
       " 'object': 388,\n",
       " 'children': 389,\n",
       " 'wanted': 390,\n",
       " 'manners': 391,\n",
       " 'already': 392,\n",
       " 'mean': 393,\n",
       " 'jennings': 394,\n",
       " 'power': 395,\n",
       " 'fairfax': 396,\n",
       " 'interest': 397,\n",
       " 'spoke': 398,\n",
       " 'understand': 399,\n",
       " 'ladies': 400,\n",
       " 'walked': 401,\n",
       " 'called': 402,\n",
       " 'sense': 403,\n",
       " 'entirely': 404,\n",
       " 'william': 405,\n",
       " 'ready': 406,\n",
       " 'themselves': 407,\n",
       " 'sorry': 408,\n",
       " 'state': 409,\n",
       " 'bath': 410,\n",
       " 'regard': 411,\n",
       " 'impossible': 412,\n",
       " 'head': 413,\n",
       " 'read': 414,\n",
       " 'fine': 415,\n",
       " 'neither': 416,\n",
       " 'dare': 417,\n",
       " 'different': 418,\n",
       " 'husband': 419,\n",
       " 'voice': 420,\n",
       " 'near': 421,\n",
       " 'care': 422,\n",
       " 'son': 423,\n",
       " 'isabella': 424,\n",
       " 'country': 425,\n",
       " 'churchill': 426,\n",
       " 'sisters': 427,\n",
       " 'nature': 428,\n",
       " 'fortune': 429,\n",
       " 'doing': 430,\n",
       " 'words': 431,\n",
       " 'tilney': 432,\n",
       " 'willoughby': 433,\n",
       " 'settled': 434,\n",
       " 'saying': 435,\n",
       " 'help': 436,\n",
       " 'expected': 437,\n",
       " 'wentworth': 438,\n",
       " 'frank': 439,\n",
       " 'rushworth': 440,\n",
       " 'marry': 441,\n",
       " 'men': 442,\n",
       " 'year': 443,\n",
       " 'new': 444,\n",
       " 'night': 445,\n",
       " 'wonder': 446,\n",
       " 'afraid': 447,\n",
       " 'change': 448,\n",
       " 'exactly': 449,\n",
       " 'charles': 450,\n",
       " 'matter': 451,\n",
       " 'true': 452,\n",
       " 'assure': 453,\n",
       " 'making': 454,\n",
       " 'full': 455,\n",
       " 'norris': 456,\n",
       " 'afterwards': 457,\n",
       " 'kindness': 458,\n",
       " 'satisfied': 459,\n",
       " 'appeared': 460,\n",
       " 'cousin': 461,\n",
       " 'engaged': 462,\n",
       " 'met': 463,\n",
       " 'talking': 464,\n",
       " 'gentleman': 465,\n",
       " 'stay': 466,\n",
       " 'appearance': 467,\n",
       " 'attachment': 468,\n",
       " 'equal': 469,\n",
       " 'among': 470,\n",
       " 'four': 471,\n",
       " 'real': 472,\n",
       " 'giving': 473,\n",
       " 'directly': 474,\n",
       " 'thinking': 475,\n",
       " 'dinner': 476,\n",
       " 'used': 477,\n",
       " 'hours': 478,\n",
       " 'early': 479,\n",
       " 'behaviour': 480,\n",
       " 'nobody': 481,\n",
       " 'likely': 482,\n",
       " 'five': 483,\n",
       " 'scarcely': 484,\n",
       " 'believed': 485,\n",
       " 'particular': 486,\n",
       " 'pleased': 487,\n",
       " 'entered': 488,\n",
       " 'turned': 489,\n",
       " 'wickham': 490,\n",
       " 'sometimes': 491,\n",
       " 'particularly': 492,\n",
       " 'alone': 493,\n",
       " 'street': 494,\n",
       " 'handsome': 495,\n",
       " 'work': 496,\n",
       " 'company': 497,\n",
       " 'week': 498,\n",
       " 'usual': 499,\n",
       " 'natural': 500,\n",
       " 'convinced': 501,\n",
       " 'got': 502,\n",
       " 'sat': 503,\n",
       " 'smith': 504,\n",
       " 'mansfield': 505,\n",
       " 'lucy': 506,\n",
       " 'use': 507,\n",
       " 'imagine': 508,\n",
       " 'engagement': 509,\n",
       " 'certain': 510,\n",
       " 'asked': 511,\n",
       " 'degree': 512,\n",
       " 'bad': 513,\n",
       " 'forward': 514,\n",
       " 'ask': 515,\n",
       " 'strong': 516,\n",
       " 'speaking': 517,\n",
       " 'farther': 518,\n",
       " 'collins': 519,\n",
       " 'conduct': 520,\n",
       " 'allow': 521,\n",
       " 'bear': 522,\n",
       " 'open': 523,\n",
       " 'greater': 524,\n",
       " 'call': 525,\n",
       " 'occasion': 526,\n",
       " 'susan': 527,\n",
       " 'table': 528,\n",
       " 'common': 529,\n",
       " 'air': 530,\n",
       " 'consequence': 531,\n",
       " 'ten': 532,\n",
       " 'necessary': 533,\n",
       " 'smile': 534,\n",
       " 'respect': 535,\n",
       " 'keep': 536,\n",
       " 'consider': 537,\n",
       " 'hoped': 538,\n",
       " 'appear': 539,\n",
       " 'small': 540,\n",
       " 'anxious': 541,\n",
       " 'question': 542,\n",
       " 'child': 543,\n",
       " 'lydia': 544,\n",
       " 'case': 545,\n",
       " 'face': 546,\n",
       " 'drawing': 547,\n",
       " 'surprise': 548,\n",
       " 'probably': 549,\n",
       " 'society': 550,\n",
       " 'advantage': 551,\n",
       " 'remember': 552,\n",
       " 'round': 553,\n",
       " 'circumstances': 554,\n",
       " 'maria': 555,\n",
       " 'amiable': 556,\n",
       " 'taste': 557,\n",
       " 'write': 558,\n",
       " 'truth': 559,\n",
       " 'greatest': 560,\n",
       " 'notice': 561,\n",
       " 'supposed': 562,\n",
       " 'former': 563,\n",
       " 'taking': 564,\n",
       " 'large': 565,\n",
       " 'meet': 566,\n",
       " 'hartfield': 567,\n",
       " 'whatever': 568,\n",
       " 'everybody': 569,\n",
       " 'expect': 570,\n",
       " 'months': 571,\n",
       " 'grant': 572,\n",
       " 'beauty': 573,\n",
       " 'meant': 574,\n",
       " 'park': 575,\n",
       " 'thoughts': 576,\n",
       " 'spite': 577,\n",
       " 'need': 578,\n",
       " 'knowledge': 579,\n",
       " 'satisfaction': 580,\n",
       " 'meeting': 581,\n",
       " 'sitting': 582,\n",
       " 'instantly': 583,\n",
       " 'james': 584,\n",
       " 'pain': 585,\n",
       " 'countenance': 586,\n",
       " 'second': 587,\n",
       " 'hearing': 588,\n",
       " 'serious': 589,\n",
       " 'cold': 590,\n",
       " 'wishes': 591,\n",
       " 'walking': 592,\n",
       " 'temper': 593,\n",
       " 'honour': 594,\n",
       " 'russell': 595,\n",
       " 'silence': 596,\n",
       " 'morland': 597,\n",
       " 'bates': 598,\n",
       " 'followed': 599,\n",
       " 'worth': 600,\n",
       " 'acquainted': 601,\n",
       " 'allowed': 602,\n",
       " 'pleasant': 603,\n",
       " 'thorpe': 604,\n",
       " 'equally': 605,\n",
       " 'proper': 606,\n",
       " 'musgrove': 607,\n",
       " 'beginning': 608,\n",
       " 'fancy': 609,\n",
       " 'eye': 610,\n",
       " 'brandon': 611,\n",
       " 'sensible': 612,\n",
       " 'delightful': 613,\n",
       " 'delighted': 614,\n",
       " 'extremely': 615,\n",
       " 'curiosity': 616,\n",
       " 'sent': 617,\n",
       " 'walter': 618,\n",
       " 'girls': 619,\n",
       " 'self': 620,\n",
       " 'bring': 621,\n",
       " 'answered': 622,\n",
       " 'considered': 623,\n",
       " 'daughters': 624,\n",
       " 'ago': 625,\n",
       " 'fond': 626,\n",
       " 'wrong': 627,\n",
       " 'julia': 628,\n",
       " 'easy': 629,\n",
       " 'sight': 630,\n",
       " 'play': 631,\n",
       " 'allen': 632,\n",
       " 'opportunity': 633,\n",
       " 'late': 634,\n",
       " 'twenty': 635,\n",
       " 'health': 636,\n",
       " 'worse': 637,\n",
       " 'charlotte': 638,\n",
       " 'evil': 639,\n",
       " 'pride': 640,\n",
       " 'creature': 641,\n",
       " 'aware': 642,\n",
       " 'louisa': 643,\n",
       " 'morrow': 644,\n",
       " 'thousand': 645,\n",
       " 'ferrars': 646,\n",
       " 'difference': 647,\n",
       " 'living': 648,\n",
       " 'sit': 649,\n",
       " 'dance': 650,\n",
       " 'arrival': 651,\n",
       " 'comfortable': 652,\n",
       " 'letters': 653,\n",
       " 'silent': 654,\n",
       " 'disposition': 655,\n",
       " 'admiration': 656,\n",
       " 'high': 657,\n",
       " 'distress': 658,\n",
       " 'spent': 659,\n",
       " 'sake': 660,\n",
       " 'view': 661,\n",
       " 'turn': 662,\n",
       " 'times': 663,\n",
       " 'except': 664,\n",
       " 'resolved': 665,\n",
       " 'excellent': 666,\n",
       " 't': 667,\n",
       " 'seem': 668,\n",
       " 'praise': 669,\n",
       " 'looks': 670,\n",
       " 'remained': 671,\n",
       " 'money': 672,\n",
       " 'became': 673,\n",
       " 'circumstance': 674,\n",
       " 'past': 675,\n",
       " 'influence': 676,\n",
       " 'loved': 677,\n",
       " 'weeks': 678,\n",
       " 'plan': 679,\n",
       " 'superior': 680,\n",
       " 'none': 681,\n",
       " 'danger': 682,\n",
       " 'thus': 683,\n",
       " 'fair': 684,\n",
       " 'observed': 685,\n",
       " 'seems': 686,\n",
       " 'invitation': 687,\n",
       " 'meaning': 688,\n",
       " 'price': 689,\n",
       " 'lost': 690,\n",
       " 'mentioned': 691,\n",
       " 'understanding': 692,\n",
       " 'merely': 693,\n",
       " 'favour': 694,\n",
       " 'knowing': 695,\n",
       " 'ball': 696,\n",
       " 'match': 697,\n",
       " 'live': 698,\n",
       " 'fixed': 699,\n",
       " 'highbury': 700,\n",
       " 'instead': 701,\n",
       " 'persuaded': 702,\n",
       " 'companion': 703,\n",
       " 'surprised': 704,\n",
       " 'happened': 705,\n",
       " 'purpose': 706,\n",
       " 'regret': 707,\n",
       " 'anybody': 708,\n",
       " 'yours': 709,\n",
       " 'judgment': 710,\n",
       " 'law': 711,\n",
       " 'leaving': 712,\n",
       " 'spoken': 713,\n",
       " 'easily': 714,\n",
       " 'hopes': 715,\n",
       " 'absence': 716,\n",
       " 'journey': 717,\n",
       " 'delight': 718,\n",
       " 'mistaken': 719,\n",
       " 'especially': 720,\n",
       " 'during': 721,\n",
       " 'ma': 722,\n",
       " 'thank': 723,\n",
       " 'attentions': 724,\n",
       " 'duty': 725,\n",
       " 'effect': 726,\n",
       " 'gratitude': 727,\n",
       " 'breakfast': 728,\n",
       " 'kept': 729,\n",
       " 'trouble': 730,\n",
       " 'also': 731,\n",
       " 'assured': 732,\n",
       " 'information': 733,\n",
       " 'event': 734,\n",
       " 'women': 735,\n",
       " 'highly': 736,\n",
       " 'vain': 737,\n",
       " 'inclination': 738,\n",
       " 'perfect': 739,\n",
       " 'arrived': 740,\n",
       " 'charming': 741,\n",
       " 'concern': 742,\n",
       " 'news': 743,\n",
       " 'future': 744,\n",
       " 'lived': 745,\n",
       " 'offer': 746,\n",
       " 'cottage': 747,\n",
       " 'tried': 748,\n",
       " 'desire': 749,\n",
       " 'age': 750,\n",
       " 'friendship': 751,\n",
       " 'mine': 752,\n",
       " 'spirit': 753,\n",
       " 'resolution': 754,\n",
       " 'dearest': 755,\n",
       " 'struck': 756,\n",
       " 'forget': 757,\n",
       " 'chance': 758,\n",
       " 'finding': 759,\n",
       " 'fear': 760,\n",
       " 'favourite': 761,\n",
       " 'turning': 762,\n",
       " 'weather': 763,\n",
       " 'pray': 764,\n",
       " 'whenever': 765,\n",
       " 'hands': 766,\n",
       " 'disappointment': 767,\n",
       " 'disposed': 768,\n",
       " 'written': 769,\n",
       " 'vernon': 770,\n",
       " 'enjoyment': 771,\n",
       " 'loss': 772,\n",
       " 'style': 773,\n",
       " 'share': 774,\n",
       " 'tone': 775,\n",
       " 'reached': 776,\n",
       " 'quiet': 777,\n",
       " 'repeated': 778,\n",
       " 'above': 779,\n",
       " 'led': 780,\n",
       " 'behind': 781,\n",
       " 'ease': 782,\n",
       " 'prevent': 783,\n",
       " 'credit': 784,\n",
       " 'conviction': 785,\n",
       " 'ashamed': 786,\n",
       " 'absolutely': 787,\n",
       " 'cause': 788,\n",
       " 'length': 789,\n",
       " 'fact': 790,\n",
       " 'decided': 791,\n",
       " 'gentlemen': 792,\n",
       " 'depend': 793,\n",
       " 'says': 794,\n",
       " 'pity': 795,\n",
       " 'importance': 796,\n",
       " 'smallest': 797,\n",
       " 'cousins': 798,\n",
       " 'eager': 799,\n",
       " 'interesting': 800,\n",
       " 'affectionate': 801,\n",
       " 'angry': 802,\n",
       " 'marrying': 803,\n",
       " 'middleton': 804,\n",
       " 'scheme': 805,\n",
       " 'distance': 806,\n",
       " 'confidence': 807,\n",
       " 'admiral': 808,\n",
       " 'unhappy': 809,\n",
       " 'exceedingly': 810,\n",
       " 'yesterday': 811,\n",
       " 'dancing': 812,\n",
       " 'otherwise': 813,\n",
       " 'listened': 814,\n",
       " 'mention': 815,\n",
       " 'anxiety': 816,\n",
       " 'following': 817,\n",
       " 'act': 818,\n",
       " 'difficulty': 819,\n",
       " 'trying': 820,\n",
       " 'light': 821,\n",
       " 'humour': 822,\n",
       " 'forced': 823,\n",
       " 'miles': 824,\n",
       " 'warm': 825,\n",
       " 'understood': 826,\n",
       " 'pleasing': 827,\n",
       " 'sweet': 828,\n",
       " 'excuse': 829,\n",
       " 'pass': 830,\n",
       " 'ah': 831,\n",
       " 'writing': 832,\n",
       " 'tea': 833,\n",
       " 'terms': 834,\n",
       " 'eldest': 835,\n",
       " 'alarm': 836,\n",
       " 'consideration': 837,\n",
       " 'reply': 838,\n",
       " 'receive': 839,\n",
       " 'window': 840,\n",
       " 'low': 841,\n",
       " 'promised': 842,\n",
       " 'altogether': 843,\n",
       " 'generally': 844,\n",
       " 'justice': 845,\n",
       " 'itself': 846,\n",
       " 'wait': 847,\n",
       " 'joined': 848,\n",
       " 'project': 849,\n",
       " 'niece': 850,\n",
       " 'martin': 851,\n",
       " 'wishing': 852,\n",
       " 'waiting': 853,\n",
       " 'avoid': 854,\n",
       " 'promise': 855,\n",
       " 'lord': 856,\n",
       " 'neighbourhood': 857,\n",
       " 'attended': 858,\n",
       " 'agitation': 859,\n",
       " 'compliment': 860,\n",
       " 'attached': 861,\n",
       " 'lizzy': 862,\n",
       " 'gardiner': 863,\n",
       " 'attempt': 864,\n",
       " 'civility': 865,\n",
       " 'wanting': 866,\n",
       " 'formed': 867,\n",
       " 'misery': 868,\n",
       " 'send': 869,\n",
       " 'please': 870,\n",
       " 'de': 871,\n",
       " 'death': 872,\n",
       " 'besides': 873,\n",
       " 'occurred': 874,\n",
       " 'pounds': 875,\n",
       " 'intended': 876,\n",
       " 'astonishment': 877,\n",
       " 'begin': 878,\n",
       " 'join': 879,\n",
       " 'fortnight': 880,\n",
       " 'gutenberg': 881,\n",
       " 'close': 882,\n",
       " 'rooms': 883,\n",
       " 'expressed': 884,\n",
       " 'actually': 885,\n",
       " 'servant': 886,\n",
       " 'due': 887,\n",
       " 'agreed': 888,\n",
       " 'choice': 889,\n",
       " 'henrietta': 890,\n",
       " 'six': 891,\n",
       " 'expectation': 892,\n",
       " 'tom': 893,\n",
       " 'support': 894,\n",
       " 'note': 895,\n",
       " 'particulars': 896,\n",
       " 'ladyship': 897,\n",
       " 'randalls': 898,\n",
       " 'barton': 899,\n",
       " 'reading': 900,\n",
       " 'latter': 901,\n",
       " 'pause': 902,\n",
       " 'immediate': 903,\n",
       " 'proof': 904,\n",
       " 'returning': 905,\n",
       " 'produced': 906,\n",
       " 'sudden': 907,\n",
       " 'hers': 908,\n",
       " 'form': 909,\n",
       " 'quick': 910,\n",
       " 'judge': 911,\n",
       " 'prevented': 912,\n",
       " 'nearly': 913,\n",
       " 'longbourn': 914,\n",
       " 'dreadful': 915,\n",
       " 'caught': 916,\n",
       " 'beautiful': 917,\n",
       " 'joy': 918,\n",
       " 'bed': 919,\n",
       " 'speech': 920,\n",
       " 'indifference': 921,\n",
       " 'pay': 922,\n",
       " 'horses': 923,\n",
       " 'folly': 924,\n",
       " 'strange': 925,\n",
       " 'knows': 926,\n",
       " 'welcome': 927,\n",
       " 'vanity': 928,\n",
       " 'capable': 929,\n",
       " 'remain': 930,\n",
       " 'greatly': 931,\n",
       " 'parsonage': 932,\n",
       " 'necessity': 933,\n",
       " 'useful': 934,\n",
       " 'income': 935,\n",
       " 'quarter': 936,\n",
       " 'command': 937,\n",
       " 'cheerful': 938,\n",
       " 'proved': 939,\n",
       " 'listen': 940,\n",
       " 'express': 941,\n",
       " 'suffered': 942,\n",
       " 'lively': 943,\n",
       " 'address': 944,\n",
       " 'frederica': 945,\n",
       " 'opened': 946,\n",
       " 'consciousness': 947,\n",
       " 'several': 948,\n",
       " 'period': 949,\n",
       " 'persuade': 950,\n",
       " 'intimacy': 951,\n",
       " 'extraordinary': 952,\n",
       " 'plain': 953,\n",
       " 'parties': 954,\n",
       " 'smiling': 955,\n",
       " 'moments': 956,\n",
       " 'stand': 957,\n",
       " 'book': 958,\n",
       " 'advice': 959,\n",
       " 'public': 960,\n",
       " 'value': 961,\n",
       " 'getting': 962,\n",
       " 'trust': 963,\n",
       " 'elegant': 964,\n",
       " 'wrote': 965,\n",
       " 'comes': 966,\n",
       " 'tears': 967,\n",
       " 'amusement': 968,\n",
       " 'acknowledged': 969,\n",
       " 'receiving': 970,\n",
       " 'fears': 971,\n",
       " 'observation': 972,\n",
       " 'education': 973,\n",
       " 'eleanor': 974,\n",
       " 'reginald': 975,\n",
       " 'disappointed': 976,\n",
       " 'spend': 977,\n",
       " 'important': 978,\n",
       " 'safe': 979,\n",
       " 'hundred': 980,\n",
       " 'fire': 981,\n",
       " 'horse': 982,\n",
       " 'try': 983,\n",
       " 'considering': 984,\n",
       " 'perry': 985,\n",
       " 'hall': 986,\n",
       " 'required': 987,\n",
       " 'secret': 988,\n",
       " 'lately': 989,\n",
       " 'staying': 990,\n",
       " 'hurry': 991,\n",
       " 'expression': 992,\n",
       " 'calling': 993,\n",
       " 'paid': 994,\n",
       " 'reflection': 995,\n",
       " 'delay': 996,\n",
       " 'palmer': 997,\n",
       " 'respectable': 998,\n",
       " 'explanation': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id_dict = tokenize_and_build_vocabulary_tf(['/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'],vocab_size = 5000)\n",
    "token_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9075b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756542260.516338    1336 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=string, numpy=\n",
       "array([b'A', b'k', b's', b'h', b'a', b't', b' ', b'K', b'h', b'a', b't',\n",
       "       b'r', b'i'], dtype=object)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = tf.constant('Akshat Khatri')\n",
    "tf.strings.bytes_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45984209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello'\n",
      "b'Worlds '\n"
     ]
    }
   ],
   "source": [
    "name = tf.constant(['Hello','Worlds '])\n",
    "for name in name.numpy():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ba567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1.,2.,3.])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046629d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(11, 512), dtype=int32, numpy=\n",
       " array([[   1,    1,    0, ...,    0,    0,    0],\n",
       "        [   1,    0,    0, ...,    0,    0,    0],\n",
       "        [  45,    0,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 389,    0,    0, ...,    0,    0,    0],\n",
       "        [   1,    0,    0, ...,    0,    0,    0],\n",
       "        [3304,    0,    0, ...,    0,    0,    0]],\n",
       "       shape=(11, 512), dtype=int32)>,\n",
       " <tf.Tensor: shape=(11, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]], shape=(11, 512), dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Akshat Khatri','hello','me','elizabeth','children','mine','coherent','i am fine','Children','trained','model']\n",
    "tokenize_and_build_token_id(token_to_id_dict,name,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d1a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable\n",
    "\n",
    "# class InitializePositionalEmbeddings(keras.layers.Layer): # Receives input of sequence of text\n",
    "#     def __init__(self,d_model: int = 128,sinusoidal_lookup_table = [],token_to_id_dict : tf.lookup.StaticHashTable = {} ,max_seq_len : int = 512,**kwargs): # type: ignore\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.d_model = d_model # d_model\n",
    "#         self.max_seq_len = max_seq_len\n",
    "        \n",
    "#         assert len(sinusoidal_lookup_table) > 0\n",
    "#         assert token_to_id_dict.size().numpy() > 0\n",
    "#         self.VOCAB_SIZE = token_to_id_dict.size().numpy()\n",
    "\n",
    "#         self.pos_table = sinusoidal_lookup_table\n",
    "#         self._embedding_dim = [self.VOCAB_SIZE,d_model]\n",
    "#         self.token_to_id_dict = token_to_id_dict\n",
    "    \n",
    "#     def build(self, input_shape): # this is batch input shape\n",
    "#         print(input_shape)\n",
    "#         self.embedding_matrix = self.add_weight(\n",
    "#             name=\"embedding_matrix\",\n",
    "#             shape=(self.VOCAB_SIZE, self.d_model),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True   # important\n",
    "#         )\n",
    "#         self.input_seq_list = input_shape[-1]\n",
    "\n",
    "#     def call(self,inputs):\n",
    "#         # print(inputs)\n",
    "#         tokens_in_id,non_padded_tokens_mask = tokenize_and_build_token_id(self.token_to_id_dict,inputs)\n",
    "#         # print(tokens_in_id,non_padded_tokens_mask,sep = '\\n')\n",
    "#         token_embeddings = tf.nn.embedding_lookup(self.embedding_matrix, tokens_in_id)\n",
    "#         # Positional embeddings\n",
    "#         seq_len = tf.shape(tokens_in_id)[1] # type: ignore\n",
    "#         pos_embeddings = self.pos_table[:seq_len, :]\n",
    "#         pos_embeddings = tf.expand_dims(pos_embeddings, 0)  # broadcast along batch\n",
    "#         # Add token + position embeddings\n",
    "#         embeddings = token_embeddings + pos_embeddings\n",
    "#         return embeddings,non_padded_tokens_mask\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         base_config = super().get_config()\n",
    "#         return {**base_config,'EMBEDDING_SIZE' : self.EMBEDDING_SIZE,'VOCAB_SIZE' : self.VOCAB_SIZE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d037241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        CONTEXT_LEN: int = 128,\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, CONTEXT_LEN)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c431a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(token_to_id_dict)\n",
    "D_MODEL = 1024\n",
    "MAX_SEQ_LEN = 512\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd66a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL)\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "batch_text = tokenize_and_build_token_id(token_to_id_dict,batch_text,MAX_SEQ_LEN) # type: ignore\n",
    "token_ids,attention_mask = batch_text\n",
    "\n",
    "layer = InitializePositionalEmbeddings(D_MODEL,VOCAB_SIZE)\n",
    "\n",
    "# @tf.function\n",
    "# def call_some(batch_text):\n",
    "#     embeddings = layer(batch_text)\n",
    "#     return embeddings\n",
    "\n",
    "# call_some(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6e1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        \n",
    "        self.Query_projection = self.add_weight(\n",
    "            name='Query_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name='Key_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name='Value_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.output_projection = self.add_weight(\n",
    "            name=\"Output_projection\",\n",
    "            initializer=\"random_normal\",\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads\n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0]\n",
    "        seq_len = tf.shape(embeddings)[1]\n",
    "\n",
    "        # 1. Project to Q, K, V\n",
    "        Q = embeddings @ self.Query_projection\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        # 3. Compute attention scores\n",
    "        scores = tf.matmul(Q, K, transpose_b=True)  # (batch, heads, seq_len, seq_len)\n",
    "        scores = scores / tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        \n",
    "        # 4. FIXED MASKING - This was your main bug\n",
    "        # 4a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        \n",
    "        # 4b. Token mask - FIXED: proper broadcasting to all heads\n",
    "        token_mask = tf.cast(token_masks, tf.float32)  # (B, L)\n",
    "        \n",
    "        # Create proper attention mask shape (B, H, L, L)\n",
    "        # Each head gets the same mask pattern\n",
    "        attention_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, L, L)\n",
    "        attention_mask = attention_mask * token_mask[:, tf.newaxis, tf.newaxis, :]  # (B, 1, 1, L)\n",
    "        attention_mask = attention_mask * token_mask[:, tf.newaxis, :, tf.newaxis]  # (B, 1, L, 1)\n",
    "        \n",
    "        # Broadcast to all heads\n",
    "        attention_mask = tf.broadcast_to(attention_mask, (batch_size, self.attention_heads, seq_len, seq_len))\n",
    "        \n",
    "        # 5. Apply mask with stronger negative value\n",
    "        scores = tf.where(\n",
    "            attention_mask > 0, \n",
    "            scores, \n",
    "            tf.constant(-1e30, dtype=scores.dtype)  # FIXED: Much more negative\n",
    "        )\n",
    "\n",
    "        # 6. Softmax and apply to values\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Add attention dropout (missing in your original)\n",
    "        attention_weights = tf.nn.dropout(attention_weights, rate=0.1)\n",
    "        \n",
    "        context = attention_weights @ V   # (batch, heads, seq_len, d_head)\n",
    "        \n",
    "        # 7. Concatenate heads\n",
    "        concat_context = tf.transpose(context, (0, 2, 1, 3))  # (batch, seq_len, heads, d_head)\n",
    "        concat_context = tf.reshape(concat_context, (batch_size, seq_len, self.d_model))\n",
    "        \n",
    "        # 8. Final projection\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a50e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 13, 4, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], shape=(16, 13, 4, 3), dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = np.random.rand(3, 13, 64)  # batch, seq_len, d_model\n",
    "q = np.ones((64, 64))          # d_model  d_model\n",
    "\n",
    "a = tf.constant(a, dtype=tf.float32)\n",
    "q = tf.constant(q, dtype=tf.float32)\n",
    "\n",
    "s = a @ q   # type: ignore # [3, 13, 64]\n",
    "d_head = 16\n",
    "num_heads = 64 // d_head # 4\n",
    "\n",
    "# split into heads\n",
    "s = tf.reshape(s, (3, num_heads, 13, d_head))  # [3, 4, 13, 16]\n",
    "f = tf.constant(np.ones_like(s))\n",
    "\n",
    "tf.transpose(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873ee087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.95257413, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.04742587, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,9,4,5],[4,5,6,7,8]])\n",
    "keras.activations.softmax(tf.cast(arr,dtype = tf.float32),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb91875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.04742587, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.95257413, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,3,4,5],[4,5,6,7,8]])\n",
    "arr = tf.cast(arr,dtype = tf.float32)\n",
    "\n",
    "tf.nn.softmax(arr,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7cc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000000.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0da4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7a994b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, seq_len, input_dim)\n",
    "        output = tf.matmul(inputs, self.kernel) + self.bias  # shape: (batch, seq_len, units))  # shape: (batch, seq_len, units)\n",
    "        return output + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch, seq_len, input_dim)\n",
    "        return (input_shape, input_shape[1], self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb7c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2,3,4,5,6)\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea053e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,context_length,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd143548",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 64\n",
    "VOCAB_SIZE = len(token_to_id_dict) # 94 currently char level\n",
    "\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids,attention_mask = tokenize_and_build_token_id(token_to_id_dict,batch_text,CONTEXT_LEN) # type: ignore # Unpacking Values\n",
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d639c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 64), dtype=int32, numpy=\n",
       " array([[  1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  1,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  1, 216,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(4, 64), dtype=int32, numpy=\n",
       " array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token_ids,attention_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f0db5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class CosineDecayWithWarmup(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, \n",
    "                 warmup_steps: int,\n",
    "                 total_steps: int,\n",
    "                 peak_learning_rate: float = 1e-4,\n",
    "                 min_learning_rate: float = 1e-6,\n",
    "                 name: str = \"cosine_decay_with_warmup\"):\n",
    "        super().__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase: linear increase from 0 to peak_learning_rate\n",
    "        warmup_lr = self.peak_learning_rate * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        cosine_decay_lr = self.min_learning_rate + 0.5 * (\n",
    "            self.peak_learning_rate - self.min_learning_rate\n",
    "        ) * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
    "        \n",
    "        return tf.where(step < warmup_steps, warmup_lr, cosine_decay_lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"peak_learning_rate\": self.peak_learning_rate,\n",
    "            \"min_learning_rate\": self.min_learning_rate,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "\n",
    "# Example usage for your model\n",
    "# Estimate your training parameters\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCH = 1000  # Adjust based on your dataset size and batch size\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
    "WARMUP_STEPS = int(0.1 * TOTAL_STEPS)  # 10% warmup\n",
    "\n",
    "# Create the learning rate schedule\n",
    "lr_schedule = CosineDecayWithWarmup(\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    peak_learning_rate=1e-4,  # Your desired peak learning rate\n",
    "    min_learning_rate=1e-6    # Minimum learning rate at the end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f54ded",
   "metadata": {},
   "source": [
    "## Modify above according to requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45b6a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " init_embeddings                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding</span>                                        \n",
       "\n",
       " decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization         (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_1       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer        (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_2       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_3       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_1      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_4       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_5       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_2      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_6       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_7       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_3      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_8       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_9       (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_4      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_10      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_11      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_5      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_12      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_13      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_6      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " decoder_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       "     layer_normalization_14      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     layer_normalization_15      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       "     self_attention_layer_7      (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)                                                   \n",
       "\n",
       "     dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       "     dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> \n",
       "\n",
       "     dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> \n",
       "\n",
       "     dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " layer_normalization_16           (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       " Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">5,125,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " init_embeddings                  (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m5,120,000\u001b[0m \n",
       " (\u001b[38;5;33mInitializePositionalEmbedding\u001b[0m                                        \n",
       "\n",
       " decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization         (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_1       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer        (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout (\u001b[38;5;33mDropout\u001b[0m)           ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense (\u001b[38;5;33mDense\u001b[0m)               (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_1 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_1 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_2       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_3       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_1      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_2 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_2 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_3 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_3 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_4       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_5       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_2      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_4 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_4 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_5 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_5 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_6       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_7       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_3      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_6 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_6 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_7 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_7 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_4 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_8       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_9       (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_4      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_8 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_8 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_9 (\u001b[38;5;33mDense\u001b[0m)             (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_9 (\u001b[38;5;33mDropout\u001b[0m)         ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_5 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_10      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_11      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_5      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_10 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_10 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_11 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_11 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_6 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_12      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_13      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_6      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_12 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_12 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_13 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_13 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " decoder_block_7 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       "     layer_normalization_14      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     layer_normalization_15      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       "     self_attention_layer_7      (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,194,304\u001b[0m \n",
       " (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)                                                   \n",
       "\n",
       "     dropout_14 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       "     dense_14 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)               \u001b[38;5;34m4,198,400\u001b[0m \n",
       "\n",
       "     dense_15 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m4,195,328\u001b[0m \n",
       "\n",
       "     dropout_15 (\u001b[38;5;33mDropout\u001b[0m)        ?                                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " layer_normalization_16           (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       " Model_head (\u001b[38;5;33mDense\u001b[0m)               (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5000\u001b[0m)               \u001b[38;5;34m5,125,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build once to get .summary()\n",
    "DECODER_BLOCKS = 8\n",
    "ATTENTION_HEADS = 4\n",
    "\n",
    "GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,0.1)\n",
    "_ = GPT_model((token_ids, attention_mask))\n",
    "GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# training (stable): use logits\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0)\n",
    "GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# inference probs (when you actually need them)\n",
    "logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "probs = keras.ops.softmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc4eb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 64, 5000), dtype=float32, numpy=\n",
       "array([[[9.17721263e-05, 1.58267620e-04, 7.89049882e-05, ...,\n",
       "         4.62345524e-05, 1.11462432e-04, 2.63327558e-04],\n",
       "        [2.16269196e-04, 3.55634198e-04, 1.23658232e-04, ...,\n",
       "         1.27934618e-04, 1.64374433e-04, 1.01247846e-04],\n",
       "        [2.25262251e-04, 3.35881399e-04, 1.28588930e-04, ...,\n",
       "         1.23376216e-04, 1.56066497e-04, 1.05651168e-04],\n",
       "        ...,\n",
       "        [2.38189517e-04, 3.20498715e-04, 1.23040430e-04, ...,\n",
       "         1.24532031e-04, 1.51542234e-04, 1.22151483e-04],\n",
       "        [2.32070524e-04, 3.26718087e-04, 1.24747923e-04, ...,\n",
       "         1.22275713e-04, 1.55416099e-04, 1.22848462e-04],\n",
       "        [2.32014121e-04, 3.25196423e-04, 1.19121287e-04, ...,\n",
       "         1.19630742e-04, 1.57797651e-04, 1.21494486e-04]],\n",
       "\n",
       "       [[1.21259392e-04, 1.85770652e-04, 9.58174496e-05, ...,\n",
       "         5.75506056e-05, 1.00423167e-04, 3.54145566e-04],\n",
       "        [1.09879955e-04, 2.03619085e-04, 8.85180925e-05, ...,\n",
       "         4.91684514e-05, 1.00627884e-04, 3.35431163e-04],\n",
       "        [2.30119884e-04, 3.41016857e-04, 1.19740675e-04, ...,\n",
       "         1.21624682e-04, 1.62198383e-04, 1.05980849e-04],\n",
       "        ...,\n",
       "        [2.42585957e-04, 3.35450924e-04, 1.17334523e-04, ...,\n",
       "         1.23407750e-04, 1.54703972e-04, 1.28248881e-04],\n",
       "        [2.28670411e-04, 3.52165953e-04, 1.21338293e-04, ...,\n",
       "         1.31195600e-04, 1.58775380e-04, 1.25402847e-04],\n",
       "        [2.34982333e-04, 3.26270441e-04, 1.20304248e-04, ...,\n",
       "         1.25254533e-04, 1.57578397e-04, 1.24298458e-04]],\n",
       "\n",
       "       [[9.25674176e-05, 2.16106331e-04, 7.63795761e-05, ...,\n",
       "         5.99753366e-05, 1.84983277e-04, 2.87745934e-04],\n",
       "        [1.03967446e-04, 2.16336237e-04, 7.84033909e-05, ...,\n",
       "         5.12811930e-05, 1.52075067e-04, 2.35437488e-04],\n",
       "        [2.20210030e-04, 3.37449630e-04, 1.26653002e-04, ...,\n",
       "         1.24424754e-04, 1.58605937e-04, 1.10730718e-04],\n",
       "        ...,\n",
       "        [2.18491085e-04, 3.32101772e-04, 1.22398153e-04, ...,\n",
       "         1.25486375e-04, 1.53812798e-04, 1.26359650e-04],\n",
       "        [2.28387406e-04, 3.47954483e-04, 1.21454577e-04, ...,\n",
       "         1.23714824e-04, 1.68886632e-04, 1.22563943e-04],\n",
       "        [2.32604827e-04, 3.29013448e-04, 1.28014843e-04, ...,\n",
       "         1.23884121e-04, 1.70317304e-04, 1.17599462e-04]],\n",
       "\n",
       "       [[1.47446452e-04, 2.96174199e-04, 1.06104802e-04, ...,\n",
       "         4.95415698e-05, 1.10551402e-04, 1.79247450e-04],\n",
       "        [2.36346008e-04, 3.36508238e-04, 1.24687489e-04, ...,\n",
       "         1.22591853e-04, 1.59353818e-04, 9.94546936e-05],\n",
       "        [2.41194983e-04, 3.47900874e-04, 1.25383100e-04, ...,\n",
       "         1.25585124e-04, 1.56057868e-04, 1.05222854e-04],\n",
       "        ...,\n",
       "        [2.44988740e-04, 3.39755148e-04, 1.27788371e-04, ...,\n",
       "         1.24598548e-04, 1.52739711e-04, 1.16298339e-04],\n",
       "        [2.31381564e-04, 3.38982703e-04, 1.26301471e-04, ...,\n",
       "         1.22323574e-04, 1.60539508e-04, 1.14169343e-04],\n",
       "        [2.31517508e-04, 3.32047348e-04, 1.24127226e-04, ...,\n",
       "         1.21430676e-04, 1.66472862e-04, 1.15081159e-04]]],\n",
       "      shape=(4, 64, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81260874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 64, 5000), dtype=float32, numpy=\n",
       "array([[[1.12514266e-04, 2.23259922e-04, 6.15297249e-05, ...,\n",
       "         6.33913223e-05, 1.57999486e-04, 1.34142581e-04],\n",
       "        [2.24163901e-04, 3.53680662e-04, 1.30546076e-04, ...,\n",
       "         1.20510158e-04, 1.67891340e-04, 9.84475919e-05],\n",
       "        [2.23144059e-04, 3.37189354e-04, 1.26456565e-04, ...,\n",
       "         1.22453566e-04, 1.62833108e-04, 1.00729550e-04],\n",
       "        ...,\n",
       "        [2.32919265e-04, 3.39150371e-04, 1.21412821e-04, ...,\n",
       "         1.25228340e-04, 1.51621061e-04, 1.20937577e-04],\n",
       "        [2.31672486e-04, 3.44316504e-04, 1.17227486e-04, ...,\n",
       "         1.23874546e-04, 1.63023476e-04, 1.11811649e-04],\n",
       "        [2.25805503e-04, 3.47522349e-04, 1.23126199e-04, ...,\n",
       "         1.23148275e-04, 1.69446255e-04, 1.13670518e-04]],\n",
       "\n",
       "       [[1.33764363e-04, 2.00560782e-04, 1.13622555e-04, ...,\n",
       "         6.27676636e-05, 8.23706287e-05, 3.12353688e-04],\n",
       "        [9.59440003e-05, 2.28337434e-04, 1.06569569e-04, ...,\n",
       "         4.90231469e-05, 1.55797723e-04, 2.79480475e-04],\n",
       "        [2.34003455e-04, 3.32135620e-04, 1.24253580e-04, ...,\n",
       "         1.22030862e-04, 1.67993989e-04, 1.09853652e-04],\n",
       "        ...,\n",
       "        [2.32153194e-04, 3.44341708e-04, 1.27446721e-04, ...,\n",
       "         1.19022698e-04, 1.59978546e-04, 1.21809426e-04],\n",
       "        [2.32409118e-04, 3.48455651e-04, 1.24696046e-04, ...,\n",
       "         1.23135775e-04, 1.61297052e-04, 1.27275562e-04],\n",
       "        [2.37164888e-04, 3.30688083e-04, 1.21809455e-04, ...,\n",
       "         1.20721066e-04, 1.58671406e-04, 1.24390266e-04]],\n",
       "\n",
       "       [[1.19114309e-04, 1.73937442e-04, 7.14267080e-05, ...,\n",
       "         5.08646117e-05, 1.22634170e-04, 2.24676420e-04],\n",
       "        [7.56033114e-05, 2.12478073e-04, 6.98226795e-05, ...,\n",
       "         6.06557223e-05, 1.19965975e-04, 2.42279857e-04],\n",
       "        [2.23755182e-04, 3.41106730e-04, 1.21835437e-04, ...,\n",
       "         1.25801133e-04, 1.58681491e-04, 1.07595108e-04],\n",
       "        ...,\n",
       "        [2.29110563e-04, 3.37743404e-04, 1.18481119e-04, ...,\n",
       "         1.25874343e-04, 1.52592300e-04, 1.28047337e-04],\n",
       "        [2.28758799e-04, 3.20413819e-04, 1.27022111e-04, ...,\n",
       "         1.23694699e-04, 1.63814038e-04, 1.24019192e-04],\n",
       "        [2.38821609e-04, 3.24710156e-04, 1.29173903e-04, ...,\n",
       "         1.22507830e-04, 1.66750106e-04, 1.22278172e-04]],\n",
       "\n",
       "       [[1.09877845e-04, 1.90593317e-04, 8.54404570e-05, ...,\n",
       "         8.21886861e-05, 1.30798813e-04, 2.73981888e-04],\n",
       "        [2.21893250e-04, 3.49633629e-04, 1.30162123e-04, ...,\n",
       "         1.26120402e-04, 1.58468902e-04, 1.01052603e-04],\n",
       "        [2.36196895e-04, 3.49795155e-04, 1.29585402e-04, ...,\n",
       "         1.20994373e-04, 1.63350356e-04, 9.58833698e-05],\n",
       "        ...,\n",
       "        [2.30734979e-04, 3.35339049e-04, 1.24990023e-04, ...,\n",
       "         1.22119833e-04, 1.54798894e-04, 1.16919786e-04],\n",
       "        [2.37103493e-04, 3.56373144e-04, 1.26694911e-04, ...,\n",
       "         1.18825577e-04, 1.57894741e-04, 1.07552914e-04],\n",
       "        [2.37199361e-04, 3.27882619e-04, 1.32515968e-04, ...,\n",
       "         1.25410254e-04, 1.59602394e-04, 1.15365910e-04]]],\n",
       "      shape=(4, 64, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = GPT_model((token_ids, attention_mask))\n",
    "probs = tf.nn.softmax(outputs, axis=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ebf9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " init_embeddings                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding</span>                                        \n",
       "\n",
       " decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " decoder_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)   (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> \n",
       "\n",
       " layer_normalization_16           (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                   \n",
       "\n",
       " Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">5,125,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " init_embeddings                  (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)               \u001b[38;5;34m5,120,000\u001b[0m \n",
       " (\u001b[38;5;33mInitializePositionalEmbedding\u001b[0m                                        \n",
       "\n",
       " decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_4 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_5 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_6 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " decoder_block_7 (\u001b[38;5;33mDecoderBlock\u001b[0m)   (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)              \u001b[38;5;34m12,592,128\u001b[0m \n",
       "\n",
       " layer_normalization_16           (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m2,048\u001b[0m \n",
       " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                   \n",
       "\n",
       " Model_head (\u001b[38;5;33mDense\u001b[0m)               (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5000\u001b[0m)               \u001b[38;5;34m5,125,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e575a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diagram to gpt_model.png\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "try:\n",
    "    plot_model(\n",
    "        GPT_model,\n",
    "        to_file=\"gpt_model.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        expand_nested=True,\n",
    "        dpi=160\n",
    "    )\n",
    "    print(\"Saved diagram to gpt_model.png\")\n",
    "except Exception as e:\n",
    "    print(\"plot_model failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fdd9c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InitializePositionalEmbeddings name=init_embeddings, built=True>,\n",
       " <DecoderBlock name=decoder_block_0, built=True>,\n",
       " <DecoderBlock name=decoder_block_1, built=True>,\n",
       " <DecoderBlock name=decoder_block_2, built=True>,\n",
       " <DecoderBlock name=decoder_block_3, built=True>,\n",
       " <DecoderBlock name=decoder_block_4, built=True>,\n",
       " <DecoderBlock name=decoder_block_5, built=True>,\n",
       " <DecoderBlock name=decoder_block_6, built=True>,\n",
       " <DecoderBlock name=decoder_block_7, built=True>,\n",
       " <LayerNormalization name=layer_normalization_16, built=True>,\n",
       " <Dense name=Model_head, built=True>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe5fecf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110984072"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7809a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               GPT MODEL SUMMARY                                \n",
      "================================================================================\n",
      "Total parameters:      110,984,072\n",
      "Total layers:          11\n",
      "Trainable weights:     101\n",
      "Final output shape(s): ['(unavailable)']\n",
      "--------------------------------------------------------------------------------\n",
      "Idx | Layer Type               | Layer Name              | Weight Name                  | Shape           |   Params\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "000 | InitializePositionalEmbeddings | init_embeddings         | embedding_matrix             | (5000, 1024)    | 5,120,000\n",
      "001 | DecoderBlock             | decoder_block_0         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "002 | DecoderBlock             | decoder_block_1         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "003 | DecoderBlock             | decoder_block_2         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "004 | DecoderBlock             | decoder_block_3         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "005 | DecoderBlock             | decoder_block_4         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "006 | DecoderBlock             | decoder_block_5         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "007 | DecoderBlock             | decoder_block_6         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "008 | DecoderBlock             | decoder_block_7         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "009 | LayerNormalization       | layer_normalization_16  | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "010 | Dense                    | Model_head              | kernel                       | (1024, 5000)    | 5,120,000\n",
      "    |                          |                         | bias                         | (5000,)         |    5,000\n",
      "    |                          |                         |                              |                 |         \n",
      "================================================================================\n",
      "Note: Only trainable weights are listed above. Output shapes may be unavailable\n",
      "for subclassed models or models not built symbolically.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_model_summary(model):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'GPT MODEL SUMMARY':^80}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    total_layers = len(model.layers)\n",
    "    total_weights = sum(len(layer.trainable_weights) for layer in model.layers)\n",
    "    try:\n",
    "        output_shapes = [tuple(out.shape) for out in model.outputs]\n",
    "    except Exception:\n",
    "        output_shapes = [\"(unavailable)\"]\n",
    "    \n",
    "    print(f\"{'Total parameters:':<22} {total_params:,}\")\n",
    "    print(f\"{'Total layers:':<22} {total_layers}\")\n",
    "    print(f\"{'Trainable weights:':<22} {total_weights}\")\n",
    "    print(f\"{'Final output shape(s):':<22} {output_shapes if output_shapes else '(N/A)'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    header = f\"{'Idx':>3} | {'Layer Type':<24} | {'Layer Name':<23} | {'Weight Name':<28} | {'Shape':<15} | {'Params':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_type = layer.__class__.__name__\n",
    "        layer_name = layer.name\n",
    "        weights = layer.trainable_weights\n",
    "        layer_weight_count = len(weights)\n",
    "\n",
    "        if layer_weight_count == 0:\n",
    "            print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {'-':<28} | {'-':<15} | {'0':>8}\")\n",
    "        else:\n",
    "            for j, w in enumerate(weights):\n",
    "                n = int(np.prod(w.shape)) if hasattr(w, \"shape\") else \"?\"\n",
    "                shape_str = str(tuple(w.shape))\n",
    "                weight_name = w.name\n",
    "                if j == 0:\n",
    "                    print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "                else:\n",
    "                    print(f\"    | {'':<24} | {'':<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "        if layer_weight_count > 1:\n",
    "            print(f\"    | {'':<24} | {'':<23} | {'':<28} | {'':<15} | {'':>8}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Note: Only trainable weights are listed above. Output shapes may be unavailable\\n\"\n",
    "          \"for subclassed models or models not built symbolically.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usage:\n",
    "format_model_summary(GPT_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab03dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_book_training_data(book_text: str, \n",
    "#                              token_to_id_dict: Dict[str, int], \n",
    "#                              context_length: int = 512,\n",
    "#                              pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Prepare training data from a Gutenberg book\n",
    "#     \"\"\"\n",
    "#     # 1. Tokenize the entire book\n",
    "#     token_ids = [token_to_id_dict.get(c, pad_value) for c in book_text]\n",
    "    \n",
    "#     # 2. Create sliding windows\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "    \n",
    "#     # Slide window across the entire book\n",
    "#     for i in range(0, len(token_ids) - context_length, context_length):\n",
    "#         # Extract window of context_length + 1 tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "        \n",
    "#         if len(window) < context_length + 1:\n",
    "#             break  # Skip incomplete windows at the end\n",
    "        \n",
    "#         # Create input-target pair\n",
    "#         input_seq = window[:-1]   # [t1, t2, ..., t512]\n",
    "#         target_seq = window[1:]   # [t2, t3, ..., t513]\n",
    "        \n",
    "#         inputs.append(input_seq)\n",
    "#         targets.append(target_seq)\n",
    "    \n",
    "#     # 3. Convert to numpy arrays\n",
    "#     inputs = np.array(inputs, dtype=np.int32)\n",
    "#     targets = np.array(targets, dtype=np.int32)\n",
    "    \n",
    "#     # 4. Create padding masks (all 1s since we're using full context)\n",
    "#     masks = np.ones_like(inputs, dtype=np.int32)\n",
    "    \n",
    "#     return inputs, targets, masks\n",
    "\n",
    "# # Usage:\n",
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r', encoding='utf-8') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea8b1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# print(f\"Total characters: {len(book_text)}\")\n",
    "# print(f\"Expected examples (rough): {len(book_text) // 512}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fc7c7",
   "metadata": {},
   "source": [
    "''' Stop '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "649d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to TensorFlow tensors\n",
    "# input_ids = tf.constant(inputs, dtype=tf.int32)\n",
    "# target_ids = tf.constant(targets, dtype=tf.int32)\n",
    "# attention_masks = tf.constant(masks, dtype=tf.int32)\n",
    "\n",
    "# model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,8,0.00001,4,0.1,sinusoidal_lookup_table)\n",
    "\n",
    "# # Compile your model\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Train with .fit()\n",
    "# history = model.fit(\n",
    "#     x=[input_ids, ,  # Your model expects (token_ids, attention_mask)\n",
    "#     y=target_ids,\n",
    "#     batch_size=16,  # Start small since 677 examples isn't huge\n",
    "#     epochs=50,\n",
    "#     validation_split=0.1,  # Use 10% for validation\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),\n",
    "#         keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e775cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_text, token_to_id, id_to_token, max_len=100, temperature=1.0):\n",
    "#     # Encode start text\n",
    "#     token_ids, attention_mask = tokenize_and_build_token_id(\n",
    "#         token_to_id, [start_text], max_seq_len=512\n",
    "#     )\n",
    "    \n",
    "#     generated = list(token_ids[0].numpy())  # flatten out\n",
    "#     mask = list(attention_mask[0].numpy())\n",
    "    \n",
    "#     for _ in range(max_len):\n",
    "#         # Trim to last 512 tokens\n",
    "#         x_tokens = np.array([generated[-512:]])\n",
    "#         x_mask   = np.array([mask[-512:]])\n",
    "\n",
    "#         # Forward pass with both inputs\n",
    "#         logits = model((x_tokens, x_mask), training=False)\n",
    "\n",
    "#         # Take last position logits\n",
    "#         next_logits = logits[0, -1] / temperature\n",
    "#         probs = tf.nn.softmax(next_logits).numpy()\n",
    "\n",
    "#         # Sample next token\n",
    "#         next_id = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "#         # Append\n",
    "#         generated.append(next_id)\n",
    "#         mask.append(1)  # mark as valid token\n",
    "\n",
    "#     # Decode\n",
    "#     return ''.join(id_to_token[i] for i in generated if i in id_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d50c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token_dict = {v: k for k, v in token_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43978b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = \"Akshat Khatri\"\n",
    "# result = generate_text(model, start, token_to_id_dict, id_to_token_dict, max_len=100, temperature=0.8)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94cf8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-v1')# Concatenate train + validation + test\n",
    "# all_texts = []\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     all_texts.extend(dataset[split][\"text\"])\n",
    "\n",
    "# # Remove empty lines\n",
    "# all_texts = [t.strip() for t in all_texts if t.strip() != \"\"]\n",
    "\n",
    "# # Join into one giant string\n",
    "# big_text = \"\\n\".join(all_texts)\n",
    "\n",
    "# # Write to file\n",
    "# with open(\"wikitext_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(big_text)\n",
    "\n",
    "# print(\"Saved dataset to wikitext_full.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c759f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f51f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming you already have:\n",
    "# # - prepare_book_training_data()\n",
    "# # - token_to_id_dict\n",
    "\n",
    "# file_path = r\"/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\"\n",
    "\n",
    "# inputs_list, targets_list, masks_list = [], [], []\n",
    "\n",
    "# buffer = \"\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         buffer += line.strip() + \" \"\n",
    "#         # Process every ~5000 chars to avoid memory spike\n",
    "#         if len(buffer) > 5000:\n",
    "#             inp, tgt, msk = prepare_book_training_data(\n",
    "#                 buffer, token_to_id_dict, context_length=512\n",
    "#             )\n",
    "#             inputs_list.append(inp)\n",
    "#             targets_list.append(tgt)\n",
    "#             masks_list.append(msk)\n",
    "#             buffer = \"\"  # reset buffer\n",
    "\n",
    "# # Process any leftover buffer\n",
    "# if buffer.strip():\n",
    "#     inp, tgt, msk = prepare_book_training_data(\n",
    "#         buffer, token_to_id_dict, context_length=512\n",
    "#     )\n",
    "#     inputs_list.append(inp)\n",
    "#     targets_list.append(tgt)\n",
    "#     masks_list.append(msk)\n",
    "\n",
    "# # Concatenate all batches into final arrays\n",
    "# inputs = np.concatenate(inputs_list, axis=0)\n",
    "# targets = np.concatenate(targets_list, axis=0)\n",
    "# masks = np.concatenate(masks_list, axis=0)\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")\n",
    "# print(f\"Masks shape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b06fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "    \"\"\"\n",
    "    Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token sequence for model input\n",
    "        target_ids: Token sequence for model targets (shifted by 1)\n",
    "        attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "    Returns:\n",
    "        Serialized TFRecord example\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "        'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "        'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "def convert_text_to_tfrecord(\n",
    "    text_file_path: str,\n",
    "    token_to_id_dict: Dict[str, int],\n",
    "    output_dir: str,\n",
    "    context_length: int = 512,\n",
    "    records_per_file: int = 1000,\n",
    "    pad_value: int = 0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert text file to TFRecord files for GPT training.\n",
    "    \n",
    "    Process:\n",
    "    1. Read and tokenize entire text file\n",
    "    2. Create sliding windows of context_length + 1 tokens\n",
    "    3. Split each window into input/target pairs (shifted by 1)\n",
    "    4. Save as TFRecord files with specified number of records per file\n",
    "    \n",
    "    Args:\n",
    "        text_file_path: Path to your text file (e.g., WikiText-103)\n",
    "        token_to_id_dict: Character-to-ID mapping dictionary\n",
    "        output_dir: Directory to save TFRecord files\n",
    "        context_length: Sequence length for training\n",
    "        records_per_file: Number of examples per TFRecord file\n",
    "        pad_value: Token ID used for unknown characters\n",
    "        \n",
    "    Returns:\n",
    "        Path to output directory containing TFRecord files\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Reading text file: {text_file_path}\")\n",
    "    \n",
    "    # Step 1: Load and tokenize text\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(f\"Text length: {len(text):,} characters\")\n",
    "    \n",
    "    # Convert each character to token ID\n",
    "    print(\"Tokenizing text...\")\n",
    "    token_ids = [token_to_id_dict.get(char, pad_value) for char in text]\n",
    "    print(f\"Token length: {len(token_ids):,} tokens\")\n",
    "    \n",
    "    # Step 2: Calculate output size\n",
    "    num_examples = (len(token_ids) - context_length) // context_length\n",
    "    print(f\"Will create {num_examples:,} training examples\")\n",
    "    \n",
    "    # Step 3: Process sliding windows and write TFRecord files\n",
    "    file_count = 0\n",
    "    examples_in_current_file = 0\n",
    "    writer = None\n",
    "    \n",
    "    print(\"Creating TFRecord files...\")\n",
    "    \n",
    "    # Slide window across token sequence\n",
    "    for i in tqdm(range(0, len(token_ids) - context_length, context_length)):\n",
    "        \n",
    "        # Extract window of tokens\n",
    "        window = token_ids[i:i + context_length + 1]\n",
    "        if len(window) < context_length + 1:\n",
    "            break\n",
    "            \n",
    "        # Create input-target pair (GPT training format)\n",
    "        input_ids = window[:-1]    # First 512 tokens: [t1, t2, ..., t512]\n",
    "        target_ids = window[1:]    # Shifted by 1: [t2, t3, ..., t513]\n",
    "        attention_mask = [1] * context_length  # All valid tokens (no padding)\n",
    "        \n",
    "        # Start new TFRecord file if needed\n",
    "        if writer is None or examples_in_current_file >= records_per_file:\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "            \n",
    "            tfrecord_filename = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "            writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "            file_count += 1\n",
    "            examples_in_current_file = 0\n",
    "        \n",
    "        # Write training example to current TFRecord file\n",
    "        tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "        writer.write(tf_example)\n",
    "        examples_in_current_file += 1\n",
    "    \n",
    "    # Cleanup\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    # Step 4: Save summary information\n",
    "    print(f\"\\nConversion complete!\")\n",
    "    print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "    print(f\"Total examples: {num_examples:,}\")\n",
    "    \n",
    "    # Write metadata file\n",
    "    metadata = {\n",
    "        'context_length': context_length,\n",
    "        'vocab_size': len(token_to_id_dict),\n",
    "        'num_examples': num_examples,\n",
    "        'num_files': file_count,\n",
    "        'records_per_file': records_per_file\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def create_tf_data_pipeline(\n",
    "    tfrecord_dir: str,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    shuffle_buffer: int = 1000,\n",
    "    prefetch_buffer: int = tf.data.AUTOTUNE\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create tf.data pipeline from TFRecord files for training.\n",
    "    \n",
    "    Process:\n",
    "    1. Find all TFRecord files in directory\n",
    "    2. Create dataset that reads and parses TFRecord examples\n",
    "    3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_dir: Directory containing TFRecord files\n",
    "        context_length: Expected sequence length in records\n",
    "        batch_size: Number of examples per training batch\n",
    "        shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "        prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "    Returns:\n",
    "        tf.data.Dataset ready for model.fit()\n",
    "    \"\"\"\n",
    "    # Step 1: Find TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Step 2: Define how to parse each TFRecord example\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_tfrecord_example(example_proto):\n",
    "        \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "        # Parse the serialized example\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Convert to correct data types\n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "        model_inputs = (input_ids, attention_mask)\n",
    "        model_targets = target_ids\n",
    "        \n",
    "        return model_inputs, model_targets\n",
    "    \n",
    "    # Step 3: Create and configure dataset pipeline\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage (commented out)\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example usage for WikiText-103 or similar large text files\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "    # example_vocab = your_token_to_id_dict\n",
    "    \n",
    "    # # Step 2: Convert text to TFRecord format (run once)\n",
    "    # tfrecord_dir = convert_text_to_tfrecord(\n",
    "    #     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "    #     token_to_id_dict=example_vocab,\n",
    "    #     output_dir='./tfrecords',\n",
    "    #     context_length=512,\n",
    "    #     records_per_file=1000\n",
    "    # )\n",
    "    \n",
    "    # # Step 3: Create training pipeline (use for training)\n",
    "    # train_dataset = create_tf_data_pipeline(\n",
    "    #     tfrecord_dir=tfrecord_dir,\n",
    "    #     context_length=512,\n",
    "    #     batch_size=16\n",
    "    # )\n",
    "    \n",
    "    # print(\"TFRecord pipeline ready for training!\")\n",
    "    # # Now you can use train_dataset with your model's .fit() method\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "253d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "# example_vocab = token_to_id_dict\n",
    "\n",
    "# # Step 2: Convert text to TFRecord format (run once)\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=example_vocab,\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=128,\n",
    "#     records_per_file=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9caffa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create training and validation pipeline (use for training)\n",
    "def create_train_val_datasets(tfrecord_dir: str, \n",
    "                             context_length: int,\n",
    "                             batch_size: int = 32,\n",
    "                             val_split: float = 0.1):\n",
    "    \"\"\"\n",
    "    Create training and validation datasets from TFRecord files\n",
    "    \"\"\"\n",
    "    # Find all TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Split files for train/val\n",
    "    num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "    val_files = tfrecord_files[:num_val_files]\n",
    "    train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "    print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "    # Feature description\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_function(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "        return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "    train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "    val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a91e6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "#     \"\"\"\n",
    "#     Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "#     Args:\n",
    "#         input_ids: Token sequence for model input\n",
    "#         target_ids: Token sequence for model targets (shifted by 1)\n",
    "#         attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "#     Returns:\n",
    "#         Serialized TFRecord example\n",
    "#     \"\"\"\n",
    "#     feature = {\n",
    "#         'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "#         'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "#         'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "#     }\n",
    "    \n",
    "#     example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#     return example.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def convert_text_to_tfrecord_sp(\n",
    "#     text_file_path: str,\n",
    "#     sp_model_path: str,\n",
    "#     output_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     records_per_file: int = 1000,\n",
    "#     overlap_size: int = 64,\n",
    "#     chunk_size: int = 100_000  # Number of characters read at a time\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Streaming version of text to TFRecord conversion with SentencePiece.\n",
    "#     Reads and tokenizes file incrementally to limit memory usage.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     file_size = os.path.getsize(text_file_path)\n",
    "#     print(f\"Reading and tokenizing text in chunks from: {text_file_path}\")\n",
    "    \n",
    "#     # Load SentencePiece processor\n",
    "#     sp = spm.SentencePieceProcessor()\n",
    "#     sp.load(sp_model_path)\n",
    "    \n",
    "#     print(f\"Loaded SentencePiece model from: {sp_model_path}\")\n",
    "#     print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "\n",
    "#     buffer_tokens = []\n",
    "#     step_size = context_length - overlap_size\n",
    "#     file_count = 0\n",
    "#     examples_in_current_file = 0\n",
    "#     writer = None\n",
    "    \n",
    "#     with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "#         with tqdm(total=file_size, unit='B', unit_scale=True, desc='Processing text') as pbar:\n",
    "#             while True:\n",
    "#                 chunk = file.read(chunk_size)\n",
    "#                 if not chunk:\n",
    "#                     break\n",
    "#                 buffer_tokens.extend(sp.encode_as_ids(chunk))\n",
    "#                 pbar.update(len(chunk.encode('utf-8')))  # update by bytes read\n",
    "                \n",
    "#                 # Process windows to create examples\n",
    "#                 while len(buffer_tokens) >= context_length + 1:\n",
    "#                     window = buffer_tokens[:context_length + 1]\n",
    "#                     input_ids = window[:-1]\n",
    "#                     target_ids = window[1:]\n",
    "#                     attention_mask = [1] * context_length\n",
    "                    \n",
    "#                     if writer is None or examples_in_current_file >= records_per_file:\n",
    "#                         if writer:\n",
    "#                             writer.close()\n",
    "#                         tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#                         writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "#                         file_count += 1\n",
    "#                         examples_in_current_file = 0\n",
    "                    \n",
    "#                     tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#                     writer.write(tf_example)\n",
    "#                     examples_in_current_file += 1\n",
    "                    \n",
    "#                     # Slide window forward by step_size tokens\n",
    "#                     buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "#     # Process any remaining tokens\n",
    "#     while len(buffer_tokens) >= context_length + 1:\n",
    "#         window = buffer_tokens[:context_length + 1]\n",
    "#         input_ids = window[:-1]\n",
    "#         target_ids = window[1:]\n",
    "#         attention_mask = [1] * context_length\n",
    "        \n",
    "#         if writer is None or examples_in_current_file >= records_per_file:\n",
    "#             if writer:\n",
    "#                 writer.close()\n",
    "#             tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#             writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "#             file_count += 1\n",
    "#             examples_in_current_file = 0\n",
    "\n",
    "#         tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#         writer.write(tf_example)\n",
    "#         examples_in_current_file += 1\n",
    "#         buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "#     if writer:\n",
    "#         writer.close()\n",
    "\n",
    "#     num_examples = file_count * records_per_file  # Approximate count\n",
    "\n",
    "#     print(f\"\\nConversion complete!\")\n",
    "#     print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "#     print(f\"Approximate total examples: {num_examples}\")\n",
    "\n",
    "#     metadata = {\n",
    "#         'context_length': context_length,\n",
    "#         'vocab_size': sp.get_piece_size(),\n",
    "#         'num_examples': num_examples,\n",
    "#         'num_files': file_count,\n",
    "#         'records_per_file': records_per_file,\n",
    "#         'overlap_size': overlap_size,\n",
    "#         'sp_model_path': sp_model_path,\n",
    "#         'tokenization': 'SentencePiece'\n",
    "#     }\n",
    "\n",
    "#     metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         for key, value in metadata.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "#     print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "#     return output_dir\n",
    "\n",
    "\n",
    "\n",
    "# def create_tf_data_pipeline_sp(\n",
    "#     tfrecord_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     batch_size: int = 32,\n",
    "#     shuffle_buffer: int = 1000,\n",
    "#     prefetch_buffer: int = tf.data.AUTOTUNE\n",
    "# ) -> tf.data.Dataset:\n",
    "#     \"\"\"\n",
    "#     Create tf.data pipeline from TFRecord files for training (SentencePiece version).\n",
    "    \n",
    "#     Process:\n",
    "#     1. Find all TFRecord files in directory\n",
    "#     2. Create dataset that reads and parses TFRecord examples\n",
    "#     3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "#     Args:\n",
    "#         tfrecord_dir: Directory containing TFRecord files\n",
    "#         context_length: Expected sequence length in records\n",
    "#         batch_size: Number of examples per training batch\n",
    "#         shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "#         prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "#     Returns:\n",
    "#         tf.data.Dataset ready for model.fit()\n",
    "#     \"\"\"\n",
    "#     # Step 1: Find TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     if not tfrecord_files:\n",
    "#         raise FileNotFoundError(f\"No TFRecord files found in {tfrecord_dir}\")\n",
    "    \n",
    "#     # Step 2: Define how to parse each TFRecord example\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_tfrecord_example(example_proto):\n",
    "#         \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "#         # Parse the serialized example\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         # Convert to correct data types\n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "#         model_inputs = (input_ids, attention_mask)\n",
    "#         model_targets = target_ids\n",
    "        \n",
    "#         return model_inputs, model_targets\n",
    "    \n",
    "#     # Step 3: Create and configure dataset pipeline\n",
    "#     dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "#     dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(shuffle_buffer)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     \"\"\"\n",
    "# #     Example usage assuming you have a pre-trained SentencePiece model\n",
    "# #     \"\"\"\n",
    "    \n",
    "# #     # Your pre-trained SentencePiece model path\n",
    "# #     sp_model_path = '/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # You provide this\n",
    "    \n",
    "# #     # Step 1: Convert text to TFRecords using your pre-trained model\n",
    "# #     tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "# #         text_file_path=r'/home/akshat/GPT_from_scratch/text_data/BookCorpus3_cleaned.txt',\n",
    "# #         sp_model_path=sp_model_path,  # Your trained model\n",
    "# #         output_dir='./tfrecords',\n",
    "# #         context_length=CONTEXT_LEN,  # Match your CONTEXT_LEN\n",
    "# #         records_per_file=1000,\n",
    "# #         overlap_size = CONTEXT_LEN // 2,\n",
    "# #         chunk_size = 150000\n",
    "# #     )\n",
    "    \n",
    "# #     # # Step 2: Create training pipeline\n",
    "# #     # train_dataset = create_tf_data_pipeline_sp(\n",
    "# #     #     tfrecord_dir=tfrecord_dir,\n",
    "# #     #     context_length=128,  # Match your CONTEXT_LEN\n",
    "# #     #     batch_size=16\n",
    "# #     # )\n",
    "    \n",
    "# #     print(\"SentencePiece TFRecord pipeline ready for training!\")\n",
    "    \n",
    "# #     # Step 3: Load your SentencePiece model for vocab size and generation\n",
    "# #     sp = spm.SentencePieceProcessor()\n",
    "# #     sp.load(sp_model_path)\n",
    "# #     VOCAB_SIZE = sp.get_piece_size()\n",
    "    \n",
    "# #     print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "#     # Now you can use:\n",
    "#     # - sp for tokenization in generation\n",
    "#     # - train_dataset for model.fit()\n",
    "#     # - VOCAB_SIZE for your model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ba0fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DATASETS\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"TFRecord pipeline ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4819661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef29291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Remove existing tfrecords directory\n",
    "# if os.path.exists('./tfrecords'):\n",
    "#     shutil.rmtree('./tfrecords')\n",
    "#     print(\"Removed existing tfrecords directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9252ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SOLUTION 1: Recreate TFRecords with correct context length\n",
    "# # Delete the existing tfrecords directory and recreate with CONTEXT_LEN\n",
    "\n",
    "\n",
    "# # Recreate with correct context length\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,  # Make sure this variable is defined\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # Use your actual context length (128)\n",
    "#     records_per_file=1000\n",
    "# )\n",
    "\n",
    "# # Now create datasets with matching context length\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # This will now match\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "# # Calculate steps per epoch\n",
    "# records_per_file = 1000  \n",
    "# tfrecord_files = tf.io.gfile.glob(os.path.join(\"./tfrecords\", \"*.tfrecord\"))\n",
    "# total_examples = len(tfrecord_files) * records_per_file\n",
    "# train_examples = int(total_examples * 0.9)\n",
    "# steps_per_epoch = train_examples // 16\n",
    "\n",
    "# print(f\"Files: {len(tfrecord_files)}\")\n",
    "# print(f\"Total examples: {total_examples}\")\n",
    "# print(f\"Steps per epoch: {steps_per_epoch}\") # When combined with batch_size sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a189899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def prepare_tfrecords(\n",
    "    text_file_path: str,\n",
    "    token_to_id_dict: dict,\n",
    "    context_length: int = 128,\n",
    "    records_per_file: int = 1000,\n",
    "    output_base_dir: str = './tfrecords',\n",
    "    version_name: str = None,\n",
    "    batch_size: int = 16,\n",
    "    val_split: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create TFRecords from text and return train/val datasets.\n",
    "    Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "    Args:\n",
    "        text_file_path: Path to input text file.\n",
    "        token_to_id_dict: Character-to-id dictionary.\n",
    "        context_length: Sequence length for training.\n",
    "        records_per_file: Number of examples per TFRecord file.\n",
    "        output_base_dir: Base folder to store TFRecords.\n",
    "        version_name: Optional unique folder name. If None, uses context_length.\n",
    "        batch_size: Batch size for dataset.\n",
    "        val_split: Fraction of data to use as validation.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, steps_per_epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine output folder\n",
    "    if version_name is None:\n",
    "        version_name = f\"context_{context_length}_bs{batch_size}\"\n",
    "    output_dir = os.path.join(output_base_dir, version_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert text to TFRecords\n",
    "    tfrecord_dir = convert_text_to_tfrecord(\n",
    "        text_file_path=text_file_path,\n",
    "        token_to_id_dict=token_to_id_dict,\n",
    "        output_dir=output_dir,\n",
    "        context_length=context_length,\n",
    "        records_per_file=records_per_file\n",
    "    )\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset, val_dataset = create_train_val_datasets(\n",
    "        tfrecord_dir=tfrecord_dir,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        val_split=val_split\n",
    "    )\n",
    "    def count_tfrecord_examples(tfrecord_files):\n",
    "        count = 0\n",
    "        for tfrecord_file in tfrecord_files:\n",
    "            for _ in tf.data.TFRecordDataset(tfrecord_file):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    # inside prepare_tfrecords\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    total_examples = count_tfrecord_examples(tfrecord_files)\n",
    "    train_examples = int(total_examples * (1 - val_split))\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "    print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Train examples: {train_examples}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    return train_dataset, val_dataset, steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b03ad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset,val_dataset,steps_per_epoch = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d131044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_24, val_ds_24, steps_24 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=24\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a6f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_32, val_ds_32, steps_32 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9916a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_44, val_ds_44, steps_44 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=44\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9933902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file: /home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt\n",
      "Text length: 4,347,531 characters\n",
      "Tokenizing text...\n",
      "Token length: 4,347,531 tokens\n",
      "Will create 67,929 training examples\n",
      "Creating TFRecord files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 67930/67930 [00:01<00:00, 38916.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete!\n",
      "Created 68 TFRecord files in: ./tfrecords/context_64_bs64\n",
      "Total examples: 67,929\n",
      "Metadata saved to: ./tfrecords/context_64_bs64/metadata.txt\n",
      "Found 68 TFRecord files\n",
      "Using 62 files for training, 6 for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:24:27.780313: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:390] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-08-30 08:24:27.855721: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:27.945970: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:28.133403: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:28.509933: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:29.307072: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:30.943169: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:33.902031: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord folder: ./tfrecords/context_64_bs64\n",
      "Total examples: 67930\n",
      "Train examples: 61137\n",
      "Steps per epoch: 955\n"
     ]
    }
   ],
   "source": [
    "train_ds_64, val_ds_64, steps_64 = prepare_tfrecords(\n",
    "    text_file_path='/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt',\n",
    "    token_to_id_dict=token_to_id_dict,\n",
    "    context_length=CONTEXT_LEN,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "train_ds_64 = train_ds_64.shuffle(10000)\n",
    "val_ds_64 = val_ds_64.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e152da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER_BLOCKS = 5\n",
    "# ATTENTION_HEADS = 8\n",
    "\n",
    "# GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,0.3)\n",
    "# _ = GPT_model((token_ids, attention_mask))\n",
    "# GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# # training (stable): use logits\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0)\n",
    "# GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# # inference probs (when you actually need them)\n",
    "# logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "# probs = keras.ops.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d46c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "DECODER_BLOCKS = 6\n",
    "ATTENTION_HEADS = 4\n",
    "DROPOUT_RATE = 0.2\n",
    "STEPS_PER_EPOCH = steps_64  # e.g. 955\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH  # 95,500\n",
    "WARMUP_STEPS = int(0.1 * TOTAL_STEPS)   # 9,550\n",
    "LR = 1e-5\n",
    "\n",
    "lr_schedule = CosineDecayWithWarmup(\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    peak_learning_rate=5e-4,\n",
    "    min_learning_rate=5e-6\n",
    ")\n",
    "\n",
    "model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,DROPOUT_RATE)\n",
    "# Compile your model (same as before)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0,beta_2=0.95) # type: ignore\n",
    "model.compile(optimizer=opt, loss=loss,metrics=['accuracy']) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total epochs:  100\n",
      "Steps per epoch: 955\n",
      "Total steps: 95500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:25:27.744229: I external/local_xla/xla/service/service.cc:163] XLA service 0x15667fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-30 08:25:27.744262: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-08-30 08:25:28.264570: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-30 08:25:29.314684: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:62] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt_1_1/decoder_block_0_1/self_attention_layer_8_1/dropout/random_uniform/RandomUniform\n",
      "2025-08-30 08:25:29.571210: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:25:31.379417: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91200\n",
      "2025-08-30 08:25:34.061146: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:34.768544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.041600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38', 2744 bytes spill stores, 2660 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.598204: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.909770: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18108', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:36.769407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_17511', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.026562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13536', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.100827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12468 bytes spill stores, 12708 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.323302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.833319: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 12036 bytes spill stores, 12220 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.990262: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_124', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-08-30 08:26:02.820469: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_2059', 468 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_2055', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "I0000 00:00:1756542362.997881    1520 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m327/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:28\u001b[0m 333ms/step - accuracy: 0.2684 - loss: 5.7694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:27:53.823627: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:27:59.161874: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_123', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:27:59.346130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_112', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:27:59.978910: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18107', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:00.569359: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:00.783343: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13535', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:01.929648: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_17510', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:01.999892: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:02.007062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 12036 bytes spill stores, 12220 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:24.734816: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_2169', 468 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_2167', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.3446 - loss: 3.9893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:31:57.139796: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:32:07.230190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:32:08.048781: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:08.843210: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:09.137387: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:09.151587: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_01_val_loss_1.9395.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:16.729382: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:16.729470: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:16.729504: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n",
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 1.93948, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 402ms/step - accuracy: 0.3954 - loss: 2.6552 - val_accuracy: 0.4226 - val_loss: 1.9395\n",
      "Epoch 2/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:13\u001b[0m 334ms/step - accuracy: 0.4162 - loss: 1.9548"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:31.978328: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:31.978374: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:31.978412: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_02_val_loss_1.9318.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:37.338226: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:37.338295: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:37.338337: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.93948 to 1.93178, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23ms/step - accuracy: 0.4180 - loss: 1.9536 - val_accuracy: 0.4241 - val_loss: 1.9318\n",
      "Epoch 3/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.4311 - loss: 1.8920\n",
      "Epoch 3: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_03_val_loss_1.7152.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.93178 to 1.71517, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 349ms/step - accuracy: 0.4424 - loss: 1.8385 - val_accuracy: 0.4714 - val_loss: 1.7152\n",
      "Epoch 4/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:01\u001b[0m 321ms/step - accuracy: 0.4646 - loss: 1.7447\n",
      "Epoch 4: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_04_val_loss_1.7157.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:38:35.358899: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:38:35.358959: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:38:35.359004: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss did not improve from 1.71517\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.4664 - loss: 1.7371 - val_accuracy: 0.4723 - val_loss: 1.7157\n",
      "Epoch 5/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.4751 - loss: 1.6974\n",
      "Epoch 5: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_05_val_loss_1.5713.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.71517 to 1.57129, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 350ms/step - accuracy: 0.4846 - loss: 1.6595 - val_accuracy: 0.5097 - val_loss: 1.5713\n",
      "Epoch 6/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:01\u001b[0m 320ms/step - accuracy: 0.5036 - loss: 1.5900\n",
      "Epoch 6: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_06_val_loss_1.5715.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.57129\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5021 - loss: 1.5897 - val_accuracy: 0.5093 - val_loss: 1.5715\n",
      "Epoch 7/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.5118 - loss: 1.5577\n",
      "Epoch 7: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_07_val_loss_1.4692.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:04.541194: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:50:04.541274: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:50:04.541316: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 1.57129 to 1.46917, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 353ms/step - accuracy: 0.5190 - loss: 1.5325 - val_accuracy: 0.5377 - val_loss: 1.4692\n",
      "Epoch 8/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:02\u001b[0m 322ms/step - accuracy: 0.5359 - loss: 1.4677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:20.586527: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_08_val_loss_1.4690.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:26.246684: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:50:26.246736: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:50:26.246769: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 1.46917 to 1.46902, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23ms/step - accuracy: 0.5343 - loss: 1.4735 - val_accuracy: 0.5382 - val_loss: 1.4690\n",
      "Epoch 9/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705ms/step - accuracy: 0.5386 - loss: 1.4617\n",
      "Epoch 9: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_09_val_loss_1.3968.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.46902 to 1.39676, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 729ms/step - accuracy: 0.5437 - loss: 1.4431 - val_accuracy: 0.5567 - val_loss: 1.3968\n",
      "Epoch 10/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m13:36\u001b[0m 868ms/step - accuracy: 0.5494 - loss: 1.4061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:02:28.486692: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:02:28.486750: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:02:28.486780: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_10_val_loss_1.3992.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.39676\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 35ms/step - accuracy: 0.5519 - loss: 1.4037 - val_accuracy: 0.5579 - val_loss: 1.3992\n",
      "Epoch 11/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730ms/step - accuracy: 0.5575 - loss: 1.3914\n",
      "Epoch 11: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_11_val_loss_1.3481.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:14:38.653176: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:14:38.653247: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:14:38.653279: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 1.39676 to 1.34815, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 750ms/step - accuracy: 0.5616 - loss: 1.3772 - val_accuracy: 0.5715 - val_loss: 1.3481\n",
      "Epoch 12/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:45\u001b[0m 431ms/step - accuracy: 0.5701 - loss: 1.3446\n",
      "Epoch 12: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_12_val_loss_1.3475.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:15:02.140081: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:15:02.140180: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:15:02.140243: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 1.34815 to 1.34746, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 28ms/step - accuracy: 0.5688 - loss: 1.3474 - val_accuracy: 0.5729 - val_loss: 1.3475\n",
      "Epoch 13/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.5735 - loss: 1.3339\n",
      "Epoch 13: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_13_val_loss_1.3130.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 1.34746 to 1.31297, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 453ms/step - accuracy: 0.5756 - loss: 1.3263 - val_accuracy: 0.5818 - val_loss: 1.3130\n",
      "Epoch 14/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m7:22\u001b[0m 470ms/step - accuracy: 0.5816 - loss: 1.3076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:22:36.433668: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:22:36.433759: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:22:36.433808: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_14_val_loss_1.3156.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:22:41.908626: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:22:41.908676: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:22:41.908712: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss did not improve from 1.31297\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5819 - loss: 1.3079 - val_accuracy: 0.5809 - val_loss: 1.3156\n",
      "Epoch 15/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.5829 - loss: 1.2999\n",
      "Epoch 15: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_15_val_loss_1.2893.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:28:13.040540: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:28:13.040615: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:28:13.040655: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 1.31297 to 1.28933, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 354ms/step - accuracy: 0.5845 - loss: 1.2943 - val_accuracy: 0.5900 - val_loss: 1.2893\n",
      "Epoch 16/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:57\u001b[0m 317ms/step - accuracy: 0.5880 - loss: 1.2776\n",
      "Epoch 16: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_16_val_loss_1.2897.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.28933\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5877 - loss: 1.2800 - val_accuracy: 0.5897 - val_loss: 1.2897\n",
      "Epoch 17/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5908 - loss: 1.2720\n",
      "Epoch 17: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_17_val_loss_1.2776.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.28933 to 1.27764, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 351ms/step - accuracy: 0.5912 - loss: 1.2712 - val_accuracy: 0.5925 - val_loss: 1.2776\n",
      "Epoch 18/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:48\u001b[0m 306ms/step - accuracy: 0.5925 - loss: 1.2690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:34:26.541810: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_18_val_loss_1.2734.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:34:31.937368: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:34:31.937436: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:34:31.937479: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 1.27764 to 1.27344, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 24ms/step - accuracy: 0.5918 - loss: 1.2707 - val_accuracy: 0.5922 - val_loss: 1.2734\n",
      "Epoch 19/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.5937 - loss: 1.2615\n",
      "Epoch 19: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_19_val_loss_1.2719.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:11.416009: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:40:11.416059: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:40:11.416088: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 1.27344 to 1.27194, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 349ms/step - accuracy: 0.5938 - loss: 1.2617 - val_accuracy: 0.5939 - val_loss: 1.2719\n",
      "Epoch 20/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:46\u001b[0m 432ms/step - accuracy: 0.5905 - loss: 1.2647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:27.182574: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:40:27.182611: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:40:27.182624: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_20_val_loss_1.2742.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:32.812566: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_6]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss did not improve from 1.27194\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.5927 - loss: 1.2595 - val_accuracy: 0.5936 - val_loss: 1.2742\n",
      "Epoch 21/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5967 - loss: 1.2513\n",
      "Epoch 21: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_21_val_loss_1.2586.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:46:06.785287: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 1.27194 to 1.25856, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 355ms/step - accuracy: 0.5971 - loss: 1.2500 - val_accuracy: 0.5979 - val_loss: 1.2586\n",
      "Epoch 22/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:14\u001b[0m 334ms/step - accuracy: 0.5949 - loss: 1.2538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:46:25.549340: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:46:25.549398: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:46:25.549408: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_22_val_loss_1.2531.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 1.25856 to 1.25311, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25ms/step - accuracy: 0.5947 - loss: 1.2556 - val_accuracy: 0.5983 - val_loss: 1.2531\n",
      "Epoch 23/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6008 - loss: 1.2362\n",
      "Epoch 23: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_23_val_loss_1.2417.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:12.076908: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:52:12.076971: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:52:12.077014: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 1.25311 to 1.24175, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 351ms/step - accuracy: 0.6008 - loss: 1.2369 - val_accuracy: 0.6028 - val_loss: 1.2417\n",
      "Epoch 24/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:05\u001b[0m 324ms/step - accuracy: 0.6091 - loss: 1.2174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:27.019364: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:52:27.019429: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:52:27.019476: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_24_val_loss_1.2405.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:33.971344: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 1.24175 to 1.24047, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 27ms/step - accuracy: 0.6082 - loss: 1.2172 - val_accuracy: 0.6039 - val_loss: 1.2405\n",
      "Epoch 25/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6034 - loss: 1.2273\n",
      "Epoch 25: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_25_val_loss_1.2424.keras\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.24047\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6035 - loss: 1.2274 - val_accuracy: 0.6027 - val_loss: 1.2424\n",
      "Epoch 26/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:13\u001b[0m 334ms/step - accuracy: 0.6108 - loss: 1.2128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:58:27.301510: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:58:27.301568: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:58:27.301610: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_26_val_loss_1.2345.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:58:33.248979: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:58:33.249054: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:58:33.249096: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 1.24047 to 1.23450, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 27ms/step - accuracy: 0.6071 - loss: 1.2163 - val_accuracy: 0.6042 - val_loss: 1.2345\n",
      "Epoch 27/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6053 - loss: 1.2199\n",
      "Epoch 27: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_27_val_loss_1.2418.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.23450\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6060 - loss: 1.2190 - val_accuracy: 0.6027 - val_loss: 1.2418\n",
      "Epoch 28/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:03\u001b[0m 323ms/step - accuracy: 0.6040 - loss: 1.2317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:04:27.238359: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:04:27.238402: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:04:27.238416: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_28_val_loss_1.2368.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:04:32.950511: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:04:32.950580: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:04:32.950627: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss did not improve from 1.23450\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16ms/step - accuracy: 0.6027 - loss: 1.2304 - val_accuracy: 0.6042 - val_loss: 1.2368\n",
      "Epoch 29/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6096 - loss: 1.2070\n",
      "Epoch 29: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_29_val_loss_1.2273.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:10:06.744126: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:10:06.744220: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:10:06.744275: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss improved from 1.23450 to 1.22727, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 360ms/step - accuracy: 0.6085 - loss: 1.2107 - val_accuracy: 0.6064 - val_loss: 1.2273\n",
      "Epoch 30/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:36\u001b[0m 422ms/step - accuracy: 0.6065 - loss: 1.2136\n",
      "Epoch 30: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_30_val_loss_1.2253.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:10:35.569342: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:10:35.569447: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:10:35.569500: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 1.22727 to 1.22529, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 33ms/step - accuracy: 0.6076 - loss: 1.2113 - val_accuracy: 0.6075 - val_loss: 1.2253\n",
      "Epoch 31/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.6106 - loss: 1.2022\n",
      "Epoch 31: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_31_val_loss_1.2184.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 1.22529 to 1.21836, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 359ms/step - accuracy: 0.6103 - loss: 1.2039 - val_accuracy: 0.6107 - val_loss: 1.2184\n",
      "Epoch 32/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:47\u001b[0m 433ms/step - accuracy: 0.6154 - loss: 1.2010\n",
      "Epoch 32: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_32_val_loss_1.2210.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:16:52.296359: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:16:52.296405: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:16:52.296436: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6152 - loss: 1.1957 - val_accuracy: 0.6104 - val_loss: 1.2210\n",
      "Epoch 33/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.6114 - loss: 1.2001\n",
      "Epoch 33: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_33_val_loss_1.2269.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:30.271103: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:22:30.271166: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:22:30.271211: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6106 - loss: 1.2032 - val_accuracy: 0.6069 - val_loss: 1.2269\n",
      "Epoch 34/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:48\u001b[0m 306ms/step - accuracy: 0.6105 - loss: 1.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:40.097738: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_34_val_loss_1.2302.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:45.946858: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:22:45.946952: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:22:45.947009: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.6091 - loss: 1.2034 - val_accuracy: 0.6069 - val_loss: 1.2302\n",
      "Epoch 35/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6109 - loss: 1.2008\n",
      "Epoch 35: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_35_val_loss_1.2273.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:23.954625: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:23.954685: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:23.954725: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 346ms/step - accuracy: 0.6101 - loss: 1.2044 - val_accuracy: 0.6072 - val_loss: 1.2273\n",
      "Epoch 36/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:33\u001b[0m 418ms/step - accuracy: 0.6091 - loss: 1.2121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:35.584793: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:35.584839: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:35.584850: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_36_val_loss_1.2289.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:41.231392: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:41.231465: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:41.231511: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.6105 - loss: 1.2076 - val_accuracy: 0.6086 - val_loss: 1.2289\n",
      "Epoch 37/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6106 - loss: 1.2022\n",
      "Epoch 37: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_37_val_loss_1.2107.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:12.315030: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:12.315111: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:12.315154: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 1.21836 to 1.21068, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 357ms/step - accuracy: 0.6118 - loss: 1.1984 - val_accuracy: 0.6114 - val_loss: 1.2107\n",
      "Epoch 38/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:50\u001b[0m 309ms/step - accuracy: 0.6182 - loss: 1.1806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:32.856592: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:32.856678: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:32.856773: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_38_val_loss_1.2154.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:38.459152: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:38.459195: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:38.459227: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6164 - loss: 1.1874 - val_accuracy: 0.6116 - val_loss: 1.2154\n",
      "Epoch 39/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6149 - loss: 1.1880\n",
      "Epoch 39: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_39_val_loss_1.2148.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 347ms/step - accuracy: 0.6144 - loss: 1.1893 - val_accuracy: 0.6112 - val_loss: 1.2148\n",
      "Epoch 40/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:09\u001b[0m 329ms/step - accuracy: 0.6215 - loss: 1.1756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:40:24.788370: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:40:24.788424: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_40_val_loss_1.2156.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:40:30.418866: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:40:30.418925: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:40:30.418961: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.6181 - loss: 1.1813 - val_accuracy: 0.6114 - val_loss: 1.2156\n",
      "Epoch 41/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6157 - loss: 1.1846\n",
      "Epoch 41: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_41_val_loss_1.2101.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:46:03.312628: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:46:03.312665: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:46:03.312687: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss improved from 1.21068 to 1.21006, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 350ms/step - accuracy: 0.6153 - loss: 1.1861 - val_accuracy: 0.6123 - val_loss: 1.2101\n",
      "Epoch 42/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:57\u001b[0m 316ms/step - accuracy: 0.6204 - loss: 1.1675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:46:17.680799: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:46:17.680844: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:46:17.680854: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_42_val_loss_1.2146.keras\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.6200 - loss: 1.1721 - val_accuracy: 0.6116 - val_loss: 1.2146\n",
      "Epoch 43/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6158 - loss: 1.1837\n",
      "Epoch 43: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_43_val_loss_1.2137.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6157 - loss: 1.1854 - val_accuracy: 0.6125 - val_loss: 1.2137\n",
      "Epoch 44/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:59\u001b[0m 318ms/step - accuracy: 0.6139 - loss: 1.1921\n",
      "Epoch 44: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_44_val_loss_1.2118.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6133 - loss: 1.1915 - val_accuracy: 0.6131 - val_loss: 1.2118\n",
      "Epoch 45/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6171 - loss: 1.1792\n",
      "Epoch 45: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_45_val_loss_1.2038.keras\n",
      "\n",
      "Epoch 45: val_loss improved from 1.21006 to 1.20376, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6171 - loss: 1.1799 - val_accuracy: 0.6160 - val_loss: 1.2038\n",
      "Epoch 46/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m7:09\u001b[0m 457ms/step - accuracy: 0.6232 - loss: 1.1655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:58:07.739056: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:58:07.739111: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:58:07.739123: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_46_val_loss_1.2071.keras\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.6208 - loss: 1.1719 - val_accuracy: 0.6155 - val_loss: 1.2071\n",
      "Epoch 47/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6178 - loss: 1.1768\n",
      "Epoch 47: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_47_val_loss_1.2048.keras\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 346ms/step - accuracy: 0.6175 - loss: 1.1785 - val_accuracy: 0.6149 - val_loss: 1.2048\n",
      "Epoch 48/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m4:59\u001b[0m 318ms/step - accuracy: 0.6211 - loss: 1.1768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:03:59.646990: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:03:59.647056: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:03:59.647068: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_48_val_loss_1.2110.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:04:06.535451: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:04:06.535542: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:04:06.535590: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.6207 - loss: 1.1766 - val_accuracy: 0.6139 - val_loss: 1.2110\n",
      "Epoch 49/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6183 - loss: 1.1764\n",
      "Epoch 49: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_49_val_loss_1.2070.keras\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 347ms/step - accuracy: 0.6189 - loss: 1.1740 - val_accuracy: 0.6142 - val_loss: 1.2070\n",
      "Epoch 50/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m5:03\u001b[0m 323ms/step - accuracy: 0.6200 - loss: 1.1736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:09:50.284128: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:09:50.284182: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:09:50.284217: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_50_val_loss_1.2032.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:09:55.896736: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:09:55.896812: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:09:55.896860: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: val_loss improved from 1.20376 to 1.20318, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.6217 - loss: 1.1707 - val_accuracy: 0.6152 - val_loss: 1.2032\n",
      "Epoch 51/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6186 - loss: 1.1746\n",
      "Epoch 51: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_51_val_loss_1.1951.keras\n",
      "\n",
      "Epoch 51: val_loss improved from 1.20318 to 1.19508, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 360ms/step - accuracy: 0.6196 - loss: 1.1713 - val_accuracy: 0.6186 - val_loss: 1.1951\n",
      "Epoch 52/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m\u001b[0m \u001b[1m6:58\u001b[0m 445ms/step - accuracy: 0.6215 - loss: 1.1607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:16:18.098477: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_52_val_loss_1.1959.keras\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    # Save model every epoch with epoch number\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.keras',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model separately\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # More reasonable early stopping\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,  # Wait 10 epochs before stopping\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.CSVLogger('training_log.csv'),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=\"./word_logs\", \n",
    "        histogram_freq=1, \n",
    "        profile_batch=0,\n",
    "        write_graph=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total epochs: \",EPOCHS)\n",
    "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"Total steps: {EPOCHS * STEPS_PER_EPOCH}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_64,\n",
    "    validation_data=val_ds_64,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "if 'learning_rate' in history.history:\n",
    "    plt.plot(history.history['learning_rate'])\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "train_perplexity = [np.exp(loss) for loss in history.history['loss']]\n",
    "val_perplexity = [np.exp(loss) for loss in history.history['val_loss']]\n",
    "plt.plot(train_perplexity, label='Train Perplexity')\n",
    "plt.plot(val_perplexity, label='Val Perplexity')\n",
    "plt.title('Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c498d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now training should work\n",
    "# history = model.fit(\n",
    "#     train_ds_32,\n",
    "#     validation_data=val_ds_32,\n",
    "#     epochs=50,\n",
    "#     steps_per_epoch=steps_32,\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint(filepath='model_epoch_{epoch:02d}.keras',save_freq='epoch'),  # saves with epoch number   save_freq='epoch',                        # save every epoch    save_best_only=False,                     # save all epochs    verbose=1)\n",
    "#         keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, verbose=1),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, verbose=1),\n",
    "#         keras.callbacks.CSVLogger('training_log.csv'),\n",
    "#         keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1, profile_batch=0)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e45dbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Positional embeddings are working. Shape: (1, 128, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "# pick a small dummy batch\n",
    "import tensorflow as tf\n",
    "\n",
    "dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)  # batch_size=1, length=CONTEXT_LEN\n",
    "dum_model.save('model_epoch_1.keras')\n",
    "# run the embeddings layer only\n",
    "pos_layer = dum_model.get_layer('init_embeddings')  # or however your layer is named\n",
    "try:\n",
    "    pos_emb = pos_layer(dummy_input)\n",
    "    print(\" Positional embeddings are working. Shape:\", pos_emb.shape)\n",
    "except Exception as e:\n",
    "    print(\" Embedding test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0b33b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'final_gpt_model.keras'\n",
      "Training history saved as 'training_history.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the final model\n",
    "model.save('final_gpt_model.keras')\n",
    "print(\"Model saved as 'final_gpt_model.keras'\")\n",
    "\n",
    "# Optional: Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved as 'training_history.pkl'\")\n",
    "\n",
    "# Step 5: Load model later (when needed)\n",
    "def load_trained_model():\n",
    "    \"\"\"Load your saved model\"\"\"\n",
    "    loaded_model = keras.models.load_model('best_model.keras')  # or 'final_gpt_model.keras'\n",
    "    return loaded_model\n",
    "\n",
    "# Usage for inference later:\n",
    "# model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "87a42df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_model.save('dum_GPT.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "306ce64e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Loaded best_model.keras\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras. Please ensure the file is an accessible `.keras` zip file.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Loaded epoch 163 model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras. Please ensure the file is an accessible `.keras` zip file.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Loaded epoch 163 model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Loaded epoch 161 model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m CONTEXT_LEN = model._context_length  \u001b[38;5;66;03m# Use the model's actual context length\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format.load_model_from_hdf5(\n\u001b[32m    197\u001b[39m         filepath, custom_objects=custom_objects, \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m\n\u001b[32m    198\u001b[39m     )\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    207\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmight have a different name).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    218\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# token_to_id_dict = tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'])\n",
    "id_to_token_dict = {id_val: token for token, id_val in token_to_id_dict.items()}\n",
    "\n",
    "# Use the latest and best model - try the best_model.keras first, then latest checkpoint\n",
    "try:\n",
    "    model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras')\n",
    "    print(\" Loaded best_model.keras\")\n",
    "except:\n",
    "    try:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras')\n",
    "        print(\" Loaded epoch 163 model\")\n",
    "    except:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras')\n",
    "        print(\" Loaded epoch 161 model\")\n",
    "\n",
    "CONTEXT_LEN = model._context_length  # Use the model's actual context length\n",
    "\n",
    "# Debug: Print vocabulary info\n",
    "print(f\"Vocabulary size: {len(token_to_id_dict)}\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Get model info from your custom GPT model\n",
    "try:\n",
    "    print(f\"Model vocab size: {model._vocab_size}\")\n",
    "    print(f\"Model context length: {model._context_length}\")\n",
    "    print(f\"Model d_model: {model._d_model}\")\n",
    "    print(f\"Model attention heads: {model._attention_heads}\")\n",
    "    print(f\"Model decoder blocks: {model._decoder_blocks}\")\n",
    "    print(f\"Vocab size matches model: {model._vocab_size == len(token_to_id_dict)}\")\n",
    "    \n",
    "    if model._vocab_size != len(token_to_id_dict):\n",
    "        print(f\"  VOCAB SIZE MISMATCH! Model expects {model._vocab_size}, got {len(token_to_id_dict)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting model info: {e}\")\n",
    "\n",
    "print(f\"Sample characters in vocab: {list(token_to_id_dict.keys())[:30]}\")\n",
    "print(f\"Common characters present: {['a' in token_to_id_dict, 'e' in token_to_id_dict, ' ' in token_to_id_dict, '.' in token_to_id_dict]}\")\n",
    "\n",
    "# Check for problematic characters in the gibberish output\n",
    "gibberish = \"4ff.mtm 64m86rfstmfm?.fmmftms777mtmkf  tm7n7m77m77\"\n",
    "print(f\"Checking gibberish characters:\")\n",
    "for char in set(gibberish):\n",
    "    if char in token_to_id_dict:\n",
    "        print(f\"  '{char}' -> ID {token_to_id_dict[char]} \")\n",
    "    else:\n",
    "        print(f\"  '{char}' -> NOT IN VOCAB \")\n",
    "\n",
    "def encode_text(text, token_to_id_dict):\n",
    "    \"\"\"Encode text to token IDs using character-level tokenizer - convert to lowercase since dataset is lowercase\"\"\"\n",
    "    # Convert input to lowercase since your dataset was lowercased\n",
    "    text = text.lower()\n",
    "    \n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "        if char in token_to_id_dict:\n",
    "            token_ids.append(token_to_id_dict[char])\n",
    "        else:\n",
    "            print(f\"Warning: '{char}' (ord: {ord(char)}) not in vocabulary, skipping\")\n",
    "            continue\n",
    "    return token_ids\n",
    "\n",
    "def decode_ids(token_ids, id_to_token_dict):\n",
    "    \"\"\"Decode token IDs back to text using character-level tokenizer\"\"\"\n",
    "    text = \"\"\n",
    "    for token_id in token_ids:\n",
    "        if token_id in id_to_token_dict:\n",
    "            text += id_to_token_dict[token_id]\n",
    "        else:\n",
    "            print(f\"Warning: token ID {token_id} not in vocabulary\")\n",
    "    return text\n",
    "\n",
    "def get_special_token_ids():\n",
    "    \"\"\"Get special token IDs - adjust these based on your tokenizer setup\"\"\"\n",
    "    # For Jane Austen data, likely no special PAD token, use newline as EOS\n",
    "    pad_id = token_to_id_dict.get('<PAD>', None)\n",
    "    eos_id = token_to_id_dict.get('\\n', None)  # Use newline as natural stopping point\n",
    "    print(f\"Special tokens - PAD: {pad_id}, EOS (newline): {eos_id}\")\n",
    "    return pad_id, eos_id\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    # Ensure we don't sample more than available tokens\n",
    "    k = min(k, len(logits))\n",
    "    \n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    \n",
    "    # Add small epsilon to avoid numerical issues\n",
    "    probs = probs + 1e-10\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_k=10, use_argmax=False):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"\\n--- Generation Debug ---\")\n",
    "    print(f\"Input prompt: '{prompt}' (will be lowercased)\")\n",
    "    \n",
    "    # Tokenize prompt with character-level tokenizer\n",
    "    input_tokens = encode_text(prompt, token_to_id_dict)\n",
    "    print(f\"Input tokens: {input_tokens}\")\n",
    "    print(f\"Input tokens decoded back: '{decode_ids(input_tokens, id_to_token_dict)}'\")\n",
    "    \n",
    "    if not input_tokens:\n",
    "        return \"Error: Could not tokenize input\"\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    pad_id, eos_id = get_special_token_ids()\n",
    "    \n",
    "    print(f\"Starting generation with {len(input_tokens)} input tokens...\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Prepare inputs - pad from left to maintain most recent context\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        \n",
    "        # Place tokens at the end of the context window\n",
    "        current_len = min(len(generated_tokens), CONTEXT_LEN)\n",
    "        start_idx = CONTEXT_LEN - current_len\n",
    "        input_ids[0, start_idx:] = generated_tokens[-current_len:]\n",
    "        attention_mask[0, start_idx:] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        try:\n",
    "            logits = model((input_ids, attention_mask), training=False)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if not use_argmax:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "        except Exception as e:\n",
    "            print(f\"Model forward pass error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Sample next token\n",
    "        try:\n",
    "            if use_argmax:\n",
    "                # Use argmax (greedy) sampling for testing\n",
    "                next_token = int(np.argmax(next_token_logits))\n",
    "            else:\n",
    "                # Use top-k sampling\n",
    "                next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Debug: Print first few tokens\n",
    "        if step < 10:\n",
    "            sampled_char = id_to_token_dict.get(next_token, f\"<UNK:{next_token}>\")\n",
    "            prob = float(tf.nn.softmax(next_token_logits)[next_token])\n",
    "            print(f\"Step {step}: Token {next_token} -> '{sampled_char}' (prob: {prob:.4f})\")\n",
    "        \n",
    "        # Check if token is valid\n",
    "        if next_token >= len(id_to_token_dict):\n",
    "            print(f\"Warning: Invalid token {next_token}, vocab size is {len(id_to_token_dict)}\")\n",
    "            break\n",
    "        \n",
    "        # Stop on special tokens\n",
    "        if pad_id is not None and next_token == pad_id:\n",
    "            print(f\"Stopping at step {step}: hit PAD token\")\n",
    "            break\n",
    "        if eos_id is not None and next_token == eos_id and step > 10:  # Don't stop too early\n",
    "            print(f\"Stopping at step {step}: hit EOS token (newline)\")\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        # Maintain sliding window\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = decode_ids(new_tokens, id_to_token_dict)\n",
    "    print(f\"Generated {len(new_tokens)} new tokens: {new_tokens[:20]}...\")  # Show first 20\n",
    "    print(f\"Generated response: '{response}'\")\n",
    "    print(f\"--- End Debug ---\\n\")\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k, use_argmax):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k, use_argmax=use_argmax)\n",
    "    history.append((message, bot_response))\n",
    "    return \"\", history\n",
    "\n",
    "# Quick test with the better model\n",
    "print(\"Testing improved model:\")\n",
    "test_cases = [\"the\", \"elizabeth\", \"it is a\"]\n",
    "\n",
    "for prompt in test_cases:\n",
    "    print(f\"\\nTesting with: '{prompt}'\")\n",
    "    tokens = encode_text(prompt, token_to_id_dict)\n",
    "    \n",
    "    # Create model input\n",
    "    input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "    attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "    input_ids[0, -len(tokens):] = tokens\n",
    "    attention_mask[0, -len(tokens):] = 1\n",
    "    \n",
    "    # Get model predictions\n",
    "    logits = model((input_ids, attention_mask), training=False)\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for i in range(5):\n",
    "        token_id = int(top_indices[i])\n",
    "        prob = float(top_probs[i])\n",
    "        char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "        print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "# Test with longer context\n",
    "print(f\"\\nTesting with longer Jane Austen context:\")\n",
    "long_prompt = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a\"\n",
    "tokens = encode_text(long_prompt, token_to_id_dict)\n",
    "print(f\"Context: '{long_prompt}'\")\n",
    "print(f\"Context length: {len(tokens)} tokens\")\n",
    "\n",
    "# Use reasonable context length\n",
    "context_tokens = tokens[-100:] if len(tokens) > 100 else tokens\n",
    "\n",
    "input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "input_ids[0, -len(context_tokens):] = context_tokens\n",
    "attention_mask[0, -len(context_tokens):] = 1\n",
    "\n",
    "logits = model((input_ids, attention_mask), training=False)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "print(\"Top 5 predictions after long context:\")\n",
    "for i in range(5):\n",
    "    token_id = int(top_indices[i])\n",
    "    prob = float(top_probs[i])\n",
    "    char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "    print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "with gr.Blocks(title=\"My Character-Level GPT Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"#  Chat with Akshat's Character-Level GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with character-level tokenization on Jane Austen data. Be gentle with me :)\")\n",
    "    \n",
    "    # Add vocab info\n",
    "    gr.Markdown(f\"**Model Info:** Vocabulary size: {len(token_to_id_dict)} characters\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.3, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=10, maximum=200, value=30, step=10, label=\"Max Length\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=5, step=1, label=\"Top-K Sampling\")\n",
    "        use_argmax = gr.Checkbox(label=\"Use Argmax (Greedy) - for testing\", value=True)\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6019,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468aeb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:  VxxZEtRh5HHeYXZtxnhx5ICxxetRx5(ZxZMqx55xejhPxYW4Rx\n",
      "GPT:  RcT2-LeJPeRHVpMKhzo37xBoxti\n",
      "GPT:  qt-45HxGefn5ZZx48.UxMeTRuOLzRHxt\n",
      "GPT:  RxHxtX5RmeMxHd2tZ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[277]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Simple console loop\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     prompt = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prompt.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=1.0):\n",
    "    input_tokens = [token_to_id_dict.get(c, 0) for c in prompt if c in token_to_id_dict]\n",
    "    if len(input_tokens) == 0:\n",
    "        input_tokens = [0]\n",
    "\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "\n",
    "    input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    input_ids[0, -len(input_tokens):] = input_tokens\n",
    "\n",
    "    attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    attention_mask[0, -len(input_tokens):] = 1\n",
    "\n",
    "    generated_tokens = input_tokens.copy()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        next_token_logits = predictions[0, -1, :] / temperature\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        if next_token == 0:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "\n",
    "        input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        attention_mask[0, -len(generated_tokens):] = 1\n",
    "\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    return ''.join([id_to_token.get(t, '') for t in generated_tokens[len(input_tokens):]]).strip()\n",
    "\n",
    "\n",
    "# Simple console loop\n",
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    response = generate_text(prompt, max_length=50, temperature=0.8)\n",
    "    print(\"GPT: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb3680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7fccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d11a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sinusoidal_lookup_table.shape)  # should be (CONTEXT_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67921403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting comprehensive testing for YOUR custom model implementation...\n",
      "\n",
      " Testing sinusoidal lookup table creation...\n",
      " Sinusoidal lookup table created successfully. Shape: (128, 128)\n",
      "   Table type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "   Table dtype: <dtype: 'float32'>\n",
      "\n",
      " Testing InitializePositionalEmbeddings layer...\n",
      " Positional embeddings test failed: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      " Testing LayerNormalization layer...\n",
      " LayerNormalization working. Input shape: (2, 128, 128), Output shape: (2, 128, 128)\n",
      "   Output mean (should be ~0): 0.000000\n",
      "   Output variance (should be ~1): 0.999990\n",
      "\n",
      " Testing YOUR SelfAttentionLayer...\n",
      "   Input embeddings shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "   Mask sample - seq 1: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " SelfAttentionLayer working. Output shape: (2, 128, 128)\n",
      "   Output range: [-0.7532, 0.7810]\n",
      "   Output mean: 0.0010\n",
      "   Output std: 0.0782\n",
      "   Attention heads: 8\n",
      "   d_head: 16\n",
      "   d_model: 128\n",
      "\n",
      " Testing YOUR DecoderBlock...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 29, in test_positional_embeddings\n",
      "    pos_layer = InitializePositionalEmbeddings(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_206177/3965528913.py\", line 11, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Input shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      " DecoderBlock working (training=False). Output shape: (2, 128, 128)\n",
      " DecoderBlock working (training=True). Output shape: (2, 128, 128)\n",
      "   Input mean: 0.0003, Output mean: -0.0041\n",
      "   Output range: [-4.3376, 4.7553]\n",
      "\n",
      " Testing YOUR complete GPT model...\n",
      " Full model test failed: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      " Testing the specific embedding issue from your original code...\n",
      " Original embedding test still fails: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n",
      "\n",
      "======================================================================\n",
      " TESTING SUMMARY FOR YOUR CUSTOM MODEL:\n",
      "======================================================================\n",
      "Sinusoidal Lookup Table...........................  PASSED\n",
      "Positional Embeddings.............................  FAILED\n",
      "Layer Normalization...............................  PASSED\n",
      "YOUR Self Attention Layer.........................  PASSED\n",
      "YOUR Decoder Block................................  PASSED\n",
      "YOUR Full Model Forward Pass......................  FAILED\n",
      "YOUR Original Embedding Issue.....................  FAILED\n",
      "Model Compilation & Training......................  FAILED\n",
      "\n",
      " Overall: 4/8 tests passed\n",
      "  Some tests failed. Please fix the issues before training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 186, in test_your_full_model\n",
      "    model = GPT(\n",
      "            ^^^^\n",
      "  File \"/tmp/ipykernel_206177/1791215164.py\", line 64, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/models/model.py\", line 158, in __init__\n",
      "    Layer.__init__(self, *args, **kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 255, in test_specific_embedding_issue\n",
      "    dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "CONTEXT_LEN = 128\n",
    "D_MODEL = 128\n",
    "VOCAB_SIZE = 94\n",
    "\n",
    "def test_sinusoidal_lookup_table():\n",
    "    \"\"\"Test the sinusoidal lookup table creation\"\"\"\n",
    "    print(\" Testing sinusoidal lookup table creation...\")\n",
    "    try:\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        print(f\" Sinusoidal lookup table created successfully. Shape: {sinusoidal_lookup_table.shape}\")\n",
    "        print(f\"   Table type: {type(sinusoidal_lookup_table)}\")\n",
    "        print(f\"   Table dtype: {sinusoidal_lookup_table.dtype}\")\n",
    "        return sinusoidal_lookup_table\n",
    "    except Exception as e:\n",
    "        print(f\" Sinusoidal lookup table creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_positional_embeddings(sinusoidal_lookup_table):\n",
    "    \"\"\"Test the positional embeddings layer\"\"\"\n",
    "    print(\"\\n Testing InitializePositionalEmbeddings layer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create the layer\n",
    "        pos_layer = InitializePositionalEmbeddings(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_pos_embeddings\"\n",
    "        )\n",
    "        \n",
    "        # Test input\n",
    "        dummy_input = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\" Positional embeddings working. Output shape: {pos_emb.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {D_MODEL})\")\n",
    "        \n",
    "        # Check if embeddings are reasonable\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(pos_emb):.4f}\")\n",
    "        \n",
    "        return pos_layer, pos_emb\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Positional embeddings test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_layer_normalization():\n",
    "    \"\"\"Test LayerNormalization layer\"\"\"\n",
    "    print(\"\\n Testing LayerNormalization layer...\")\n",
    "    \n",
    "    try:\n",
    "        ln = LayerNormalization(eps=1e-5, name=\"test_ln\")\n",
    "        test_input = tf.random.normal((2, CONTEXT_LEN, D_MODEL))\n",
    "        \n",
    "        output = ln(test_input)\n",
    "        print(f\" LayerNormalization working. Input shape: {test_input.shape}, Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check normalization properties\n",
    "        mean = tf.reduce_mean(output, axis=-1)\n",
    "        var = tf.reduce_mean(tf.square(output - tf.expand_dims(mean, -1)), axis=-1)\n",
    "        print(f\"   Output mean (should be ~0): {tf.reduce_mean(mean):.6f}\")\n",
    "        print(f\"   Output variance (should be ~1): {tf.reduce_mean(var):.6f}\")\n",
    "        \n",
    "        return ln, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" LayerNormalization test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_self_attention():\n",
    "    \"\"\"Test YOUR custom SelfAttentionLayer\"\"\"\n",
    "    print(\"\\n Testing YOUR SelfAttentionLayer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your attention layer with correct parameter name\n",
    "        attn_layer = SelfAttentionLayer(attention_heads=8, name=\"test_attention\")\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        embeddings = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        # Make some positions masked (set to 0)\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 50:] = 0  # Mask second half of first sequence\n",
    "        attention_mask[1, 80:] = 0  # Mask last part of second sequence\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample - seq 1: {attention_mask[0, :10].numpy()} ... {attention_mask[0, -10:].numpy()}\")\n",
    "        \n",
    "        # Test with your layer's expected input format: (embeddings, mask)\n",
    "        output = attn_layer([embeddings, attention_mask])\n",
    "        print(f\" SelfAttentionLayer working. Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(output):.4f}\")\n",
    "        print(f\"   Output std: {tf.math.reduce_std(output):.4f}\")\n",
    "        \n",
    "        # Verify attention heads are working\n",
    "        print(f\"   Attention heads: {attn_layer.attention_heads}\")\n",
    "        print(f\"   d_head: {attn_layer.d_head}\")\n",
    "        print(f\"   d_model: {attn_layer.d_model}\")\n",
    "        \n",
    "        return attn_layer, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" SelfAttentionLayer test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_decoder_block():\n",
    "    \"\"\"Test YOUR custom DecoderBlock\"\"\"\n",
    "    print(\"\\n Testing YOUR DecoderBlock...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your decoder block\n",
    "        decoder = DecoderBlock(\n",
    "            d_model=D_MODEL,\n",
    "            n_heads=8,\n",
    "            dropout_rate=0.1,\n",
    "            epsilon=1e-5,\n",
    "            name=\"test_decoder\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        test_input = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        \n",
    "        # Make some positions masked\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 60:] = 0\n",
    "        attention_mask[1, 90:] = 0\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input shape: {test_input.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Test training=False\n",
    "        output = decoder(test_input, attention_mask, training=False)\n",
    "        print(f\" DecoderBlock working (training=False). Output shape: {output.shape}\")\n",
    "        \n",
    "        # Test training=True\n",
    "        output_train = decoder(test_input, attention_mask, training=True)\n",
    "        print(f\" DecoderBlock working (training=True). Output shape: {output_train.shape}\")\n",
    "        \n",
    "        # Check residual connections work (output should be different from input)\n",
    "        input_mean = tf.reduce_mean(test_input)\n",
    "        output_mean = tf.reduce_mean(output)\n",
    "        print(f\"   Input mean: {input_mean:.4f}, Output mean: {output_mean:.4f}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        \n",
    "        return decoder, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" DecoderBlock test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_full_model(sinusoidal_lookup_table):\n",
    "    \"\"\"Test YOUR complete GPT model\"\"\"\n",
    "    print(\"\\n Testing YOUR complete GPT model...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you do\n",
    "        model = GPT(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            context_length=CONTEXT_LEN,\n",
    "            attention_heads=8,\n",
    "            epsilon=1e-5,\n",
    "            decoder_blocks=4,\n",
    "            dropout_rate=0.1,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_gpt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        token_ids = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        attention_mask = tf.ones((1, CONTEXT_LEN), dtype=tf.float32)\n",
    "        \n",
    "        # Set mask to 0 for padding tokens\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 5:] = 0  # Only first 5 tokens are real\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Token IDs shape: {token_ids.shape}\")\n",
    "        print(f\"   Token IDs sample: {token_ids[0, :10].numpy()}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample: {attention_mask[0, :10].numpy()}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        logits = model([token_ids, attention_mask], training=False)\n",
    "        print(f\" Full model forward pass successful!\")\n",
    "        print(f\"   Output logits shape: {logits.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {VOCAB_SIZE})\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Logits range: [{tf.reduce_min(logits):.4f}, {tf.reduce_max(logits):.4f}]\")\n",
    "        print(f\"   Logits mean: {tf.reduce_mean(logits):.4f}\")\n",
    "        \n",
    "        # Test with different batch size\n",
    "        token_ids_batch = tf.constant([\n",
    "            [1, 2, 3] + [0] * (CONTEXT_LEN - 3),\n",
    "            [4, 5, 6, 7] + [0] * (CONTEXT_LEN - 4)\n",
    "        ], dtype=tf.int32)\n",
    "        attention_mask_batch = tf.ones((2, CONTEXT_LEN), dtype=tf.float32)\n",
    "        attention_mask_batch = attention_mask_batch.numpy()\n",
    "        attention_mask_batch[0, 3:] = 0  # First seq has 3 real tokens\n",
    "        attention_mask_batch[1, 4:] = 0  # Second seq has 4 real tokens\n",
    "        attention_mask_batch = tf.constant(attention_mask_batch)\n",
    "        \n",
    "        logits_batch = model([token_ids_batch, attention_mask_batch], training=False)\n",
    "        print(f\" Batch processing successful! Output shape: {logits_batch.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        logits_train = model([token_ids, attention_mask], training=True)\n",
    "        print(f\" Training mode successful! Output shape: {logits_train.shape}\")\n",
    "        \n",
    "        return model, logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Full model test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_specific_embedding_issue():\n",
    "    \"\"\"Test the specific embedding issue you encountered\"\"\"\n",
    "    print(\"\\n Testing the specific embedding issue from your original code...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you did\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
    "        \n",
    "        # Test exactly as you did\n",
    "        dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)\n",
    "        \n",
    "        # Get the embeddings layer\n",
    "        pos_layer = dum_model.get_layer('init_embeddings')\n",
    "        \n",
    "        # Run the embeddings layer\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\" Your original embedding test now works! Shape: {pos_emb.shape}\")\n",
    "        print(f\"   Input was all zeros: {dummy_input[0, :5].numpy()}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Original embedding test still fails: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_model_compilation_and_training(model):\n",
    "    \"\"\"Test model compilation and training capability\"\"\"\n",
    "    print(\"\\n Testing model compilation and training...\")\n",
    "    \n",
    "    try:\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.AdamW(learning_rate=1e-4),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\" Model compilation successful!\")\n",
    "        \n",
    "        # Create dummy training data\n",
    "        batch_size = 4\n",
    "        dummy_x = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        dummy_mask = tf.ones((batch_size, CONTEXT_LEN), dtype=tf.float32)\n",
    "        # Create some realistic masking\n",
    "        for i in range(batch_size):\n",
    "            seq_len = tf.random.uniform([], minval=10, maxval=CONTEXT_LEN, dtype=tf.int32)\n",
    "            dummy_mask = dummy_mask.numpy()\n",
    "            dummy_mask[i, seq_len:] = 0\n",
    "            dummy_mask = tf.constant(dummy_mask)\n",
    "        \n",
    "        dummy_y = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        \n",
    "        print(f\"   Training data shapes: X={dummy_x.shape}, mask={dummy_mask.shape}, Y={dummy_y.shape}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        predictions = model.predict([dummy_x, dummy_mask], verbose=0)\n",
    "        print(f\" Model prediction successful! Predictions shape: {predictions.shape}\")\n",
    "        \n",
    "        # Test training step\n",
    "        loss = model.train_on_batch([dummy_x, dummy_mask], dummy_y)\n",
    "        print(f\" Training step successful! Loss: {loss}\")\n",
    "        \n",
    "        # Test model summary\n",
    "        print(f\"\\n Model has {model.count_params():,} parameters\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Model compilation/training test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def run_all_tests_for_your_model():\n",
    "    \"\"\"Run all tests specifically for your model implementation\"\"\"\n",
    "    print(\" Starting comprehensive testing for YOUR custom model implementation...\\n\")\n",
    "    \n",
    "    # Test 1: Sinusoidal lookup table\n",
    "    sinusoidal_lookup_table = test_sinusoidal_lookup_table()\n",
    "    if sinusoidal_lookup_table is None:\n",
    "        print(\" Cannot proceed without sinusoidal lookup table\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Positional embeddings\n",
    "    pos_layer, pos_emb = test_positional_embeddings(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 3: Layer normalization\n",
    "    ln_layer, ln_output = test_layer_normalization()\n",
    "    \n",
    "    # Test 4: Your self attention\n",
    "    attn_layer, attn_output = test_your_self_attention()\n",
    "    \n",
    "    # Test 5: Your decoder block\n",
    "    decoder_layer, decoder_output = test_your_decoder_block()\n",
    "    \n",
    "    # Test 6: Your full model\n",
    "    model, logits = test_your_full_model(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 7: Your specific embedding issue\n",
    "    embedding_issue_fixed = test_specific_embedding_issue()\n",
    "    \n",
    "    # Test 8: Model compilation and training\n",
    "    compilation_success = False\n",
    "    if model is not None:\n",
    "        compilation_success = test_model_compilation_and_training(model)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" TESTING SUMMARY FOR YOUR CUSTOM MODEL:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Sinusoidal Lookup Table\", sinusoidal_lookup_table is not None),\n",
    "        (\"Positional Embeddings\", pos_emb is not None),\n",
    "        (\"Layer Normalization\", ln_output is not None),\n",
    "        (\"YOUR Self Attention Layer\", attn_output is not None),\n",
    "        (\"YOUR Decoder Block\", decoder_output is not None),\n",
    "        (\"YOUR Full Model Forward Pass\", logits is not None),\n",
    "        (\"YOUR Original Embedding Issue\", embedding_issue_fixed),\n",
    "        (\"Model Compilation & Training\", compilation_success)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, result in tests if result)\n",
    "    total = len(tests)\n",
    "    \n",
    "    for test_name, result in tests:\n",
    "        status = \" PASSED\" if result else \" FAILED\"\n",
    "        print(f\"{test_name:.<50} {status}\")\n",
    "    \n",
    "    print(f\"\\n Overall: {passed}/{total} tests passed\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\" All tests passed! Your model is working perfectly!\")\n",
    "        print(\" Your model is ready for training and inference!\")\n",
    "    elif passed >= total - 2:\n",
    "        print(\" Almost all tests passed! Your model is mostly working correctly!\")\n",
    "        print(\" Check the failed tests above for minor issues.\")\n",
    "    else:\n",
    "        print(\"  Some tests failed. Please fix the issues before training.\")\n",
    "    \n",
    "    return model if logits is not None else None\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    final_model = run_all_tests_for_your_model()\n",
    "    \n",
    "    if final_model is not None:\n",
    "        print(f\"\\n Model returned successfully!\")\n",
    "        print(f\"   Total parameters: {final_model.count_params():,}\")\n",
    "        print(f\"   Ready for: training, inference, and saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_bot(model, token_to_id_dict, prompt, max_length=100, temperature=0.7, context_len=128):\n",
    "    \"\"\"Generate text response from your GPT model\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_tokens = [token_to_id_dict.get(char, 0) for char in prompt]\n",
    "    \n",
    "    # Handle context length\n",
    "    if len(input_tokens) > context_len:\n",
    "        input_tokens = input_tokens[-context_len:]\n",
    "    \n",
    "    # Pad to context length\n",
    "    input_ids = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        input_ids[-len(input_tokens):] = input_tokens\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        attention_mask[-len(input_tokens):] = 1\n",
    "    \n",
    "    # Prepare for model\n",
    "    input_ids = np.expand_dims(input_ids, axis=0)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    \n",
    "    # Generate response token by token\n",
    "    generated_tokens = input_tokens.copy()\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        \n",
    "        # Get last token logits (find the last non-zero position in attention mask)\n",
    "        last_pos = np.sum(attention_mask[0]) - 1\n",
    "        if last_pos < 0:\n",
    "            last_pos = 0\n",
    "        next_token_logits = predictions[0, last_pos, :] / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "        \n",
    "        # Stop if we hit a stop token or newline\n",
    "        if next_token == 0 or (next_token in token_to_id_dict.values() and id_to_token[next_token] == '\\n'):\n",
    "            break\n",
    "            \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Update input for next iteration\n",
    "        if len(generated_tokens) > context_len:\n",
    "            generated_tokens = generated_tokens[-context_len:]\n",
    "        \n",
    "        # Create new input\n",
    "        new_input_ids = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        \n",
    "        new_attention_mask = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        input_ids = new_input_ids\n",
    "        attention_mask = new_attention_mask\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    response_tokens = generated_tokens[len(input_tokens):]  # Only the new tokens\n",
    "    response = ''.join([id_to_token.get(token, '') for token in response_tokens])\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b200e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"]8'Vi3A;#sFXxr3][xM6!dlwx$pb:Orxx1JkW0:pyy;94!GHQG:::$fwrg3Rg!R!/gxrgg/PJIYPl6J%6RpLp\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_from_bot(model,token_to_id_dict,prompt = 'yoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
