{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62adc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 05:31:06.917820: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-14 05:31:07.557079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-14 05:31:09.828724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007f70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str] = [r\"/home/akshat/GPT_from_scratch/text_data/pg76702.txt\"],\n",
    "#     existing_vocab: Dict[str, int] = None # type: ignore\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TensorFlow lookup tables.\n",
    "    \n",
    "#     Returns:\n",
    "#         token_to_id_table: tf.lookup.StaticHashTable mapping char -> int\n",
    "#         id_to_token_table: tf.lookup.StaticHashTable mapping int -> char\n",
    "#     \"\"\"\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "\n",
    "#     # Collect characters from all files\n",
    "#     for file_name in file_path_list:\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     # Sort for consistency\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "\n",
    "#     # Assign IDs (keep existing IDs if possible)\n",
    "#     token_to_id = {token: i for i, token in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "#     # Convert dicts to tensors\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()))\n",
    "#     token_values = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_values = tf.constant(list(id_to_token.values()))\n",
    "\n",
    "#     # Create TensorFlow lookup tables\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_values),\n",
    "#         default_value=-1  # unknown token\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_values),\n",
    "#         default_value=\"\"  # unknown ID\n",
    "#     )\n",
    "\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9856e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import List, Dict, Tuple\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_vocabulary_tf(\n",
    "#     file_path_list: List[str],\n",
    "#     existing_vocab: Dict[str, int] | None = None\n",
    "# ) -> Tuple[tf.lookup.StaticHashTable, tf.lookup.StaticHashTable]:\n",
    "#     \"\"\"\n",
    "#     Build a character-level vocabulary from text files and return TF lookup tables:\n",
    "#       token_to_id: char -> int\n",
    "#       id_to_token: int -> char\n",
    "#     \"\"\"\n",
    "#     if isinstance(file_path_list, (str, bytes)):\n",
    "#         file_path_list = [file_path_list] # type: ignore\n",
    "#     if existing_vocab is None:\n",
    "#         existing_vocab = {}\n",
    "\n",
    "#     vocab_set = set(existing_vocab.keys())\n",
    "#     for file_name in file_path_list:\n",
    "#         if os.path.isdir(file_name):\n",
    "#             raise IsADirectoryError(f\"Expected file path, got directory: {file_name}\")\n",
    "#         if not os.path.isfile(file_name):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_name}\")\n",
    "#         with open(file_name, encoding=\"utf-8\") as f:\n",
    "#             text = f.read()\n",
    "#             vocab_set.update(text)\n",
    "\n",
    "#     sorted_tokens = sorted(vocab_set)\n",
    "#     token_to_id = {tok: i for i, tok in enumerate(sorted_tokens)}\n",
    "#     id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "#     token_keys = tf.constant(list(token_to_id.keys()), dtype=tf.string)\n",
    "#     token_vals = tf.constant(list(token_to_id.values()), dtype=tf.int32)\n",
    "\n",
    "#     id_keys = tf.constant(list(id_to_token.keys()), dtype=tf.int32)\n",
    "#     id_vals = tf.constant(list(id_to_token.values()), dtype=tf.string)\n",
    "\n",
    "#     token_to_id_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(token_keys, token_vals),\n",
    "#         default_value=-1\n",
    "#     )\n",
    "#     id_to_token_table = tf.lookup.StaticHashTable(\n",
    "#         initializer=tf.lookup.KeyValueTensorInitializer(id_keys, id_vals),\n",
    "#         default_value=tf.constant(\"\", dtype=tf.string)\n",
    "#     )\n",
    "#     return token_to_id_table, id_to_token_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "@keras.saving.register_keras_serializable()\n",
    "def prepare_sinusoidal_lookup_table(EMBEDDING_SIZE: int = 128, max_seq_len: int = 512):\n",
    "    \"\"\"\n",
    "    Builds a sinusoidal positional encoding lookup table.\n",
    "    \n",
    "    Args:\n",
    "      EMBEDDING_SIZE: dimensionality of each position encoding vector (must be even).\n",
    "      max_seq_len: maximum sequence length (number of positions).\n",
    "    \n",
    "    Returns:\n",
    "      lookup_table: a tf array of shape (max_seq_len, EMBEDDING_SIZE)\n",
    "                    where row p gives the positional encoding for position p.\n",
    "    \"\"\"\n",
    "    # Initialize the table\n",
    "    lookup_table = np.zeros((max_seq_len, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    \n",
    "    # Compute the angle rates for each dimension\n",
    "    # angle_rates[k] = 1 / (10000^(2*(k//2) / EMBEDDING_SIZE))\n",
    "    dims = np.arange(EMBEDDING_SIZE)[np.newaxis, :]   # shape (1, EMBEDDING_SIZE)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis] # shape (max_seq_len, 1)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / EMBEDDING_SIZE)\n",
    "    \n",
    "    # Compute the angle for each position and dimension: position * angle_rate\n",
    "    angle_rads = positions * angle_rates  # shape (max_seq_len, EMBEDDING_SIZE)\n",
    "    \n",
    "    # Apply sin to even indices (0,2,4,...) and cos to odd indices (1,3,5,...)\n",
    "    lookup_table[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    lookup_table[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.constant(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize_and_build_token_id(token_to_id_table: tf.lookup.StaticHashTable,\n",
    "#                                 text_batch: tf.Tensor,\n",
    "#                                 pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Tokenize a batch of strings character by character, pad sequences,\n",
    "#     and return attention masks.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_table: TF lookup table mapping char -> int\n",
    "#         text_batch: tf.Tensor of shape [batch_size], dtype=tf.string\n",
    "#         pad_value: int, ID to use for padding\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: tf.Tensor [batch_size, max_seq_len]\n",
    "#         attention_mask: tf.Tensor [batch_size, max_seq_len]\n",
    "#     \"\"\"\n",
    "#     token_ids_list = []\n",
    "\n",
    "#     for text in text_batch.numpy():  # type: ignore\n",
    "#         # Convert bytes to TF string\n",
    "\n",
    "#         # Split into characters\n",
    "#         char_tensor = tf.strings.bytes_split(text)\n",
    "\n",
    "#         # Lookup token IDs\n",
    "#         token_ids = token_to_id_table.lookup(char_tensor)\n",
    "\n",
    "#         token_ids_list.append(token_ids)\n",
    "\n",
    "#     # Pad all sequences to the same length\n",
    "#     token_ids_padded = tf.ragged.stack(token_ids_list).to_tensor(default_value=pad_value) # type: ignore\n",
    "#     # Create attention mask: 1 for real tokens, 0 for padding\n",
    "#     attention_mask = tf.cast(token_ids_padded != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids_padded, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66c28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from typing import Tuple\n",
    "\n",
    "\n",
    "# def tokenize_and_build_token_id(\n",
    "#     token_to_id_dict: dict,\n",
    "#     text_batch: list[str],\n",
    "#     max_seq_len: int,\n",
    "#     pad_value: int = 0,\n",
    "#     unk_value: int = None\n",
    "# ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "#     \"\"\"\n",
    "#     TensorFlow-compatible tokenization converting batch of strings to char token IDs,\n",
    "#     padded/truncated to max_seq_len, along with attention mask.\n",
    "\n",
    "#     Args:\n",
    "#         token_to_id_dict: dict mapping character str -> int ID\n",
    "#         text_batch: list of strings to tokenize\n",
    "#         max_seq_len: max length to pad/truncate sequences\n",
    "#         pad_value: int ID for padding tokens\n",
    "#         unk_value: int ID for unknown tokens; if None, uses pad_value\n",
    "\n",
    "#     Returns:\n",
    "#         token_ids: (batch_size, max_seq_len) tf.int32 tensor of token IDs\n",
    "#         attention_mask: (batch_size, max_seq_len) tf.int32 tensor (1 for tokens, 0 for padding)\n",
    "#     \"\"\"\n",
    "\n",
    "#     if unk_value is None:\n",
    "#         unk_value = pad_value\n",
    "\n",
    "#     # Create lookup table from token_to_id_dict\n",
    "#     keys = tf.constant(list(token_to_id_dict.keys()))\n",
    "#     values = tf.constant(list(token_to_id_dict.values()), dtype=tf.int32)\n",
    "#     table = tf.lookup.StaticHashTable(\n",
    "#         tf.lookup.KeyValueTensorInitializer(keys, values),\n",
    "#         default_value=unk_value\n",
    "#     )\n",
    "\n",
    "#     # Convert text batch to a RaggedTensor of chars\n",
    "#     rt_chars = tf.strings.unicode_split(text_batch, 'UTF-8')  # shape: [batch_size, (seq_len)]\n",
    "\n",
    "#     # Lookup token IDs for each char\n",
    "#     token_ids = table.lookup(rt_chars)\n",
    "\n",
    "#     # Pad or truncate sequences to max_seq_len\n",
    "#     token_ids = token_ids.to_tensor(default_value=pad_value, shape=[None, max_seq_len])\n",
    "#     token_ids = token_ids[:, :max_seq_len]  # truncate if longer\n",
    "\n",
    "#     # Construct attention mask: 1 where not pad_value, else 0\n",
    "#     attention_mask = tf.cast(token_ids != pad_value, tf.int32)\n",
    "\n",
    "#     return token_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e3a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_vocabulary_tf(\n",
    "    file_path_list: List[str],\n",
    "    vocab_size: int = 10000,\n",
    "    existing_vocab: Dict[str,int]|None = None\n",
    ") -> Dict[str,int]:\n",
    "    \"\"\"\n",
    "    Build a word-level vocabulary from text files with size control.\n",
    "    \n",
    "    Args:\n",
    "        file_path_list: List of file paths to process\n",
    "        vocab_size: Maximum vocabulary size (includes special tokens)\n",
    "        existing_vocab: Optional existing vocabulary to extend\n",
    "        \n",
    "    Returns:\n",
    "        token_to_id mapping each word (or <UNK>, <PAD>) to an integer ID.\n",
    "    \"\"\"\n",
    "    if isinstance(file_path_list, (str, bytes)):\n",
    "        file_path_list = [file_path_list]  # type: ignore\n",
    "    \n",
    "    if existing_vocab is None:\n",
    "        existing_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Count word frequencies across all files\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for fn in file_path_list:\n",
    "        if os.path.isdir(fn):\n",
    "            raise IsADirectoryError(fn)\n",
    "        if not os.path.isfile(fn):\n",
    "            raise FileNotFoundError(fn)\n",
    "        \n",
    "        with open(fn, encoding='utf-8') as f:\n",
    "            text = f.read().lower()\n",
    "        \n",
    "        # Simple word tokenizer; keeps only alphanumeric words\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Remove special tokens from word counts if they exist\n",
    "    for special_token in ['<PAD>', '<UNK>']:\n",
    "        if special_token in word_counts:\n",
    "            del word_counts[special_token]\n",
    "    \n",
    "    # Get the most common words (excluding space for special tokens)\n",
    "    num_special_tokens = len(existing_vocab)\n",
    "    max_regular_words = vocab_size - num_special_tokens\n",
    "    \n",
    "    most_common_words = [word for word, _ in word_counts.most_common(max_regular_words)]\n",
    "    \n",
    "    # Build the final vocabulary\n",
    "    token_to_id = existing_vocab.copy()\n",
    "    \n",
    "    for idx, word in enumerate(most_common_words, start=num_special_tokens):\n",
    "        token_to_id[word] = idx\n",
    "    \n",
    "    print(f\"Built vocabulary with {len(token_to_id)} tokens \"\n",
    "          f\"({num_special_tokens} special + {len(most_common_words)} regular)\")\n",
    "    \n",
    "    return token_to_id\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def tokenize_and_build_token_id(\n",
    "    token_to_id_dict: Dict[str,int],\n",
    "    text_batch: List[str],\n",
    "    max_seq_len: int,\n",
    "    pad_value: int = 0\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    '''\n",
    "    Tokenize batch of strings into word-IDs, pad/truncate, and build attention masks.\n",
    "    \n",
    "    Args:\n",
    "        token_to_id_dict: Dictionary mapping tokens to IDs\n",
    "        text_batch: List of text strings to tokenize\n",
    "        max_seq_len: Maximum sequence length (pad/truncate to this length)\n",
    "        pad_value: Value to use for padding (should match <PAD> token ID)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (token_ids, attention_mask) as TensorFlow tensors\n",
    "    '''\n",
    "    batch_ids = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "        ids = [token_to_id_dict.get(w, token_to_id_dict['<UNK>']) for w in words]\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        # Pad if too short\n",
    "        else:\n",
    "            ids += [pad_value] * (max_seq_len - len(ids))\n",
    "        \n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    token_ids = np.array(batch_ids, dtype=np.int32)\n",
    "    attention_mask = (token_ids != pad_value).astype(np.int32)\n",
    "\n",
    "    return tf.constant(token_ids), tf.constant(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4783fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocabulary with 5000 tokens (2 special + 4998 regular)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'the': 2,\n",
       " 'to': 3,\n",
       " 'and': 4,\n",
       " 'of': 5,\n",
       " 'a': 6,\n",
       " 'her': 7,\n",
       " 'i': 8,\n",
       " 'in': 9,\n",
       " 'was': 10,\n",
       " 'it': 11,\n",
       " 'she': 12,\n",
       " 'not': 13,\n",
       " 'that': 14,\n",
       " 'be': 15,\n",
       " 'you': 16,\n",
       " 'he': 17,\n",
       " 'had': 18,\n",
       " 'as': 19,\n",
       " 'for': 20,\n",
       " 'with': 21,\n",
       " 'his': 22,\n",
       " 'but': 23,\n",
       " 'have': 24,\n",
       " 'is': 25,\n",
       " 'at': 26,\n",
       " 's': 27,\n",
       " 'so': 28,\n",
       " 'my': 29,\n",
       " 'all': 30,\n",
       " 'on': 31,\n",
       " 'very': 32,\n",
       " 'by': 33,\n",
       " 'him': 34,\n",
       " 'could': 35,\n",
       " 'which': 36,\n",
       " 'been': 37,\n",
       " 'would': 38,\n",
       " 'no': 39,\n",
       " 'were': 40,\n",
       " 'they': 41,\n",
       " 'mr': 42,\n",
       " 'from': 43,\n",
       " 'this': 44,\n",
       " 'me': 45,\n",
       " 'what': 46,\n",
       " 'mrs': 47,\n",
       " 'will': 48,\n",
       " 'do': 49,\n",
       " 'or': 50,\n",
       " 'an': 51,\n",
       " 'them': 52,\n",
       " 'more': 53,\n",
       " 'there': 54,\n",
       " 'such': 55,\n",
       " 'their': 56,\n",
       " 'must': 57,\n",
       " 'your': 58,\n",
       " 'any': 59,\n",
       " 'said': 60,\n",
       " 'if': 61,\n",
       " 'one': 62,\n",
       " 'much': 63,\n",
       " 'when': 64,\n",
       " 'than': 65,\n",
       " 'are': 66,\n",
       " 'am': 67,\n",
       " 'miss': 68,\n",
       " 'we': 69,\n",
       " 'only': 70,\n",
       " 'should': 71,\n",
       " 'who': 72,\n",
       " 'well': 73,\n",
       " 'did': 74,\n",
       " 'every': 75,\n",
       " 'being': 76,\n",
       " 'now': 77,\n",
       " 'how': 78,\n",
       " 'think': 79,\n",
       " 'own': 80,\n",
       " 'never': 81,\n",
       " 'know': 82,\n",
       " 'good': 83,\n",
       " 'might': 84,\n",
       " 'time': 85,\n",
       " 'herself': 86,\n",
       " 'some': 87,\n",
       " 'little': 88,\n",
       " 'can': 89,\n",
       " 'before': 90,\n",
       " 'has': 91,\n",
       " 'nothing': 92,\n",
       " 'other': 93,\n",
       " 'too': 94,\n",
       " 'most': 95,\n",
       " 'though': 96,\n",
       " 'soon': 97,\n",
       " 'may': 98,\n",
       " 'say': 99,\n",
       " 'without': 100,\n",
       " 'first': 101,\n",
       " 'great': 102,\n",
       " 'lady': 103,\n",
       " 'see': 104,\n",
       " 'after': 105,\n",
       " 'again': 106,\n",
       " 'out': 107,\n",
       " 'about': 108,\n",
       " 'fanny': 109,\n",
       " 'dear': 110,\n",
       " 'two': 111,\n",
       " 'shall': 112,\n",
       " 'ever': 113,\n",
       " 'man': 114,\n",
       " 'sir': 115,\n",
       " 'into': 116,\n",
       " 'sister': 117,\n",
       " 'quite': 118,\n",
       " 'made': 119,\n",
       " 'then': 120,\n",
       " 'always': 121,\n",
       " 'day': 122,\n",
       " 'emma': 123,\n",
       " 'make': 124,\n",
       " 'thought': 125,\n",
       " 'up': 126,\n",
       " 'room': 127,\n",
       " 'young': 128,\n",
       " 'thing': 129,\n",
       " 'mother': 130,\n",
       " 'father': 131,\n",
       " 'however': 132,\n",
       " 'sure': 133,\n",
       " 'like': 134,\n",
       " 'house': 135,\n",
       " 'elizabeth': 136,\n",
       " 'long': 137,\n",
       " 'himself': 138,\n",
       " 'give': 139,\n",
       " 'its': 140,\n",
       " 'away': 141,\n",
       " 'go': 142,\n",
       " 'indeed': 143,\n",
       " 'having': 144,\n",
       " 'elinor': 145,\n",
       " 'last': 146,\n",
       " 'many': 147,\n",
       " 'even': 148,\n",
       " 'better': 149,\n",
       " 'us': 150,\n",
       " 'way': 151,\n",
       " 'cannot': 152,\n",
       " 'upon': 153,\n",
       " 'friend': 154,\n",
       " 'hope': 155,\n",
       " 'over': 156,\n",
       " 'enough': 157,\n",
       " 'here': 158,\n",
       " 'come': 159,\n",
       " 'felt': 160,\n",
       " 'catherine': 161,\n",
       " 'moment': 162,\n",
       " 'jane': 163,\n",
       " 'family': 164,\n",
       " 'oh': 165,\n",
       " 'our': 166,\n",
       " 'crawford': 167,\n",
       " 'just': 168,\n",
       " 'mind': 169,\n",
       " 'done': 170,\n",
       " 'wish': 171,\n",
       " 'marianne': 172,\n",
       " 'while': 173,\n",
       " 'still': 174,\n",
       " 'seemed': 175,\n",
       " 'feelings': 176,\n",
       " 'yet': 177,\n",
       " 'home': 178,\n",
       " 'love': 179,\n",
       " 'same': 180,\n",
       " 'brother': 181,\n",
       " 'happy': 182,\n",
       " 'something': 183,\n",
       " 'saw': 184,\n",
       " 'came': 185,\n",
       " 'myself': 186,\n",
       " 'letter': 187,\n",
       " 'place': 188,\n",
       " 'few': 189,\n",
       " 'perhaps': 190,\n",
       " 'look': 191,\n",
       " 'half': 192,\n",
       " 'really': 193,\n",
       " 'till': 194,\n",
       " 'anne': 195,\n",
       " 'heart': 196,\n",
       " 'where': 197,\n",
       " 'harriet': 198,\n",
       " 'yes': 199,\n",
       " 'found': 200,\n",
       " 'take': 201,\n",
       " 'woman': 202,\n",
       " 'almost': 203,\n",
       " 'another': 204,\n",
       " 'going': 205,\n",
       " 'pleasure': 206,\n",
       " 'heard': 207,\n",
       " 'rather': 208,\n",
       " 'morning': 209,\n",
       " 'believe': 210,\n",
       " 'down': 211,\n",
       " 'present': 212,\n",
       " 'does': 213,\n",
       " 'poor': 214,\n",
       " 'those': 215,\n",
       " 'world': 216,\n",
       " 'general': 217,\n",
       " 'off': 218,\n",
       " 'left': 219,\n",
       " 'subject': 220,\n",
       " 'certainly': 221,\n",
       " 'less': 222,\n",
       " 'replied': 223,\n",
       " 'weston': 224,\n",
       " 'together': 225,\n",
       " 'whom': 226,\n",
       " 'part': 227,\n",
       " 'feel': 228,\n",
       " 'both': 229,\n",
       " 'least': 230,\n",
       " 'manner': 231,\n",
       " 'tell': 232,\n",
       " 'darcy': 233,\n",
       " 'cried': 234,\n",
       " 'evening': 235,\n",
       " 'speak': 236,\n",
       " 'hear': 237,\n",
       " 'between': 238,\n",
       " 'once': 239,\n",
       " 'nor': 240,\n",
       " 'looked': 241,\n",
       " 'edmund': 242,\n",
       " 'these': 243,\n",
       " 'three': 244,\n",
       " 'anything': 245,\n",
       " 'kind': 246,\n",
       " 'seen': 247,\n",
       " 'often': 248,\n",
       " 'therefore': 249,\n",
       " 'happiness': 250,\n",
       " 'life': 251,\n",
       " 'people': 252,\n",
       " 'knew': 253,\n",
       " 'whole': 254,\n",
       " 'since': 255,\n",
       " 'knightley': 256,\n",
       " 'elton': 257,\n",
       " 'far': 258,\n",
       " 'party': 259,\n",
       " 'told': 260,\n",
       " 'gone': 261,\n",
       " 'suppose': 262,\n",
       " 'find': 263,\n",
       " 'opinion': 264,\n",
       " 'each': 265,\n",
       " 'ought': 266,\n",
       " 'given': 267,\n",
       " 'immediately': 268,\n",
       " 'right': 269,\n",
       " 'hour': 270,\n",
       " 'back': 271,\n",
       " 'side': 272,\n",
       " 'spirits': 273,\n",
       " 'let': 274,\n",
       " 'want': 275,\n",
       " 'best': 276,\n",
       " 'acquaintance': 277,\n",
       " 'possible': 278,\n",
       " 'others': 279,\n",
       " 'daughter': 280,\n",
       " 'next': 281,\n",
       " 'captain': 282,\n",
       " 'thomas': 283,\n",
       " 'known': 284,\n",
       " 'ill': 285,\n",
       " 'short': 286,\n",
       " 'leave': 287,\n",
       " 'get': 288,\n",
       " 'edward': 289,\n",
       " 'hardly': 290,\n",
       " 'began': 291,\n",
       " 'friends': 292,\n",
       " 'towards': 293,\n",
       " 'able': 294,\n",
       " 'eyes': 295,\n",
       " 'idea': 296,\n",
       " 'bennet': 297,\n",
       " 'years': 298,\n",
       " 'visit': 299,\n",
       " 'passed': 300,\n",
       " 'either': 301,\n",
       " 'against': 302,\n",
       " 'deal': 303,\n",
       " 'affection': 304,\n",
       " 'under': 305,\n",
       " 'everything': 306,\n",
       " 'attention': 307,\n",
       " 'old': 308,\n",
       " 'colonel': 309,\n",
       " 'went': 310,\n",
       " 'woodhouse': 311,\n",
       " 'aunt': 312,\n",
       " 'henry': 313,\n",
       " 'obliged': 314,\n",
       " 'seeing': 315,\n",
       " 'word': 316,\n",
       " 'return': 317,\n",
       " 'longer': 318,\n",
       " 'bingley': 319,\n",
       " 'gave': 320,\n",
       " 'coming': 321,\n",
       " 'john': 322,\n",
       " 'else': 323,\n",
       " 'looking': 324,\n",
       " 'sort': 325,\n",
       " 'character': 326,\n",
       " 'person': 327,\n",
       " 'through': 328,\n",
       " 'chapter': 329,\n",
       " 'doubt': 330,\n",
       " 'comfort': 331,\n",
       " 'wife': 332,\n",
       " 'answer': 333,\n",
       " 'because': 334,\n",
       " 'whether': 335,\n",
       " 'brought': 336,\n",
       " 'rest': 337,\n",
       " 'perfectly': 338,\n",
       " 'end': 339,\n",
       " 'why': 340,\n",
       " 'course': 341,\n",
       " 'elliot': 342,\n",
       " 'account': 343,\n",
       " 'town': 344,\n",
       " 'took': 345,\n",
       " 'put': 346,\n",
       " 'beyond': 347,\n",
       " 'received': 348,\n",
       " 'minutes': 349,\n",
       " 'whose': 350,\n",
       " 'glad': 351,\n",
       " 'days': 352,\n",
       " 'within': 353,\n",
       " 'bertram': 354,\n",
       " 'set': 355,\n",
       " 'means': 356,\n",
       " 'situation': 357,\n",
       " 'body': 358,\n",
       " 'door': 359,\n",
       " 'walk': 360,\n",
       " 'london': 361,\n",
       " 'reason': 362,\n",
       " 'point': 363,\n",
       " 'wished': 364,\n",
       " 'conversation': 365,\n",
       " 'mary': 366,\n",
       " 'girl': 367,\n",
       " 'hand': 368,\n",
       " 'business': 369,\n",
       " 'dashwood': 370,\n",
       " 'yourself': 371,\n",
       " 'pretty': 372,\n",
       " 'feeling': 373,\n",
       " 'continued': 374,\n",
       " 'name': 375,\n",
       " 'uncle': 376,\n",
       " 'agreeable': 377,\n",
       " 'added': 378,\n",
       " 'talked': 379,\n",
       " 'carriage': 380,\n",
       " 'things': 381,\n",
       " 'talk': 382,\n",
       " 'marriage': 383,\n",
       " 'taken': 384,\n",
       " 'married': 385,\n",
       " 'returned': 386,\n",
       " 'determined': 387,\n",
       " 'object': 388,\n",
       " 'children': 389,\n",
       " 'wanted': 390,\n",
       " 'manners': 391,\n",
       " 'already': 392,\n",
       " 'mean': 393,\n",
       " 'jennings': 394,\n",
       " 'power': 395,\n",
       " 'fairfax': 396,\n",
       " 'interest': 397,\n",
       " 'spoke': 398,\n",
       " 'understand': 399,\n",
       " 'ladies': 400,\n",
       " 'walked': 401,\n",
       " 'called': 402,\n",
       " 'sense': 403,\n",
       " 'entirely': 404,\n",
       " 'william': 405,\n",
       " 'ready': 406,\n",
       " 'themselves': 407,\n",
       " 'sorry': 408,\n",
       " 'state': 409,\n",
       " 'bath': 410,\n",
       " 'regard': 411,\n",
       " 'impossible': 412,\n",
       " 'head': 413,\n",
       " 'read': 414,\n",
       " 'fine': 415,\n",
       " 'neither': 416,\n",
       " 'dare': 417,\n",
       " 'different': 418,\n",
       " 'husband': 419,\n",
       " 'voice': 420,\n",
       " 'near': 421,\n",
       " 'care': 422,\n",
       " 'son': 423,\n",
       " 'isabella': 424,\n",
       " 'country': 425,\n",
       " 'churchill': 426,\n",
       " 'sisters': 427,\n",
       " 'nature': 428,\n",
       " 'fortune': 429,\n",
       " 'doing': 430,\n",
       " 'words': 431,\n",
       " 'tilney': 432,\n",
       " 'willoughby': 433,\n",
       " 'settled': 434,\n",
       " 'saying': 435,\n",
       " 'help': 436,\n",
       " 'expected': 437,\n",
       " 'wentworth': 438,\n",
       " 'frank': 439,\n",
       " 'rushworth': 440,\n",
       " 'marry': 441,\n",
       " 'men': 442,\n",
       " 'year': 443,\n",
       " 'new': 444,\n",
       " 'night': 445,\n",
       " 'wonder': 446,\n",
       " 'afraid': 447,\n",
       " 'change': 448,\n",
       " 'exactly': 449,\n",
       " 'charles': 450,\n",
       " 'matter': 451,\n",
       " 'true': 452,\n",
       " 'assure': 453,\n",
       " 'making': 454,\n",
       " 'full': 455,\n",
       " 'norris': 456,\n",
       " 'afterwards': 457,\n",
       " 'kindness': 458,\n",
       " 'satisfied': 459,\n",
       " 'appeared': 460,\n",
       " 'cousin': 461,\n",
       " 'engaged': 462,\n",
       " 'met': 463,\n",
       " 'talking': 464,\n",
       " 'gentleman': 465,\n",
       " 'stay': 466,\n",
       " 'appearance': 467,\n",
       " 'attachment': 468,\n",
       " 'equal': 469,\n",
       " 'among': 470,\n",
       " 'four': 471,\n",
       " 'real': 472,\n",
       " 'giving': 473,\n",
       " 'directly': 474,\n",
       " 'thinking': 475,\n",
       " 'dinner': 476,\n",
       " 'used': 477,\n",
       " 'hours': 478,\n",
       " 'early': 479,\n",
       " 'behaviour': 480,\n",
       " 'nobody': 481,\n",
       " 'likely': 482,\n",
       " 'five': 483,\n",
       " 'scarcely': 484,\n",
       " 'believed': 485,\n",
       " 'particular': 486,\n",
       " 'pleased': 487,\n",
       " 'entered': 488,\n",
       " 'turned': 489,\n",
       " 'wickham': 490,\n",
       " 'sometimes': 491,\n",
       " 'particularly': 492,\n",
       " 'alone': 493,\n",
       " 'street': 494,\n",
       " 'handsome': 495,\n",
       " 'work': 496,\n",
       " 'company': 497,\n",
       " 'week': 498,\n",
       " 'usual': 499,\n",
       " 'natural': 500,\n",
       " 'convinced': 501,\n",
       " 'got': 502,\n",
       " 'sat': 503,\n",
       " 'smith': 504,\n",
       " 'mansfield': 505,\n",
       " 'lucy': 506,\n",
       " 'use': 507,\n",
       " 'imagine': 508,\n",
       " 'engagement': 509,\n",
       " 'certain': 510,\n",
       " 'asked': 511,\n",
       " 'degree': 512,\n",
       " 'bad': 513,\n",
       " 'forward': 514,\n",
       " 'ask': 515,\n",
       " 'strong': 516,\n",
       " 'speaking': 517,\n",
       " 'farther': 518,\n",
       " 'collins': 519,\n",
       " 'conduct': 520,\n",
       " 'allow': 521,\n",
       " 'bear': 522,\n",
       " 'open': 523,\n",
       " 'greater': 524,\n",
       " 'call': 525,\n",
       " 'occasion': 526,\n",
       " 'susan': 527,\n",
       " 'table': 528,\n",
       " 'common': 529,\n",
       " 'air': 530,\n",
       " 'consequence': 531,\n",
       " 'ten': 532,\n",
       " 'necessary': 533,\n",
       " 'smile': 534,\n",
       " 'respect': 535,\n",
       " 'keep': 536,\n",
       " 'consider': 537,\n",
       " 'hoped': 538,\n",
       " 'appear': 539,\n",
       " 'small': 540,\n",
       " 'anxious': 541,\n",
       " 'question': 542,\n",
       " 'child': 543,\n",
       " 'lydia': 544,\n",
       " 'case': 545,\n",
       " 'face': 546,\n",
       " 'drawing': 547,\n",
       " 'surprise': 548,\n",
       " 'probably': 549,\n",
       " 'society': 550,\n",
       " 'advantage': 551,\n",
       " 'remember': 552,\n",
       " 'round': 553,\n",
       " 'circumstances': 554,\n",
       " 'maria': 555,\n",
       " 'amiable': 556,\n",
       " 'taste': 557,\n",
       " 'write': 558,\n",
       " 'truth': 559,\n",
       " 'greatest': 560,\n",
       " 'notice': 561,\n",
       " 'supposed': 562,\n",
       " 'former': 563,\n",
       " 'taking': 564,\n",
       " 'large': 565,\n",
       " 'meet': 566,\n",
       " 'hartfield': 567,\n",
       " 'whatever': 568,\n",
       " 'everybody': 569,\n",
       " 'expect': 570,\n",
       " 'months': 571,\n",
       " 'grant': 572,\n",
       " 'beauty': 573,\n",
       " 'meant': 574,\n",
       " 'park': 575,\n",
       " 'thoughts': 576,\n",
       " 'spite': 577,\n",
       " 'need': 578,\n",
       " 'knowledge': 579,\n",
       " 'satisfaction': 580,\n",
       " 'meeting': 581,\n",
       " 'sitting': 582,\n",
       " 'instantly': 583,\n",
       " 'james': 584,\n",
       " 'pain': 585,\n",
       " 'countenance': 586,\n",
       " 'second': 587,\n",
       " 'hearing': 588,\n",
       " 'serious': 589,\n",
       " 'cold': 590,\n",
       " 'wishes': 591,\n",
       " 'walking': 592,\n",
       " 'temper': 593,\n",
       " 'honour': 594,\n",
       " 'russell': 595,\n",
       " 'silence': 596,\n",
       " 'morland': 597,\n",
       " 'bates': 598,\n",
       " 'followed': 599,\n",
       " 'worth': 600,\n",
       " 'acquainted': 601,\n",
       " 'allowed': 602,\n",
       " 'pleasant': 603,\n",
       " 'thorpe': 604,\n",
       " 'equally': 605,\n",
       " 'proper': 606,\n",
       " 'musgrove': 607,\n",
       " 'beginning': 608,\n",
       " 'fancy': 609,\n",
       " 'eye': 610,\n",
       " 'brandon': 611,\n",
       " 'sensible': 612,\n",
       " 'delightful': 613,\n",
       " 'delighted': 614,\n",
       " 'extremely': 615,\n",
       " 'curiosity': 616,\n",
       " 'sent': 617,\n",
       " 'walter': 618,\n",
       " 'girls': 619,\n",
       " 'self': 620,\n",
       " 'bring': 621,\n",
       " 'answered': 622,\n",
       " 'considered': 623,\n",
       " 'daughters': 624,\n",
       " 'ago': 625,\n",
       " 'fond': 626,\n",
       " 'wrong': 627,\n",
       " 'julia': 628,\n",
       " 'easy': 629,\n",
       " 'sight': 630,\n",
       " 'play': 631,\n",
       " 'allen': 632,\n",
       " 'opportunity': 633,\n",
       " 'late': 634,\n",
       " 'twenty': 635,\n",
       " 'health': 636,\n",
       " 'worse': 637,\n",
       " 'charlotte': 638,\n",
       " 'evil': 639,\n",
       " 'pride': 640,\n",
       " 'creature': 641,\n",
       " 'aware': 642,\n",
       " 'louisa': 643,\n",
       " 'morrow': 644,\n",
       " 'thousand': 645,\n",
       " 'ferrars': 646,\n",
       " 'difference': 647,\n",
       " 'living': 648,\n",
       " 'sit': 649,\n",
       " 'dance': 650,\n",
       " 'arrival': 651,\n",
       " 'comfortable': 652,\n",
       " 'letters': 653,\n",
       " 'silent': 654,\n",
       " 'disposition': 655,\n",
       " 'admiration': 656,\n",
       " 'high': 657,\n",
       " 'distress': 658,\n",
       " 'spent': 659,\n",
       " 'sake': 660,\n",
       " 'view': 661,\n",
       " 'turn': 662,\n",
       " 'times': 663,\n",
       " 'except': 664,\n",
       " 'resolved': 665,\n",
       " 'excellent': 666,\n",
       " 't': 667,\n",
       " 'seem': 668,\n",
       " 'praise': 669,\n",
       " 'looks': 670,\n",
       " 'remained': 671,\n",
       " 'money': 672,\n",
       " 'became': 673,\n",
       " 'circumstance': 674,\n",
       " 'past': 675,\n",
       " 'influence': 676,\n",
       " 'loved': 677,\n",
       " 'weeks': 678,\n",
       " 'plan': 679,\n",
       " 'superior': 680,\n",
       " 'none': 681,\n",
       " 'danger': 682,\n",
       " 'thus': 683,\n",
       " 'fair': 684,\n",
       " 'observed': 685,\n",
       " 'seems': 686,\n",
       " 'invitation': 687,\n",
       " 'meaning': 688,\n",
       " 'price': 689,\n",
       " 'lost': 690,\n",
       " 'mentioned': 691,\n",
       " 'understanding': 692,\n",
       " 'merely': 693,\n",
       " 'favour': 694,\n",
       " 'knowing': 695,\n",
       " 'ball': 696,\n",
       " 'match': 697,\n",
       " 'live': 698,\n",
       " 'fixed': 699,\n",
       " 'highbury': 700,\n",
       " 'instead': 701,\n",
       " 'persuaded': 702,\n",
       " 'companion': 703,\n",
       " 'surprised': 704,\n",
       " 'happened': 705,\n",
       " 'purpose': 706,\n",
       " 'regret': 707,\n",
       " 'anybody': 708,\n",
       " 'yours': 709,\n",
       " 'judgment': 710,\n",
       " 'law': 711,\n",
       " 'leaving': 712,\n",
       " 'spoken': 713,\n",
       " 'easily': 714,\n",
       " 'hopes': 715,\n",
       " 'absence': 716,\n",
       " 'journey': 717,\n",
       " 'delight': 718,\n",
       " 'mistaken': 719,\n",
       " 'especially': 720,\n",
       " 'during': 721,\n",
       " 'ma': 722,\n",
       " 'thank': 723,\n",
       " 'attentions': 724,\n",
       " 'duty': 725,\n",
       " 'effect': 726,\n",
       " 'gratitude': 727,\n",
       " 'breakfast': 728,\n",
       " 'kept': 729,\n",
       " 'trouble': 730,\n",
       " 'also': 731,\n",
       " 'assured': 732,\n",
       " 'information': 733,\n",
       " 'event': 734,\n",
       " 'women': 735,\n",
       " 'highly': 736,\n",
       " 'vain': 737,\n",
       " 'inclination': 738,\n",
       " 'perfect': 739,\n",
       " 'arrived': 740,\n",
       " 'charming': 741,\n",
       " 'concern': 742,\n",
       " 'news': 743,\n",
       " 'future': 744,\n",
       " 'lived': 745,\n",
       " 'offer': 746,\n",
       " 'cottage': 747,\n",
       " 'tried': 748,\n",
       " 'desire': 749,\n",
       " 'age': 750,\n",
       " 'friendship': 751,\n",
       " 'mine': 752,\n",
       " 'spirit': 753,\n",
       " 'resolution': 754,\n",
       " 'dearest': 755,\n",
       " 'struck': 756,\n",
       " 'forget': 757,\n",
       " 'chance': 758,\n",
       " 'finding': 759,\n",
       " 'fear': 760,\n",
       " 'favourite': 761,\n",
       " 'turning': 762,\n",
       " 'weather': 763,\n",
       " 'pray': 764,\n",
       " 'whenever': 765,\n",
       " 'hands': 766,\n",
       " 'disappointment': 767,\n",
       " 'disposed': 768,\n",
       " 'written': 769,\n",
       " 'vernon': 770,\n",
       " 'enjoyment': 771,\n",
       " 'loss': 772,\n",
       " 'style': 773,\n",
       " 'share': 774,\n",
       " 'tone': 775,\n",
       " 'reached': 776,\n",
       " 'quiet': 777,\n",
       " 'repeated': 778,\n",
       " 'above': 779,\n",
       " 'led': 780,\n",
       " 'behind': 781,\n",
       " 'ease': 782,\n",
       " 'prevent': 783,\n",
       " 'credit': 784,\n",
       " 'conviction': 785,\n",
       " 'ashamed': 786,\n",
       " 'absolutely': 787,\n",
       " 'cause': 788,\n",
       " 'length': 789,\n",
       " 'fact': 790,\n",
       " 'decided': 791,\n",
       " 'gentlemen': 792,\n",
       " 'depend': 793,\n",
       " 'says': 794,\n",
       " 'pity': 795,\n",
       " 'importance': 796,\n",
       " 'smallest': 797,\n",
       " 'cousins': 798,\n",
       " 'eager': 799,\n",
       " 'interesting': 800,\n",
       " 'affectionate': 801,\n",
       " 'angry': 802,\n",
       " 'marrying': 803,\n",
       " 'middleton': 804,\n",
       " 'scheme': 805,\n",
       " 'distance': 806,\n",
       " 'confidence': 807,\n",
       " 'admiral': 808,\n",
       " 'unhappy': 809,\n",
       " 'exceedingly': 810,\n",
       " 'yesterday': 811,\n",
       " 'dancing': 812,\n",
       " 'otherwise': 813,\n",
       " 'listened': 814,\n",
       " 'mention': 815,\n",
       " 'anxiety': 816,\n",
       " 'following': 817,\n",
       " 'act': 818,\n",
       " 'difficulty': 819,\n",
       " 'trying': 820,\n",
       " 'light': 821,\n",
       " 'humour': 822,\n",
       " 'forced': 823,\n",
       " 'miles': 824,\n",
       " 'warm': 825,\n",
       " 'understood': 826,\n",
       " 'pleasing': 827,\n",
       " 'sweet': 828,\n",
       " 'excuse': 829,\n",
       " 'pass': 830,\n",
       " 'ah': 831,\n",
       " 'writing': 832,\n",
       " 'tea': 833,\n",
       " 'terms': 834,\n",
       " 'eldest': 835,\n",
       " 'alarm': 836,\n",
       " 'consideration': 837,\n",
       " 'reply': 838,\n",
       " 'receive': 839,\n",
       " 'window': 840,\n",
       " 'low': 841,\n",
       " 'promised': 842,\n",
       " 'altogether': 843,\n",
       " 'generally': 844,\n",
       " 'justice': 845,\n",
       " 'itself': 846,\n",
       " 'wait': 847,\n",
       " 'joined': 848,\n",
       " 'project': 849,\n",
       " 'niece': 850,\n",
       " 'martin': 851,\n",
       " 'wishing': 852,\n",
       " 'waiting': 853,\n",
       " 'avoid': 854,\n",
       " 'promise': 855,\n",
       " 'lord': 856,\n",
       " 'neighbourhood': 857,\n",
       " 'attended': 858,\n",
       " 'agitation': 859,\n",
       " 'compliment': 860,\n",
       " 'attached': 861,\n",
       " 'lizzy': 862,\n",
       " 'gardiner': 863,\n",
       " 'attempt': 864,\n",
       " 'civility': 865,\n",
       " 'wanting': 866,\n",
       " 'formed': 867,\n",
       " 'misery': 868,\n",
       " 'send': 869,\n",
       " 'please': 870,\n",
       " 'de': 871,\n",
       " 'death': 872,\n",
       " 'besides': 873,\n",
       " 'occurred': 874,\n",
       " 'pounds': 875,\n",
       " 'intended': 876,\n",
       " 'astonishment': 877,\n",
       " 'begin': 878,\n",
       " 'join': 879,\n",
       " 'fortnight': 880,\n",
       " 'gutenberg': 881,\n",
       " 'close': 882,\n",
       " 'rooms': 883,\n",
       " 'expressed': 884,\n",
       " 'actually': 885,\n",
       " 'servant': 886,\n",
       " 'due': 887,\n",
       " 'agreed': 888,\n",
       " 'choice': 889,\n",
       " 'henrietta': 890,\n",
       " 'six': 891,\n",
       " 'expectation': 892,\n",
       " 'tom': 893,\n",
       " 'support': 894,\n",
       " 'note': 895,\n",
       " 'particulars': 896,\n",
       " 'ladyship': 897,\n",
       " 'randalls': 898,\n",
       " 'barton': 899,\n",
       " 'reading': 900,\n",
       " 'latter': 901,\n",
       " 'pause': 902,\n",
       " 'immediate': 903,\n",
       " 'proof': 904,\n",
       " 'returning': 905,\n",
       " 'produced': 906,\n",
       " 'sudden': 907,\n",
       " 'hers': 908,\n",
       " 'form': 909,\n",
       " 'quick': 910,\n",
       " 'judge': 911,\n",
       " 'prevented': 912,\n",
       " 'nearly': 913,\n",
       " 'longbourn': 914,\n",
       " 'dreadful': 915,\n",
       " 'caught': 916,\n",
       " 'beautiful': 917,\n",
       " 'joy': 918,\n",
       " 'bed': 919,\n",
       " 'speech': 920,\n",
       " 'indifference': 921,\n",
       " 'pay': 922,\n",
       " 'horses': 923,\n",
       " 'folly': 924,\n",
       " 'strange': 925,\n",
       " 'knows': 926,\n",
       " 'welcome': 927,\n",
       " 'vanity': 928,\n",
       " 'capable': 929,\n",
       " 'remain': 930,\n",
       " 'greatly': 931,\n",
       " 'parsonage': 932,\n",
       " 'necessity': 933,\n",
       " 'useful': 934,\n",
       " 'income': 935,\n",
       " 'quarter': 936,\n",
       " 'command': 937,\n",
       " 'cheerful': 938,\n",
       " 'proved': 939,\n",
       " 'listen': 940,\n",
       " 'express': 941,\n",
       " 'suffered': 942,\n",
       " 'lively': 943,\n",
       " 'address': 944,\n",
       " 'frederica': 945,\n",
       " 'opened': 946,\n",
       " 'consciousness': 947,\n",
       " 'several': 948,\n",
       " 'period': 949,\n",
       " 'persuade': 950,\n",
       " 'intimacy': 951,\n",
       " 'extraordinary': 952,\n",
       " 'plain': 953,\n",
       " 'parties': 954,\n",
       " 'smiling': 955,\n",
       " 'moments': 956,\n",
       " 'stand': 957,\n",
       " 'book': 958,\n",
       " 'advice': 959,\n",
       " 'public': 960,\n",
       " 'value': 961,\n",
       " 'getting': 962,\n",
       " 'trust': 963,\n",
       " 'elegant': 964,\n",
       " 'wrote': 965,\n",
       " 'comes': 966,\n",
       " 'tears': 967,\n",
       " 'amusement': 968,\n",
       " 'acknowledged': 969,\n",
       " 'receiving': 970,\n",
       " 'fears': 971,\n",
       " 'observation': 972,\n",
       " 'education': 973,\n",
       " 'eleanor': 974,\n",
       " 'reginald': 975,\n",
       " 'disappointed': 976,\n",
       " 'spend': 977,\n",
       " 'important': 978,\n",
       " 'safe': 979,\n",
       " 'hundred': 980,\n",
       " 'fire': 981,\n",
       " 'horse': 982,\n",
       " 'try': 983,\n",
       " 'considering': 984,\n",
       " 'perry': 985,\n",
       " 'hall': 986,\n",
       " 'required': 987,\n",
       " 'secret': 988,\n",
       " 'lately': 989,\n",
       " 'staying': 990,\n",
       " 'hurry': 991,\n",
       " 'expression': 992,\n",
       " 'calling': 993,\n",
       " 'paid': 994,\n",
       " 'reflection': 995,\n",
       " 'delay': 996,\n",
       " 'palmer': 997,\n",
       " 'respectable': 998,\n",
       " 'explanation': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id_dict = tokenize_and_build_vocabulary_tf(['/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'],vocab_size = 5000)\n",
    "token_to_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9075b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756542260.516338    1336 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=string, numpy=\n",
       "array([b'A', b'k', b's', b'h', b'a', b't', b' ', b'K', b'h', b'a', b't',\n",
       "       b'r', b'i'], dtype=object)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = tf.constant('Akshat Khatri')\n",
    "tf.strings.bytes_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45984209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello'\n",
      "b'Worlds '\n"
     ]
    }
   ],
   "source": [
    "name = tf.constant(['Hello','Worlds '])\n",
    "for name in name.numpy():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ba567a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1.,2.,3.])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046629d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(11, 512), dtype=int32, numpy=\n",
       " array([[   1,    1,    0, ...,    0,    0,    0],\n",
       "        [   1,    0,    0, ...,    0,    0,    0],\n",
       "        [  45,    0,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 389,    0,    0, ...,    0,    0,    0],\n",
       "        [   1,    0,    0, ...,    0,    0,    0],\n",
       "        [3304,    0,    0, ...,    0,    0,    0]],\n",
       "       shape=(11, 512), dtype=int32)>,\n",
       " <tf.Tensor: shape=(11, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]], shape=(11, 512), dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = ['Akshat Khatri','hello','me','elizabeth','children','mine','coherent','i am fine','Children','trained','model']\n",
    "tokenize_and_build_token_id(token_to_id_dict,name,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d1a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable\n",
    "\n",
    "# class InitializePositionalEmbeddings(keras.layers.Layer): # Receives input of sequence of text\n",
    "#     def __init__(self,d_model: int = 128,sinusoidal_lookup_table = [],token_to_id_dict : tf.lookup.StaticHashTable = {} ,max_seq_len : int = 512,**kwargs): # type: ignore\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.d_model = d_model # d_model\n",
    "#         self.max_seq_len = max_seq_len\n",
    "        \n",
    "#         assert len(sinusoidal_lookup_table) > 0\n",
    "#         assert token_to_id_dict.size().numpy() > 0\n",
    "#         self.VOCAB_SIZE = token_to_id_dict.size().numpy()\n",
    "\n",
    "#         self.pos_table = sinusoidal_lookup_table\n",
    "#         self._embedding_dim = [self.VOCAB_SIZE,d_model]\n",
    "#         self.token_to_id_dict = token_to_id_dict\n",
    "    \n",
    "#     def build(self, input_shape): # this is batch input shape\n",
    "#         print(input_shape)\n",
    "#         self.embedding_matrix = self.add_weight(\n",
    "#             name=\"embedding_matrix\",\n",
    "#             shape=(self.VOCAB_SIZE, self.d_model),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True   # important\n",
    "#         )\n",
    "#         self.input_seq_list = input_shape[-1]\n",
    "\n",
    "#     def call(self,inputs):\n",
    "#         # print(inputs)\n",
    "#         tokens_in_id,non_padded_tokens_mask = tokenize_and_build_token_id(self.token_to_id_dict,inputs)\n",
    "#         # print(tokens_in_id,non_padded_tokens_mask,sep = '\\n')\n",
    "#         token_embeddings = tf.nn.embedding_lookup(self.embedding_matrix, tokens_in_id)\n",
    "#         # Positional embeddings\n",
    "#         seq_len = tf.shape(tokens_in_id)[1] # type: ignore\n",
    "#         pos_embeddings = self.pos_table[:seq_len, :]\n",
    "#         pos_embeddings = tf.expand_dims(pos_embeddings, 0)  # broadcast along batch\n",
    "#         # Add token + position embeddings\n",
    "#         embeddings = token_embeddings + pos_embeddings\n",
    "#         return embeddings,non_padded_tokens_mask\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         base_config = super().get_config()\n",
    "#         return {**base_config,'EMBEDDING_SIZE' : self.EMBEDDING_SIZE,'VOCAB_SIZE' : self.VOCAB_SIZE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d037241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class InitializePositionalEmbeddings(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        vocab_size : int,\n",
    "        CONTEXT_LEN: int = 128,\n",
    "        pad_value: int = 0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = int(d_model)\n",
    "        self.pad_value = int(pad_value)\n",
    "        self.vocab_size = vocab_size\n",
    "        self._pos_table = prepare_sinusoidal_lookup_table(d_model, CONTEXT_LEN)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embedding_matrix = self.add_weight(\n",
    "            name=\"embedding_matrix\",\n",
    "            shape=(self.vocab_size, self.d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, text_batch):\n",
    "\n",
    "        token_ids= text_batch # Unpacking Data Pre-processing inputs Embeddings\n",
    "        \n",
    "        # Embeddings lookup: (B, T, D)\n",
    "        token_emb = tf.nn.embedding_lookup(self.embedding_matrix, token_ids)\n",
    "        # Positional embeddings: slice and broadcast\n",
    "        seq_len = tf.shape(token_ids)[1] # type: ignore\n",
    "        pos_emb = self._pos_table[:seq_len, :]    # type: ignore # (T, D)\n",
    "        pos_emb = tf.expand_dims(pos_emb, 0)     # (1, T, D)\n",
    "        embeddings = token_emb + pos_emb         # (B, T, D)\n",
    "        return embeddings\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     # input_shape: (batch_size,)\n",
    "    #     batch = input_shape\n",
    "    #     # Sequence length is dynamic: None\n",
    "    #     return (batch, None, self.d_model), (batch, None)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            \"pad_value\": self.pad_value,\n",
    "        })\n",
    "        return cfg\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch_size, seq_len)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        return (batch_size, seq_len, self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c431a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(token_to_id_dict)\n",
    "D_MODEL = 1024\n",
    "MAX_SEQ_LEN = 512\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd66a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL)\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "batch_text = tokenize_and_build_token_id(token_to_id_dict,batch_text,MAX_SEQ_LEN) # type: ignore\n",
    "token_ids,attention_mask = batch_text\n",
    "\n",
    "layer = InitializePositionalEmbeddings(D_MODEL,VOCAB_SIZE)\n",
    "\n",
    "# @tf.function\n",
    "# def call_some(batch_text):\n",
    "#     embeddings = layer(batch_text)\n",
    "#     return embeddings\n",
    "\n",
    "# call_some(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6e1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SelfAttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, attention_heads=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention_heads = attention_heads\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[0][-1]\n",
    "        \n",
    "        self.Query_projection = self.add_weight(\n",
    "            name='Query_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Key_projection = self.add_weight(\n",
    "            name='Key_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.Value_projection = self.add_weight(\n",
    "            name='Value_Vector_for_projection',\n",
    "            initializer='random_normal',\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True \n",
    "        )\n",
    "        self.output_projection = self.add_weight(\n",
    "            name=\"Output_projection\",\n",
    "            initializer=\"random_normal\",\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.d_head = self.d_model // self.attention_heads\n",
    "        assert self.d_model % self.attention_heads == 0, \"d_model must be divisible by attention_heads\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embeddings = inputs[0]\n",
    "        token_masks = inputs[1]\n",
    "\n",
    "        batch_size = tf.shape(embeddings)[0]\n",
    "        seq_len = tf.shape(embeddings)[1]\n",
    "\n",
    "        # 1. Project to Q, K, V\n",
    "        Q = embeddings @ self.Query_projection\n",
    "        K = embeddings @ self.Key_projection\n",
    "        V = embeddings @ self.Value_projection\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        Q = tf.reshape(Q, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        K = tf.reshape(K, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "        V = tf.reshape(V, (batch_size, seq_len, self.attention_heads, self.d_head))\n",
    "\n",
    "        Q = tf.transpose(Q, (0, 2, 1, 3))  # (batch, heads, seq_len, d_head)\n",
    "        K = tf.transpose(K, (0, 2, 1, 3))\n",
    "        V = tf.transpose(V, (0, 2, 1, 3))\n",
    "\n",
    "        # 3. Compute attention scores\n",
    "        scores = tf.matmul(Q, K, transpose_b=True)  # (batch, heads, seq_len, seq_len)\n",
    "        scores = scores / tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        \n",
    "        # 4. FIXED MASKING - This was your main bug\n",
    "        # 4a. Causal mask (L,L) lower triangular\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        \n",
    "        # 4b. Token mask - FIXED: proper broadcasting to all heads\n",
    "        token_mask = tf.cast(token_masks, tf.float32)  # (B, L)\n",
    "        \n",
    "        # Create proper attention mask shape (B, H, L, L)\n",
    "        # Each head gets the same mask pattern\n",
    "        attention_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, L, L)\n",
    "        attention_mask = attention_mask * token_mask[:, tf.newaxis, tf.newaxis, :]  # (B, 1, 1, L)\n",
    "        attention_mask = attention_mask * token_mask[:, tf.newaxis, :, tf.newaxis]  # (B, 1, L, 1)\n",
    "        \n",
    "        # Broadcast to all heads\n",
    "        attention_mask = tf.broadcast_to(attention_mask, (batch_size, self.attention_heads, seq_len, seq_len))\n",
    "        \n",
    "        # 5. Apply mask with stronger negative value\n",
    "        scores = tf.where(\n",
    "            attention_mask > 0, \n",
    "            scores, \n",
    "            tf.constant(-1e30, dtype=scores.dtype)  # FIXED: Much more negative\n",
    "        )\n",
    "\n",
    "        # 6. Softmax and apply to values\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Add attention dropout (missing in your original)\n",
    "        attention_weights = tf.nn.dropout(attention_weights, rate=0.1)\n",
    "        \n",
    "        context = attention_weights @ V   # (batch, heads, seq_len, d_head)\n",
    "        \n",
    "        # 7. Concatenate heads\n",
    "        concat_context = tf.transpose(context, (0, 2, 1, 3))  # (batch, seq_len, heads, d_head)\n",
    "        concat_context = tf.reshape(concat_context, (batch_size, seq_len, self.d_model))\n",
    "        \n",
    "        # 8. Final projection\n",
    "        final_context = concat_context @ self.output_projection \n",
    "        return final_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"attention_heads\": self.attention_heads})\n",
    "        return config\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a50e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 13, 4, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], shape=(16, 13, 4, 3), dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = np.random.rand(3, 13, 64)  # batch, seq_len, d_model\n",
    "q = np.ones((64, 64))          # d_model × d_model\n",
    "\n",
    "a = tf.constant(a, dtype=tf.float32)\n",
    "q = tf.constant(q, dtype=tf.float32)\n",
    "\n",
    "s = a @ q   # type: ignore # [3, 13, 64]\n",
    "d_head = 16\n",
    "num_heads = 64 // d_head # 4\n",
    "\n",
    "# split into heads\n",
    "s = tf.reshape(s, (3, num_heads, 13, d_head))  # [3, 4, 13, 16]\n",
    "f = tf.constant(np.ones_like(s))\n",
    "\n",
    "tf.transpose(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "873ee087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.95257413, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.04742587, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,9,4,5],[4,5,6,7,8]])\n",
    "keras.activations.softmax(tf.cast(arr,dtype = tf.float32),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb91875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0.04742587, 0.04742587, 0.04742587, 0.04742587, 0.04742587],\n",
       "       [0.95257413, 0.95257413, 0.95257413, 0.95257413, 0.95257413]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tf.constant([[1,2,3,4,5],[4,5,6,7,8]])\n",
    "arr = tf.cast(arr,dtype = tf.float32)\n",
    "\n",
    "tf.nn.softmax(arr,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7cc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000000.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0da4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self,eps=1e-5,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self,input_shape): # Near Attention (batch, seq_len, d_model)\n",
    "        self.alpha = self.add_weight(\n",
    "            name = 'alpha',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'ones',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name = 'beta',\n",
    "            shape = input_shape[-1:],\n",
    "            initializer = 'zeros',\n",
    "            dtype = tf.float32,\n",
    "            trainable = True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mean, var = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.eps) # type: ignore\n",
    "        return self.alpha * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"eps\": self.eps}\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7a994b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"he_normal\",\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"bias\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, seq_len, input_dim)\n",
    "        output = tf.matmul(inputs, self.kernel) + self.bias  # shape: (batch, seq_len, units))  # shape: (batch, seq_len, units)\n",
    "        return output + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape: (batch, seq_len, input_dim)\n",
    "        return (input_shape, input_shape[1], self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb7c629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (2,3,4,5,6)\n",
    "a[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea053e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.Model):\n",
    "    '''A single Decoder Block'''\n",
    "    def __init__(self, d_model, n_heads, dropout_rate=0.1, epsilon=1e-5, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epsilon = epsilon\n",
    "        # norms\n",
    "        self.ln1 = LayerNormalization(epsilon)   # pre-attn\n",
    "        self.ln2 = LayerNormalization(epsilon)   # pre-ffn\n",
    "        # attention (assumes your SelfAttentionLayer accepts (x, attention_mask))\n",
    "        self.attn = SelfAttentionLayer(n_heads)\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        # FFN\n",
    "        self.ffn1 = keras.layers.Dense(4 * d_model, activation=\"gelu\")\n",
    "        self.ffn2 = keras.layers.Dense(d_model)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, attention_mask, training=False):\n",
    "        # Self-attention sublayer\n",
    "        y = self.ln1(x)\n",
    "        y = self.attn((y, attention_mask))          # shape: (B, T, d_model)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "\n",
    "        # FFN sublayer\n",
    "        y = self.ln2(x)\n",
    "        y = self.ffn1(y)\n",
    "        y = self.ffn2(y)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        x = x + y                                    # residual\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # input_shape is typically (batch_size, seq_len, d_model)\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"epsilon\": self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class GPT(keras.Model):\n",
    "    '''\n",
    "    GPT model with N distinct blocks\n",
    "      -----------------------------------'''\n",
    "    def __init__(self,\n",
    "                 d_model: int = 128,\n",
    "                 vocab_size: int = 94,\n",
    "                 context_length: int = 512,\n",
    "                 attention_heads: int = 8,\n",
    "                 epsilon: float = 1e-5,\n",
    "                 decoder_blocks: int = 3,\n",
    "                 dropout_rate: float = 0.1,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._d_model = d_model\n",
    "        self._vocab_size = vocab_size\n",
    "        self._context_length = context_length\n",
    "        self._attention_heads = attention_heads\n",
    "        self._epsilon = epsilon\n",
    "        self._decoder_blocks = decoder_blocks\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings (yours)\n",
    "        self.emb = InitializePositionalEmbeddings(\n",
    "            d_model, vocab_size,context_length,name=\"init_embeddings\"\n",
    "        )\n",
    "\n",
    "        # stack of distinct decoder blocks\n",
    "        self.blocks = [\n",
    "            DecoderBlock(d_model, attention_heads, dropout_rate, epsilon, name=f\"decoder_block_{i}\")\n",
    "            for i in range(decoder_blocks)\n",
    "        ]\n",
    "\n",
    "        # final norm (GPT-2 style) and LM head\n",
    "        self.final_ln = LayerNormalization(epsilon)\n",
    "        self.lm_head = keras.layers.Dense(vocab_size, name=\"Model_head\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (token_ids, attention_mask)\n",
    "          - token_ids: int32 (B, T)\n",
    "          - attention_mask: int32/float32 mask broadcasting to attention logits.\n",
    "            Common shapes: (B, 1, 1, T) or (B, T) if your SelfAttentionLayer handles expansion.\n",
    "        \"\"\"\n",
    "        token_ids, attention_mask = inputs\n",
    "        x = self.emb(token_ids)                         # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask, training=training)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.lm_head(x)                        # (B, T, vocab_size)\n",
    "        return logits                                   # keep softmax outside\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"d_model\": self._d_model,\n",
    "            \"vocab_size\": self._vocab_size,\n",
    "            \"context_length\": self._context_length,\n",
    "            \"attention_heads\": self._attention_heads,\n",
    "            \"epsilon\": self._epsilon,\n",
    "            \"decoder_blocks\": self._decoder_blocks,\n",
    "            \"dropout_rate\": self._dropout_rate,\n",
    "        })\n",
    "        return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd143548",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 64\n",
    "VOCAB_SIZE = len(token_to_id_dict) # 94 currently char level\n",
    "\n",
    "batch_text = ['yo','Akshat Khatri', 'Hello World','Me']\n",
    "token_ids,attention_mask = tokenize_and_build_token_id(token_to_id_dict,batch_text,CONTEXT_LEN) # type: ignore # Unpacking Values\n",
    "sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL,CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d639c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 64), dtype=int32, numpy=\n",
       " array([[  1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  1,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  1, 216,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(4, 64), dtype=int32, numpy=\n",
       " array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token_ids,attention_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f0db5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class CosineDecayWithWarmup(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, \n",
    "                 warmup_steps: int,\n",
    "                 total_steps: int,\n",
    "                 peak_learning_rate: float = 1e-4,\n",
    "                 min_learning_rate: float = 1e-6,\n",
    "                 name: str = \"cosine_decay_with_warmup\"):\n",
    "        super().__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.peak_learning_rate = peak_learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase: linear increase from 0 to peak_learning_rate\n",
    "        warmup_lr = self.peak_learning_rate * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        cosine_decay_lr = self.min_learning_rate + 0.5 * (\n",
    "            self.peak_learning_rate - self.min_learning_rate\n",
    "        ) * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
    "        \n",
    "        return tf.where(step < warmup_steps, warmup_lr, cosine_decay_lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"peak_learning_rate\": self.peak_learning_rate,\n",
    "            \"min_learning_rate\": self.min_learning_rate,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "\n",
    "# Example usage for your model\n",
    "# Estimate your training parameters\n",
    "EPOCHS = 20\n",
    "STEPS_PER_EPOCH = 1000  # Adjust based on your dataset size and batch size\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
    "WARMUP_STEPS = int(0.1 * TOTAL_STEPS)  # 10% warmup\n",
    "\n",
    "# Create the learning rate schedule\n",
    "lr_schedule = CosineDecayWithWarmup(\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    peak_learning_rate=1e-4,  # Your desired peak learning rate\n",
    "    min_learning_rate=1e-6    # Minimum learning rate at the end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f54ded",
   "metadata": {},
   "source": [
    "## Modify above according to requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45b6a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_8      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_9      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_4     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_10     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_11     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_5     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_12     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_13     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_6     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_14     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_15     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_7     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttentionLayer</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,198,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,195,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_16          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,125,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization        │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_1      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer       │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout (\u001b[38;5;33mDropout\u001b[0m)          │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_1 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_2      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_3      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_1     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_2 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_3 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_4      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_5      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_2     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_4 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_5 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_6      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_7      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_3     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_6 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_7 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_4 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_8      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_9      │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_4     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_8 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_9 (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_5 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_10     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_11     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_5     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_10 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_11 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_6 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_12     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_13     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_6     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_12 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_13 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_7 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_14     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ layer_normalization_15     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ self_attention_layer_7     │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,194,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttentionLayer\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_14 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m4096\u001b[0m)          │     \u001b[38;5;34m4,198,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dense_15 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m4,195,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│    └ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)       │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_16          │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5000\u001b[0m)          │     \u001b[38;5;34m5,125,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build once to get .summary()\n",
    "DECODER_BLOCKS = 8\n",
    "ATTENTION_HEADS = 4\n",
    "\n",
    "GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,0.1)\n",
    "_ = GPT_model((token_ids, attention_mask))\n",
    "GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# training (stable): use logits\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0)\n",
    "GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# inference probs (when you actually need them)\n",
    "logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "probs = keras.ops.softmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc4eb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 64, 5000), dtype=float32, numpy=\n",
       "array([[[9.17721263e-05, 1.58267620e-04, 7.89049882e-05, ...,\n",
       "         4.62345524e-05, 1.11462432e-04, 2.63327558e-04],\n",
       "        [2.16269196e-04, 3.55634198e-04, 1.23658232e-04, ...,\n",
       "         1.27934618e-04, 1.64374433e-04, 1.01247846e-04],\n",
       "        [2.25262251e-04, 3.35881399e-04, 1.28588930e-04, ...,\n",
       "         1.23376216e-04, 1.56066497e-04, 1.05651168e-04],\n",
       "        ...,\n",
       "        [2.38189517e-04, 3.20498715e-04, 1.23040430e-04, ...,\n",
       "         1.24532031e-04, 1.51542234e-04, 1.22151483e-04],\n",
       "        [2.32070524e-04, 3.26718087e-04, 1.24747923e-04, ...,\n",
       "         1.22275713e-04, 1.55416099e-04, 1.22848462e-04],\n",
       "        [2.32014121e-04, 3.25196423e-04, 1.19121287e-04, ...,\n",
       "         1.19630742e-04, 1.57797651e-04, 1.21494486e-04]],\n",
       "\n",
       "       [[1.21259392e-04, 1.85770652e-04, 9.58174496e-05, ...,\n",
       "         5.75506056e-05, 1.00423167e-04, 3.54145566e-04],\n",
       "        [1.09879955e-04, 2.03619085e-04, 8.85180925e-05, ...,\n",
       "         4.91684514e-05, 1.00627884e-04, 3.35431163e-04],\n",
       "        [2.30119884e-04, 3.41016857e-04, 1.19740675e-04, ...,\n",
       "         1.21624682e-04, 1.62198383e-04, 1.05980849e-04],\n",
       "        ...,\n",
       "        [2.42585957e-04, 3.35450924e-04, 1.17334523e-04, ...,\n",
       "         1.23407750e-04, 1.54703972e-04, 1.28248881e-04],\n",
       "        [2.28670411e-04, 3.52165953e-04, 1.21338293e-04, ...,\n",
       "         1.31195600e-04, 1.58775380e-04, 1.25402847e-04],\n",
       "        [2.34982333e-04, 3.26270441e-04, 1.20304248e-04, ...,\n",
       "         1.25254533e-04, 1.57578397e-04, 1.24298458e-04]],\n",
       "\n",
       "       [[9.25674176e-05, 2.16106331e-04, 7.63795761e-05, ...,\n",
       "         5.99753366e-05, 1.84983277e-04, 2.87745934e-04],\n",
       "        [1.03967446e-04, 2.16336237e-04, 7.84033909e-05, ...,\n",
       "         5.12811930e-05, 1.52075067e-04, 2.35437488e-04],\n",
       "        [2.20210030e-04, 3.37449630e-04, 1.26653002e-04, ...,\n",
       "         1.24424754e-04, 1.58605937e-04, 1.10730718e-04],\n",
       "        ...,\n",
       "        [2.18491085e-04, 3.32101772e-04, 1.22398153e-04, ...,\n",
       "         1.25486375e-04, 1.53812798e-04, 1.26359650e-04],\n",
       "        [2.28387406e-04, 3.47954483e-04, 1.21454577e-04, ...,\n",
       "         1.23714824e-04, 1.68886632e-04, 1.22563943e-04],\n",
       "        [2.32604827e-04, 3.29013448e-04, 1.28014843e-04, ...,\n",
       "         1.23884121e-04, 1.70317304e-04, 1.17599462e-04]],\n",
       "\n",
       "       [[1.47446452e-04, 2.96174199e-04, 1.06104802e-04, ...,\n",
       "         4.95415698e-05, 1.10551402e-04, 1.79247450e-04],\n",
       "        [2.36346008e-04, 3.36508238e-04, 1.24687489e-04, ...,\n",
       "         1.22591853e-04, 1.59353818e-04, 9.94546936e-05],\n",
       "        [2.41194983e-04, 3.47900874e-04, 1.25383100e-04, ...,\n",
       "         1.25585124e-04, 1.56057868e-04, 1.05222854e-04],\n",
       "        ...,\n",
       "        [2.44988740e-04, 3.39755148e-04, 1.27788371e-04, ...,\n",
       "         1.24598548e-04, 1.52739711e-04, 1.16298339e-04],\n",
       "        [2.31381564e-04, 3.38982703e-04, 1.26301471e-04, ...,\n",
       "         1.22323574e-04, 1.60539508e-04, 1.14169343e-04],\n",
       "        [2.31517508e-04, 3.32047348e-04, 1.24127226e-04, ...,\n",
       "         1.21430676e-04, 1.66472862e-04, 1.15081159e-04]]],\n",
       "      shape=(4, 64, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81260874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 64, 5000), dtype=float32, numpy=\n",
       "array([[[1.12514266e-04, 2.23259922e-04, 6.15297249e-05, ...,\n",
       "         6.33913223e-05, 1.57999486e-04, 1.34142581e-04],\n",
       "        [2.24163901e-04, 3.53680662e-04, 1.30546076e-04, ...,\n",
       "         1.20510158e-04, 1.67891340e-04, 9.84475919e-05],\n",
       "        [2.23144059e-04, 3.37189354e-04, 1.26456565e-04, ...,\n",
       "         1.22453566e-04, 1.62833108e-04, 1.00729550e-04],\n",
       "        ...,\n",
       "        [2.32919265e-04, 3.39150371e-04, 1.21412821e-04, ...,\n",
       "         1.25228340e-04, 1.51621061e-04, 1.20937577e-04],\n",
       "        [2.31672486e-04, 3.44316504e-04, 1.17227486e-04, ...,\n",
       "         1.23874546e-04, 1.63023476e-04, 1.11811649e-04],\n",
       "        [2.25805503e-04, 3.47522349e-04, 1.23126199e-04, ...,\n",
       "         1.23148275e-04, 1.69446255e-04, 1.13670518e-04]],\n",
       "\n",
       "       [[1.33764363e-04, 2.00560782e-04, 1.13622555e-04, ...,\n",
       "         6.27676636e-05, 8.23706287e-05, 3.12353688e-04],\n",
       "        [9.59440003e-05, 2.28337434e-04, 1.06569569e-04, ...,\n",
       "         4.90231469e-05, 1.55797723e-04, 2.79480475e-04],\n",
       "        [2.34003455e-04, 3.32135620e-04, 1.24253580e-04, ...,\n",
       "         1.22030862e-04, 1.67993989e-04, 1.09853652e-04],\n",
       "        ...,\n",
       "        [2.32153194e-04, 3.44341708e-04, 1.27446721e-04, ...,\n",
       "         1.19022698e-04, 1.59978546e-04, 1.21809426e-04],\n",
       "        [2.32409118e-04, 3.48455651e-04, 1.24696046e-04, ...,\n",
       "         1.23135775e-04, 1.61297052e-04, 1.27275562e-04],\n",
       "        [2.37164888e-04, 3.30688083e-04, 1.21809455e-04, ...,\n",
       "         1.20721066e-04, 1.58671406e-04, 1.24390266e-04]],\n",
       "\n",
       "       [[1.19114309e-04, 1.73937442e-04, 7.14267080e-05, ...,\n",
       "         5.08646117e-05, 1.22634170e-04, 2.24676420e-04],\n",
       "        [7.56033114e-05, 2.12478073e-04, 6.98226795e-05, ...,\n",
       "         6.06557223e-05, 1.19965975e-04, 2.42279857e-04],\n",
       "        [2.23755182e-04, 3.41106730e-04, 1.21835437e-04, ...,\n",
       "         1.25801133e-04, 1.58681491e-04, 1.07595108e-04],\n",
       "        ...,\n",
       "        [2.29110563e-04, 3.37743404e-04, 1.18481119e-04, ...,\n",
       "         1.25874343e-04, 1.52592300e-04, 1.28047337e-04],\n",
       "        [2.28758799e-04, 3.20413819e-04, 1.27022111e-04, ...,\n",
       "         1.23694699e-04, 1.63814038e-04, 1.24019192e-04],\n",
       "        [2.38821609e-04, 3.24710156e-04, 1.29173903e-04, ...,\n",
       "         1.22507830e-04, 1.66750106e-04, 1.22278172e-04]],\n",
       "\n",
       "       [[1.09877845e-04, 1.90593317e-04, 8.54404570e-05, ...,\n",
       "         8.21886861e-05, 1.30798813e-04, 2.73981888e-04],\n",
       "        [2.21893250e-04, 3.49633629e-04, 1.30162123e-04, ...,\n",
       "         1.26120402e-04, 1.58468902e-04, 1.01052603e-04],\n",
       "        [2.36196895e-04, 3.49795155e-04, 1.29585402e-04, ...,\n",
       "         1.20994373e-04, 1.63350356e-04, 9.58833698e-05],\n",
       "        ...,\n",
       "        [2.30734979e-04, 3.35339049e-04, 1.24990023e-04, ...,\n",
       "         1.22119833e-04, 1.54798894e-04, 1.16919786e-04],\n",
       "        [2.37103493e-04, 3.56373144e-04, 1.26694911e-04, ...,\n",
       "         1.18825577e-04, 1.57894741e-04, 1.07552914e-04],\n",
       "        [2.37199361e-04, 3.27882619e-04, 1.32515968e-04, ...,\n",
       "         1.25410254e-04, 1.59602394e-04, 1.15365910e-04]]],\n",
       "      shape=(4, 64, 5000), dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = GPT_model((token_ids, attention_mask))\n",
    "probs = tf.nn.softmax(outputs, axis=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ebf9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InitializePositionalEmbedding…</span> │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,592,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_16          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,125,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ init_embeddings                 │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
       "│ (\u001b[38;5;33mInitializePositionalEmbedding…\u001b[0m │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_0 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_1 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_2 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_3 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_4 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_5 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_6 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_7 (\u001b[38;5;33mDecoderBlock\u001b[0m)  │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │    \u001b[38;5;34m12,592,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_16          │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Model_head (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5000\u001b[0m)          │     \u001b[38;5;34m5,125,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,984,072</span> (423.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,984,072\u001b[0m (423.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPT_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80e575a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diagram to gpt_model.png\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "try:\n",
    "    plot_model(\n",
    "        GPT_model,\n",
    "        to_file=\"gpt_model.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        expand_nested=True,\n",
    "        dpi=160\n",
    "    )\n",
    "    print(\"Saved diagram to gpt_model.png\")\n",
    "except Exception as e:\n",
    "    print(\"plot_model failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fdd9c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InitializePositionalEmbeddings name=init_embeddings, built=True>,\n",
       " <DecoderBlock name=decoder_block_0, built=True>,\n",
       " <DecoderBlock name=decoder_block_1, built=True>,\n",
       " <DecoderBlock name=decoder_block_2, built=True>,\n",
       " <DecoderBlock name=decoder_block_3, built=True>,\n",
       " <DecoderBlock name=decoder_block_4, built=True>,\n",
       " <DecoderBlock name=decoder_block_5, built=True>,\n",
       " <DecoderBlock name=decoder_block_6, built=True>,\n",
       " <DecoderBlock name=decoder_block_7, built=True>,\n",
       " <LayerNormalization name=layer_normalization_16, built=True>,\n",
       " <Dense name=Model_head, built=True>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe5fecf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110984072"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7809a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                               GPT MODEL SUMMARY                                \n",
      "================================================================================\n",
      "Total parameters:      110,984,072\n",
      "Total layers:          11\n",
      "Trainable weights:     101\n",
      "Final output shape(s): ['(unavailable)']\n",
      "--------------------------------------------------------------------------------\n",
      "Idx | Layer Type               | Layer Name              | Weight Name                  | Shape           |   Params\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "000 | InitializePositionalEmbeddings | init_embeddings         | embedding_matrix             | (5000, 1024)    | 5,120,000\n",
      "001 | DecoderBlock             | decoder_block_0         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "002 | DecoderBlock             | decoder_block_1         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "003 | DecoderBlock             | decoder_block_2         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "004 | DecoderBlock             | decoder_block_3         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "005 | DecoderBlock             | decoder_block_4         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "006 | DecoderBlock             | decoder_block_5         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "007 | DecoderBlock             | decoder_block_6         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "008 | DecoderBlock             | decoder_block_7         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         | Query_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Key_Vector_for_projection    | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Value_Vector_for_projection  | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | Output_projection            | (1024, 1024)    | 1,048,576\n",
      "    |                          |                         | kernel                       | (1024, 4096)    | 4,194,304\n",
      "    |                          |                         | bias                         | (4096,)         |    4,096\n",
      "    |                          |                         | kernel                       | (4096, 1024)    | 4,194,304\n",
      "    |                          |                         | bias                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "009 | LayerNormalization       | layer_normalization_16  | alpha                        | (1024,)         |    1,024\n",
      "    |                          |                         | beta                         | (1024,)         |    1,024\n",
      "    |                          |                         |                              |                 |         \n",
      "010 | Dense                    | Model_head              | kernel                       | (1024, 5000)    | 5,120,000\n",
      "    |                          |                         | bias                         | (5000,)         |    5,000\n",
      "    |                          |                         |                              |                 |         \n",
      "================================================================================\n",
      "Note: Only trainable weights are listed above. Output shapes may be unavailable\n",
      "for subclassed models or models not built symbolically.\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_model_summary(model):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'GPT MODEL SUMMARY':^80}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    total_layers = len(model.layers)\n",
    "    total_weights = sum(len(layer.trainable_weights) for layer in model.layers)\n",
    "    try:\n",
    "        output_shapes = [tuple(out.shape) for out in model.outputs]\n",
    "    except Exception:\n",
    "        output_shapes = [\"(unavailable)\"]\n",
    "    \n",
    "    print(f\"{'Total parameters:':<22} {total_params:,}\")\n",
    "    print(f\"{'Total layers:':<22} {total_layers}\")\n",
    "    print(f\"{'Trainable weights:':<22} {total_weights}\")\n",
    "    print(f\"{'Final output shape(s):':<22} {output_shapes if output_shapes else '(N/A)'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    header = f\"{'Idx':>3} | {'Layer Type':<24} | {'Layer Name':<23} | {'Weight Name':<28} | {'Shape':<15} | {'Params':>8}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_type = layer.__class__.__name__\n",
    "        layer_name = layer.name\n",
    "        weights = layer.trainable_weights\n",
    "        layer_weight_count = len(weights)\n",
    "\n",
    "        if layer_weight_count == 0:\n",
    "            print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {'-':<28} | {'-':<15} | {'0':>8}\")\n",
    "        else:\n",
    "            for j, w in enumerate(weights):\n",
    "                n = int(np.prod(w.shape)) if hasattr(w, \"shape\") else \"?\"\n",
    "                shape_str = str(tuple(w.shape))\n",
    "                weight_name = w.name\n",
    "                if j == 0:\n",
    "                    print(f\"{i:03} | {layer_type:<24} | {layer_name:<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "                else:\n",
    "                    print(f\"    | {'':<24} | {'':<23} | {weight_name:<28} | {shape_str:<15} | {n:>8,}\")\n",
    "        if layer_weight_count > 1:\n",
    "            print(f\"    | {'':<24} | {'':<23} | {'':<28} | {'':<15} | {'':>8}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"Note: Only trainable weights are listed above. Output shapes may be unavailable\\n\"\n",
    "          \"for subclassed models or models not built symbolically.\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usage:\n",
    "format_model_summary(GPT_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab03dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_book_training_data(book_text: str, \n",
    "#                              token_to_id_dict: Dict[str, int], \n",
    "#                              context_length: int = 512,\n",
    "#                              pad_value: int = 0):\n",
    "#     \"\"\"\n",
    "#     Prepare training data from a Gutenberg book\n",
    "#     \"\"\"\n",
    "#     # 1. Tokenize the entire book\n",
    "#     token_ids = [token_to_id_dict.get(c, pad_value) for c in book_text]\n",
    "    \n",
    "#     # 2. Create sliding windows\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "    \n",
    "#     # Slide window across the entire book\n",
    "#     for i in range(0, len(token_ids) - context_length, context_length):\n",
    "#         # Extract window of context_length + 1 tokens\n",
    "#         window = token_ids[i:i + context_length + 1]\n",
    "        \n",
    "#         if len(window) < context_length + 1:\n",
    "#             break  # Skip incomplete windows at the end\n",
    "        \n",
    "#         # Create input-target pair\n",
    "#         input_seq = window[:-1]   # [t1, t2, ..., t512]\n",
    "#         target_seq = window[1:]   # [t2, t3, ..., t513]\n",
    "        \n",
    "#         inputs.append(input_seq)\n",
    "#         targets.append(target_seq)\n",
    "    \n",
    "#     # 3. Convert to numpy arrays\n",
    "#     inputs = np.array(inputs, dtype=np.int32)\n",
    "#     targets = np.array(targets, dtype=np.int32)\n",
    "    \n",
    "#     # 4. Create padding masks (all 1s since we're using full context)\n",
    "#     masks = np.ones_like(inputs, dtype=np.int32)\n",
    "    \n",
    "#     return inputs, targets, masks\n",
    "\n",
    "# # Usage:\n",
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r', encoding='utf-8') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea8b1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'/home/akshat/GPT_from_scratch/text_data/pg76702.txt', 'r') as f:\n",
    "#     book_text = f.read()\n",
    "\n",
    "# print(f\"Total characters: {len(book_text)}\")\n",
    "# print(f\"Expected examples (rough): {len(book_text) // 512}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fc7c7",
   "metadata": {},
   "source": [
    "''' Stop '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "649d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to TensorFlow tensors\n",
    "# input_ids = tf.constant(inputs, dtype=tf.int32)\n",
    "# target_ids = tf.constant(targets, dtype=tf.int32)\n",
    "# attention_masks = tf.constant(masks, dtype=tf.int32)\n",
    "\n",
    "# model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,8,0.00001,4,0.1,sinusoidal_lookup_table)\n",
    "\n",
    "# # Compile your model\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.AdamW(learning_rate=1e-4), # type: ignore\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Train with .fit()\n",
    "# history = model.fit(\n",
    "#     x=[input_ids, ,  # Your model expects (token_ids, attention_mask)\n",
    "#     y=target_ids,\n",
    "#     batch_size=16,  # Start small since 677 examples isn't huge\n",
    "#     epochs=50,\n",
    "#     validation_split=0.1,  # Use 10% for validation\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True),\n",
    "#         keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e775cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, start_text, token_to_id, id_to_token, max_len=100, temperature=1.0):\n",
    "#     # Encode start text\n",
    "#     token_ids, attention_mask = tokenize_and_build_token_id(\n",
    "#         token_to_id, [start_text], max_seq_len=512\n",
    "#     )\n",
    "    \n",
    "#     generated = list(token_ids[0].numpy())  # flatten out\n",
    "#     mask = list(attention_mask[0].numpy())\n",
    "    \n",
    "#     for _ in range(max_len):\n",
    "#         # Trim to last 512 tokens\n",
    "#         x_tokens = np.array([generated[-512:]])\n",
    "#         x_mask   = np.array([mask[-512:]])\n",
    "\n",
    "#         # Forward pass with both inputs\n",
    "#         logits = model((x_tokens, x_mask), training=False)\n",
    "\n",
    "#         # Take last position logits\n",
    "#         next_logits = logits[0, -1] / temperature\n",
    "#         probs = tf.nn.softmax(next_logits).numpy()\n",
    "\n",
    "#         # Sample next token\n",
    "#         next_id = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "#         # Append\n",
    "#         generated.append(next_id)\n",
    "#         mask.append(1)  # mark as valid token\n",
    "\n",
    "#     # Decode\n",
    "#     return ''.join(id_to_token[i] for i in generated if i in id_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d50c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token_dict = {v: k for k, v in token_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43978b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = \"Akshat Khatri\"\n",
    "# result = generate_text(model, start, token_to_id_dict, id_to_token_dict, max_len=100, temperature=0.8)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94cf8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-v1')# Concatenate train + validation + test\n",
    "# all_texts = []\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "#     all_texts.extend(dataset[split][\"text\"])\n",
    "\n",
    "# # Remove empty lines\n",
    "# all_texts = [t.strip() for t in all_texts if t.strip() != \"\"]\n",
    "\n",
    "# # Join into one giant string\n",
    "# big_text = \"\\n\".join(all_texts)\n",
    "\n",
    "# # Write to file\n",
    "# with open(\"wikitext_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(big_text)\n",
    "\n",
    "# print(\"Saved dataset to wikitext_full.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c759f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, targets, masks = prepare_book_training_data(\n",
    "#     book_text, \n",
    "#     token_to_id_dict, \n",
    "#     context_length=512\n",
    "# )\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f51f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming you already have:\n",
    "# # - prepare_book_training_data()\n",
    "# # - token_to_id_dict\n",
    "\n",
    "# file_path = r\"/home/akshat/GPT_from_scratch/text_data/wikitext_full.txt\"\n",
    "\n",
    "# inputs_list, targets_list, masks_list = [], [], []\n",
    "\n",
    "# buffer = \"\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         buffer += line.strip() + \" \"\n",
    "#         # Process every ~5000 chars to avoid memory spike\n",
    "#         if len(buffer) > 5000:\n",
    "#             inp, tgt, msk = prepare_book_training_data(\n",
    "#                 buffer, token_to_id_dict, context_length=512\n",
    "#             )\n",
    "#             inputs_list.append(inp)\n",
    "#             targets_list.append(tgt)\n",
    "#             masks_list.append(msk)\n",
    "#             buffer = \"\"  # reset buffer\n",
    "\n",
    "# # Process any leftover buffer\n",
    "# if buffer.strip():\n",
    "#     inp, tgt, msk = prepare_book_training_data(\n",
    "#         buffer, token_to_id_dict, context_length=512\n",
    "#     )\n",
    "#     inputs_list.append(inp)\n",
    "#     targets_list.append(tgt)\n",
    "#     masks_list.append(msk)\n",
    "\n",
    "# # Concatenate all batches into final arrays\n",
    "# inputs = np.concatenate(inputs_list, axis=0)\n",
    "# targets = np.concatenate(targets_list, axis=0)\n",
    "# masks = np.concatenate(masks_list, axis=0)\n",
    "\n",
    "# print(f\"Created {len(inputs)} training examples\")\n",
    "# print(f\"Input shape: {inputs.shape}\")\n",
    "# print(f\"Target shape: {targets.shape}\")\n",
    "# print(f\"Masks shape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b06fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "    \"\"\"\n",
    "    Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Token sequence for model input\n",
    "        target_ids: Token sequence for model targets (shifted by 1)\n",
    "        attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "    Returns:\n",
    "        Serialized TFRecord example\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "        'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "        'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "    }\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "def convert_text_to_tfrecord(\n",
    "    text_file_path: str,\n",
    "    token_to_id_dict: Dict[str, int],\n",
    "    output_dir: str,\n",
    "    context_length: int = 512,\n",
    "    records_per_file: int = 1000,\n",
    "    pad_value: int = 0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convert text file to TFRecord files for GPT training.\n",
    "    \n",
    "    Process:\n",
    "    1. Read and tokenize entire text file\n",
    "    2. Create sliding windows of context_length + 1 tokens\n",
    "    3. Split each window into input/target pairs (shifted by 1)\n",
    "    4. Save as TFRecord files with specified number of records per file\n",
    "    \n",
    "    Args:\n",
    "        text_file_path: Path to your text file (e.g., WikiText-103)\n",
    "        token_to_id_dict: Character-to-ID mapping dictionary\n",
    "        output_dir: Directory to save TFRecord files\n",
    "        context_length: Sequence length for training\n",
    "        records_per_file: Number of examples per TFRecord file\n",
    "        pad_value: Token ID used for unknown characters\n",
    "        \n",
    "    Returns:\n",
    "        Path to output directory containing TFRecord files\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Reading text file: {text_file_path}\")\n",
    "    \n",
    "    # Step 1: Load and tokenize text\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(f\"Text length: {len(text):,} characters\")\n",
    "    \n",
    "    # Convert each character to token ID\n",
    "    print(\"Tokenizing text...\")\n",
    "    token_ids = [token_to_id_dict.get(char, pad_value) for char in text]\n",
    "    print(f\"Token length: {len(token_ids):,} tokens\")\n",
    "    \n",
    "    # Step 2: Calculate output size\n",
    "    num_examples = (len(token_ids) - context_length) // context_length\n",
    "    print(f\"Will create {num_examples:,} training examples\")\n",
    "    \n",
    "    # Step 3: Process sliding windows and write TFRecord files\n",
    "    file_count = 0\n",
    "    examples_in_current_file = 0\n",
    "    writer = None\n",
    "    \n",
    "    print(\"Creating TFRecord files...\")\n",
    "    \n",
    "    # Slide window across token sequence\n",
    "    for i in tqdm(range(0, len(token_ids) - context_length, context_length)):\n",
    "        \n",
    "        # Extract window of tokens\n",
    "        window = token_ids[i:i + context_length + 1]\n",
    "        if len(window) < context_length + 1:\n",
    "            break\n",
    "            \n",
    "        # Create input-target pair (GPT training format)\n",
    "        input_ids = window[:-1]    # First 512 tokens: [t1, t2, ..., t512]\n",
    "        target_ids = window[1:]    # Shifted by 1: [t2, t3, ..., t513]\n",
    "        attention_mask = [1] * context_length  # All valid tokens (no padding)\n",
    "        \n",
    "        # Start new TFRecord file if needed\n",
    "        if writer is None or examples_in_current_file >= records_per_file:\n",
    "            if writer is not None:\n",
    "                writer.close()\n",
    "            \n",
    "            tfrecord_filename = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "            writer = tf.io.TFRecordWriter(tfrecord_filename)\n",
    "            file_count += 1\n",
    "            examples_in_current_file = 0\n",
    "        \n",
    "        # Write training example to current TFRecord file\n",
    "        tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "        writer.write(tf_example)\n",
    "        examples_in_current_file += 1\n",
    "    \n",
    "    # Cleanup\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    \n",
    "    # Step 4: Save summary information\n",
    "    print(f\"\\nConversion complete!\")\n",
    "    print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "    print(f\"Total examples: {num_examples:,}\")\n",
    "    \n",
    "    # Write metadata file\n",
    "    metadata = {\n",
    "        'context_length': context_length,\n",
    "        'vocab_size': len(token_to_id_dict),\n",
    "        'num_examples': num_examples,\n",
    "        'num_files': file_count,\n",
    "        'records_per_file': records_per_file\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def create_tf_data_pipeline(\n",
    "    tfrecord_dir: str,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 32,\n",
    "    shuffle_buffer: int = 1000,\n",
    "    prefetch_buffer: int = tf.data.AUTOTUNE\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create tf.data pipeline from TFRecord files for training.\n",
    "    \n",
    "    Process:\n",
    "    1. Find all TFRecord files in directory\n",
    "    2. Create dataset that reads and parses TFRecord examples\n",
    "    3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "    Args:\n",
    "        tfrecord_dir: Directory containing TFRecord files\n",
    "        context_length: Expected sequence length in records\n",
    "        batch_size: Number of examples per training batch\n",
    "        shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "        prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "    Returns:\n",
    "        tf.data.Dataset ready for model.fit()\n",
    "    \"\"\"\n",
    "    # Step 1: Find TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Step 2: Define how to parse each TFRecord example\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_tfrecord_example(example_proto):\n",
    "        \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "        # Parse the serialized example\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        # Convert to correct data types\n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "        model_inputs = (input_ids, attention_mask)\n",
    "        model_targets = target_ids\n",
    "        \n",
    "        return model_inputs, model_targets\n",
    "    \n",
    "    # Step 3: Create and configure dataset pipeline\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "    dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Example usage (commented out)\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example usage for WikiText-103 or similar large text files\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "    # example_vocab = your_token_to_id_dict\n",
    "    \n",
    "    # # Step 2: Convert text to TFRecord format (run once)\n",
    "    # tfrecord_dir = convert_text_to_tfrecord(\n",
    "    #     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "    #     token_to_id_dict=example_vocab,\n",
    "    #     output_dir='./tfrecords',\n",
    "    #     context_length=512,\n",
    "    #     records_per_file=1000\n",
    "    # )\n",
    "    \n",
    "    # # Step 3: Create training pipeline (use for training)\n",
    "    # train_dataset = create_tf_data_pipeline(\n",
    "    #     tfrecord_dir=tfrecord_dir,\n",
    "    #     context_length=512,\n",
    "    #     batch_size=16\n",
    "    # )\n",
    "    \n",
    "    # print(\"TFRecord pipeline ready for training!\")\n",
    "    # # Now you can use train_dataset with your model's .fit() method\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "253d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define your vocabulary (replace with actual token_to_id_dict)\n",
    "# example_vocab = token_to_id_dict\n",
    "\n",
    "# # Step 2: Convert text to TFRecord format (run once)\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=example_vocab,\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=128,\n",
    "#     records_per_file=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9caffa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create training and validation pipeline (use for training)\n",
    "def create_train_val_datasets(tfrecord_dir: str, \n",
    "                             context_length: int,\n",
    "                             batch_size: int = 32,\n",
    "                             val_split: float = 0.1):\n",
    "    \"\"\"\n",
    "    Create training and validation datasets from TFRecord files\n",
    "    \"\"\"\n",
    "    # Find all TFRecord files\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "    # Split files for train/val\n",
    "    num_val_files = max(1, int(len(tfrecord_files) * val_split))\n",
    "    val_files = tfrecord_files[:num_val_files]\n",
    "    train_files = tfrecord_files[num_val_files:]\n",
    "    \n",
    "    print(f\"Using {len(train_files)} files for training, {len(val_files)} for validation\")\n",
    "    \n",
    "    # Feature description\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "    }\n",
    "    \n",
    "    def parse_function(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "        input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "        target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "        attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "        # Return in format your model expects: ([input_ids, attention_mask], targets)\n",
    "        return (input_ids, attention_mask), target_ids\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = tf.data.TFRecordDataset(train_files)\n",
    "    train_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(5000)  # Shuffle buffer\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_dataset = tf.data.TFRecordDataset(val_files)\n",
    "    val_dataset = val_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a91e6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def create_tf_example(input_ids: List[int], target_ids: List[int], attention_mask: List[int]) -> bytes:\n",
    "#     \"\"\"\n",
    "#     Create a serialized TFRecord example from input-target pair.\n",
    "    \n",
    "#     Args:\n",
    "#         input_ids: Token sequence for model input\n",
    "#         target_ids: Token sequence for model targets (shifted by 1)\n",
    "#         attention_mask: Mask for valid tokens (1) vs padding (0)\n",
    "        \n",
    "#     Returns:\n",
    "#         Serialized TFRecord example\n",
    "#     \"\"\"\n",
    "#     feature = {\n",
    "#         'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "#         'target_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=target_ids)),\n",
    "#         'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask))\n",
    "#     }\n",
    "    \n",
    "#     example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "#     return example.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def convert_text_to_tfrecord_sp(\n",
    "#     text_file_path: str,\n",
    "#     sp_model_path: str,\n",
    "#     output_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     records_per_file: int = 1000,\n",
    "#     overlap_size: int = 64,\n",
    "#     chunk_size: int = 100_000  # Number of characters read at a time\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Streaming version of text to TFRecord conversion with SentencePiece.\n",
    "#     Reads and tokenizes file incrementally to limit memory usage.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     file_size = os.path.getsize(text_file_path)\n",
    "#     print(f\"Reading and tokenizing text in chunks from: {text_file_path}\")\n",
    "    \n",
    "#     # Load SentencePiece processor\n",
    "#     sp = spm.SentencePieceProcessor()\n",
    "#     sp.load(sp_model_path)\n",
    "    \n",
    "#     print(f\"Loaded SentencePiece model from: {sp_model_path}\")\n",
    "#     print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "\n",
    "#     buffer_tokens = []\n",
    "#     step_size = context_length - overlap_size\n",
    "#     file_count = 0\n",
    "#     examples_in_current_file = 0\n",
    "#     writer = None\n",
    "    \n",
    "#     with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "#         with tqdm(total=file_size, unit='B', unit_scale=True, desc='Processing text') as pbar:\n",
    "#             while True:\n",
    "#                 chunk = file.read(chunk_size)\n",
    "#                 if not chunk:\n",
    "#                     break\n",
    "#                 buffer_tokens.extend(sp.encode_as_ids(chunk))\n",
    "#                 pbar.update(len(chunk.encode('utf-8')))  # update by bytes read\n",
    "                \n",
    "#                 # Process windows to create examples\n",
    "#                 while len(buffer_tokens) >= context_length + 1:\n",
    "#                     window = buffer_tokens[:context_length + 1]\n",
    "#                     input_ids = window[:-1]\n",
    "#                     target_ids = window[1:]\n",
    "#                     attention_mask = [1] * context_length\n",
    "                    \n",
    "#                     if writer is None or examples_in_current_file >= records_per_file:\n",
    "#                         if writer:\n",
    "#                             writer.close()\n",
    "#                         tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#                         writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "#                         file_count += 1\n",
    "#                         examples_in_current_file = 0\n",
    "                    \n",
    "#                     tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#                     writer.write(tf_example)\n",
    "#                     examples_in_current_file += 1\n",
    "                    \n",
    "#                     # Slide window forward by step_size tokens\n",
    "#                     buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "#     # Process any remaining tokens\n",
    "#     while len(buffer_tokens) >= context_length + 1:\n",
    "#         window = buffer_tokens[:context_length + 1]\n",
    "#         input_ids = window[:-1]\n",
    "#         target_ids = window[1:]\n",
    "#         attention_mask = [1] * context_length\n",
    "        \n",
    "#         if writer is None or examples_in_current_file >= records_per_file:\n",
    "#             if writer:\n",
    "#                 writer.close()\n",
    "#             tfrecord_path = os.path.join(output_dir, f'train_{file_count:04d}.tfrecord')\n",
    "#             writer = tf.io.TFRecordWriter(tfrecord_path)\n",
    "#             file_count += 1\n",
    "#             examples_in_current_file = 0\n",
    "\n",
    "#         tf_example = create_tf_example(input_ids, target_ids, attention_mask)\n",
    "#         writer.write(tf_example)\n",
    "#         examples_in_current_file += 1\n",
    "#         buffer_tokens = buffer_tokens[step_size:]\n",
    "\n",
    "#     if writer:\n",
    "#         writer.close()\n",
    "\n",
    "#     num_examples = file_count * records_per_file  # Approximate count\n",
    "\n",
    "#     print(f\"\\nConversion complete!\")\n",
    "#     print(f\"Created {file_count} TFRecord files in: {output_dir}\")\n",
    "#     print(f\"Approximate total examples: {num_examples}\")\n",
    "\n",
    "#     metadata = {\n",
    "#         'context_length': context_length,\n",
    "#         'vocab_size': sp.get_piece_size(),\n",
    "#         'num_examples': num_examples,\n",
    "#         'num_files': file_count,\n",
    "#         'records_per_file': records_per_file,\n",
    "#         'overlap_size': overlap_size,\n",
    "#         'sp_model_path': sp_model_path,\n",
    "#         'tokenization': 'SentencePiece'\n",
    "#     }\n",
    "\n",
    "#     metadata_path = os.path.join(output_dir, 'metadata.txt')\n",
    "#     with open(metadata_path, 'w') as f:\n",
    "#         for key, value in metadata.items():\n",
    "#             f.write(f\"{key}: {value}\\n\")\n",
    "#     print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "#     return output_dir\n",
    "\n",
    "\n",
    "\n",
    "# def create_tf_data_pipeline_sp(\n",
    "#     tfrecord_dir: str,\n",
    "#     context_length: int = 512,\n",
    "#     batch_size: int = 32,\n",
    "#     shuffle_buffer: int = 1000,\n",
    "#     prefetch_buffer: int = tf.data.AUTOTUNE\n",
    "# ) -> tf.data.Dataset:\n",
    "#     \"\"\"\n",
    "#     Create tf.data pipeline from TFRecord files for training (SentencePiece version).\n",
    "    \n",
    "#     Process:\n",
    "#     1. Find all TFRecord files in directory\n",
    "#     2. Create dataset that reads and parses TFRecord examples\n",
    "#     3. Apply shuffling, batching, and prefetching for efficient training\n",
    "    \n",
    "#     Args:\n",
    "#         tfrecord_dir: Directory containing TFRecord files\n",
    "#         context_length: Expected sequence length in records\n",
    "#         batch_size: Number of examples per training batch\n",
    "#         shuffle_buffer: Size of shuffle buffer (larger = more random)\n",
    "#         prefetch_buffer: Number of batches to prefetch (AUTOTUNE = automatic)\n",
    "        \n",
    "#     Returns:\n",
    "#         tf.data.Dataset ready for model.fit()\n",
    "#     \"\"\"\n",
    "#     # Step 1: Find TFRecord files\n",
    "#     tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "#     print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "    \n",
    "#     if not tfrecord_files:\n",
    "#         raise FileNotFoundError(f\"No TFRecord files found in {tfrecord_dir}\")\n",
    "    \n",
    "#     # Step 2: Define how to parse each TFRecord example\n",
    "#     feature_description = {\n",
    "#         'input_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'target_ids': tf.io.FixedLenFeature([context_length], tf.int64),\n",
    "#         'attention_mask': tf.io.FixedLenFeature([context_length], tf.int64)\n",
    "#     }\n",
    "    \n",
    "#     def parse_tfrecord_example(example_proto):\n",
    "#         \"\"\"Parse a single TFRecord example into model inputs and targets.\"\"\"\n",
    "#         # Parse the serialized example\n",
    "#         parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        \n",
    "#         # Convert to correct data types\n",
    "#         input_ids = tf.cast(parsed_features['input_ids'], tf.int32)\n",
    "#         target_ids = tf.cast(parsed_features['target_ids'], tf.int32)\n",
    "#         attention_mask = tf.cast(parsed_features['attention_mask'], tf.int32)\n",
    "        \n",
    "#         # Return in format expected by your GPT model: ((input_ids, attention_mask), targets)\n",
    "#         model_inputs = (input_ids, attention_mask)\n",
    "#         model_targets = target_ids\n",
    "        \n",
    "#         return model_inputs, model_targets\n",
    "    \n",
    "#     # Step 3: Create and configure dataset pipeline\n",
    "#     dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "#     dataset = dataset.map(parse_tfrecord_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(shuffle_buffer)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(prefetch_buffer)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     \"\"\"\n",
    "# #     Example usage assuming you have a pre-trained SentencePiece model\n",
    "# #     \"\"\"\n",
    "    \n",
    "# #     # Your pre-trained SentencePiece model path\n",
    "# #     sp_model_path = '/home/akshat/GPT_from_scratch/notebooks/spm_gpt.model'  # You provide this\n",
    "    \n",
    "# #     # Step 1: Convert text to TFRecords using your pre-trained model\n",
    "# #     tfrecord_dir = convert_text_to_tfrecord_sp(\n",
    "# #         text_file_path=r'/home/akshat/GPT_from_scratch/text_data/BookCorpus3_cleaned.txt',\n",
    "# #         sp_model_path=sp_model_path,  # Your trained model\n",
    "# #         output_dir='./tfrecords',\n",
    "# #         context_length=CONTEXT_LEN,  # Match your CONTEXT_LEN\n",
    "# #         records_per_file=1000,\n",
    "# #         overlap_size = CONTEXT_LEN // 2,\n",
    "# #         chunk_size = 150000\n",
    "# #     )\n",
    "    \n",
    "# #     # # Step 2: Create training pipeline\n",
    "# #     # train_dataset = create_tf_data_pipeline_sp(\n",
    "# #     #     tfrecord_dir=tfrecord_dir,\n",
    "# #     #     context_length=128,  # Match your CONTEXT_LEN\n",
    "# #     #     batch_size=16\n",
    "# #     # )\n",
    "    \n",
    "# #     print(\"SentencePiece TFRecord pipeline ready for training!\")\n",
    "    \n",
    "# #     # Step 3: Load your SentencePiece model for vocab size and generation\n",
    "# #     sp = spm.SentencePieceProcessor()\n",
    "# #     sp.load(sp_model_path)\n",
    "# #     VOCAB_SIZE = sp.get_piece_size()\n",
    "    \n",
    "# #     print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "#     # Now you can use:\n",
    "#     # - sp for tokenization in generation\n",
    "#     # - train_dataset for model.fit()\n",
    "#     # - VOCAB_SIZE for your model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ba0fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DATASETS\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"TFRecord pipeline ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4819661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef29291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Remove existing tfrecords directory\n",
    "# if os.path.exists('./tfrecords'):\n",
    "#     shutil.rmtree('./tfrecords')\n",
    "#     print(\"Removed existing tfrecords directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9252ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SOLUTION 1: Recreate TFRecords with correct context length\n",
    "# # Delete the existing tfrecords directory and recreate with CONTEXT_LEN\n",
    "\n",
    "\n",
    "# # Recreate with correct context length\n",
    "# tfrecord_dir = convert_text_to_tfrecord(\n",
    "#     text_file_path=r'/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,  # Make sure this variable is defined\n",
    "#     output_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # Use your actual context length (128)\n",
    "#     records_per_file=1000\n",
    "# )\n",
    "\n",
    "# # Now create datasets with matching context length\n",
    "# train_dataset, val_dataset = create_train_val_datasets(\n",
    "#     tfrecord_dir='./tfrecords',\n",
    "#     context_length=CONTEXT_LEN,  # This will now match\n",
    "#     batch_size=16,\n",
    "#     val_split=0.1\n",
    "# )\n",
    "\n",
    "# # Calculate steps per epoch\n",
    "# records_per_file = 1000  \n",
    "# tfrecord_files = tf.io.gfile.glob(os.path.join(\"./tfrecords\", \"*.tfrecord\"))\n",
    "# total_examples = len(tfrecord_files) * records_per_file\n",
    "# train_examples = int(total_examples * 0.9)\n",
    "# steps_per_epoch = train_examples // 16\n",
    "\n",
    "# print(f\"Files: {len(tfrecord_files)}\")\n",
    "# print(f\"Total examples: {total_examples}\")\n",
    "# print(f\"Steps per epoch: {steps_per_epoch}\") # When combined with batch_size sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a189899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def prepare_tfrecords(\n",
    "    text_file_path: str,\n",
    "    token_to_id_dict: dict,\n",
    "    context_length: int = 128,\n",
    "    records_per_file: int = 1000,\n",
    "    output_base_dir: str = './tfrecords',\n",
    "    version_name: str = None,\n",
    "    batch_size: int = 16,\n",
    "    val_split: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create TFRecords from text and return train/val datasets.\n",
    "    Stores TFRecords in a versioned folder to avoid overwriting previous ones.\n",
    "\n",
    "    Args:\n",
    "        text_file_path: Path to input text file.\n",
    "        token_to_id_dict: Character-to-id dictionary.\n",
    "        context_length: Sequence length for training.\n",
    "        records_per_file: Number of examples per TFRecord file.\n",
    "        output_base_dir: Base folder to store TFRecords.\n",
    "        version_name: Optional unique folder name. If None, uses context_length.\n",
    "        batch_size: Batch size for dataset.\n",
    "        val_split: Fraction of data to use as validation.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, steps_per_epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine output folder\n",
    "    if version_name is None:\n",
    "        version_name = f\"context_{context_length}_bs{batch_size}\"\n",
    "    output_dir = os.path.join(output_base_dir, version_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert text to TFRecords\n",
    "    tfrecord_dir = convert_text_to_tfrecord(\n",
    "        text_file_path=text_file_path,\n",
    "        token_to_id_dict=token_to_id_dict,\n",
    "        output_dir=output_dir,\n",
    "        context_length=context_length,\n",
    "        records_per_file=records_per_file\n",
    "    )\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset, val_dataset = create_train_val_datasets(\n",
    "        tfrecord_dir=tfrecord_dir,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        val_split=val_split\n",
    "    )\n",
    "    def count_tfrecord_examples(tfrecord_files):\n",
    "        count = 0\n",
    "        for tfrecord_file in tfrecord_files:\n",
    "            for _ in tf.data.TFRecordDataset(tfrecord_file):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    # inside prepare_tfrecords\n",
    "    tfrecord_files = tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecord\"))\n",
    "    total_examples = count_tfrecord_examples(tfrecord_files)\n",
    "    train_examples = int(total_examples * (1 - val_split))\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "    print(f\"TFRecord folder: {tfrecord_dir}\")\n",
    "    print(f\"Total examples: {total_examples}\")\n",
    "    print(f\"Train examples: {train_examples}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    return train_dataset, val_dataset, steps_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b03ad574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset,val_dataset,steps_per_epoch = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d131044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_24, val_ds_24, steps_24 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=24\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a6f3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_32, val_ds_32, steps_32 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9916a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_44, val_ds_44, steps_44 = prepare_tfrecords(\n",
    "#     text_file_path='/home/akshat/GPT_from_scratch/notebooks/wikitext_full.txt',\n",
    "#     token_to_id_dict=token_to_id_dict,\n",
    "#     context_length=128,\n",
    "#     batch_size=44\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9933902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file: /home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt\n",
      "Text length: 4,347,531 characters\n",
      "Tokenizing text...\n",
      "Token length: 4,347,531 tokens\n",
      "Will create 67,929 training examples\n",
      "Creating TFRecord files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67930/67930 [00:01<00:00, 38916.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete!\n",
      "Created 68 TFRecord files in: ./tfrecords/context_64_bs64\n",
      "Total examples: 67,929\n",
      "Metadata saved to: ./tfrecords/context_64_bs64/metadata.txt\n",
      "Found 68 TFRecord files\n",
      "Using 62 files for training, 6 for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:24:27.780313: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:390] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-08-30 08:24:27.855721: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:27.945970: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:28.133403: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:28.509933: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:29.307072: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:30.943169: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-08-30 08:24:33.902031: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord folder: ./tfrecords/context_64_bs64\n",
      "Total examples: 67930\n",
      "Train examples: 61137\n",
      "Steps per epoch: 955\n"
     ]
    }
   ],
   "source": [
    "train_ds_64, val_ds_64, steps_64 = prepare_tfrecords(\n",
    "    text_file_path='/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt',\n",
    "    token_to_id_dict=token_to_id_dict,\n",
    "    context_length=CONTEXT_LEN,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "train_ds_64 = train_ds_64.shuffle(10000)\n",
    "val_ds_64 = val_ds_64.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e152da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODER_BLOCKS = 5\n",
    "# ATTENTION_HEADS = 8\n",
    "\n",
    "# GPT_model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,0.3)\n",
    "# _ = GPT_model((token_ids, attention_mask))\n",
    "# GPT_model.summary(expand_nested=True)\n",
    "\n",
    "# # training (stable): use logits\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0)\n",
    "# GPT_model.compile(optimizer=opt, loss=loss) # type: ignore\n",
    "\n",
    "# # inference probs (when you actually need them)\n",
    "# logits = GPT_model((token_ids, attention_mask), training=False)\n",
    "# probs = keras.ops.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d46c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "DECODER_BLOCKS = 6\n",
    "ATTENTION_HEADS = 4\n",
    "DROPOUT_RATE = 0.2\n",
    "STEPS_PER_EPOCH = steps_64  # e.g. 955\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH  # 95,500\n",
    "WARMUP_STEPS = int(0.1 * TOTAL_STEPS)   # 9,550\n",
    "LR = 1e-5\n",
    "\n",
    "lr_schedule = CosineDecayWithWarmup(\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    peak_learning_rate=5e-4,\n",
    "    min_learning_rate=5e-6\n",
    ")\n",
    "\n",
    "model = GPT(D_MODEL,VOCAB_SIZE,CONTEXT_LEN,ATTENTION_HEADS,0.00001,DECODER_BLOCKS,DROPOUT_RATE)\n",
    "# Compile your model (same as before)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0,beta_2=0.95) # type: ignore\n",
    "model.compile(optimizer=opt, loss=loss,metrics=['accuracy']) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total epochs:  100\n",
      "Steps per epoch: 955\n",
      "Total steps: 95500\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:25:27.744229: I external/local_xla/xla/service/service.cc:163] XLA service 0x15667fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-30 08:25:27.744262: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-08-30 08:25:28.264570: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-30 08:25:29.314684: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:62] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt_1_1/decoder_block_0_1/self_attention_layer_8_1/dropout/random_uniform/RandomUniform\n",
      "2025-08-30 08:25:29.571210: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:25:31.379417: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91200\n",
      "2025-08-30 08:25:34.061146: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:34.768544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_111', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.041600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38', 2744 bytes spill stores, 2660 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.598204: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:35.909770: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18108', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:36.769407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_17511', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.026562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13536', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.100827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12468 bytes spill stores, 12708 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.323302: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.833319: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 12036 bytes spill stores, 12220 bytes spill loads\n",
      "\n",
      "2025-08-30 08:25:37.990262: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_124', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-08-30 08:26:02.820469: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_2059', 468 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_2055', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "I0000 00:00:1756542362.997881    1520 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m327/955\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 333ms/step - accuracy: 0.2684 - loss: 5.7694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:27:53.823627: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:27:59.161874: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_123', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:27:59.346130: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_112', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:27:59.978910: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_18107', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:00.569359: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:00.783343: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_13535', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:01.929648: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_17510', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:01.999892: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:02.007062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 12036 bytes spill stores, 12220 bytes spill loads\n",
      "\n",
      "2025-08-30 08:28:24.734816: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_2169', 468 bytes spill stores, 484 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_2167', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step - accuracy: 0.3446 - loss: 3.9893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:31:57.139796: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:32:07.230190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-08-30 08:32:08.048781: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:08.843210: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:09.137387: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-30 08:32:09.151587: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_36', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_01_val_loss_1.9395.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:16.729382: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:16.729470: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:16.729504: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n",
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 1.93948, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 402ms/step - accuracy: 0.3954 - loss: 2.6552 - val_accuracy: 0.4226 - val_loss: 1.9395\n",
      "Epoch 2/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:13\u001b[0m 334ms/step - accuracy: 0.4162 - loss: 1.9548"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:31.978328: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:31.978374: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:31.978412: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_02_val_loss_1.9318.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:32:37.338226: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:32:37.338295: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:32:37.338337: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.93948 to 1.93178, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23ms/step - accuracy: 0.4180 - loss: 1.9536 - val_accuracy: 0.4241 - val_loss: 1.9318\n",
      "Epoch 3/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.4311 - loss: 1.8920\n",
      "Epoch 3: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_03_val_loss_1.7152.keras\n",
      "\n",
      "Epoch 3: val_loss improved from 1.93178 to 1.71517, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 349ms/step - accuracy: 0.4424 - loss: 1.8385 - val_accuracy: 0.4714 - val_loss: 1.7152\n",
      "Epoch 4/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:01\u001b[0m 321ms/step - accuracy: 0.4646 - loss: 1.7447\n",
      "Epoch 4: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_04_val_loss_1.7157.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:38:35.358899: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:38:35.358959: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:38:35.359004: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss did not improve from 1.71517\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.4664 - loss: 1.7371 - val_accuracy: 0.4723 - val_loss: 1.7157\n",
      "Epoch 5/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.4751 - loss: 1.6974\n",
      "Epoch 5: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_05_val_loss_1.5713.keras\n",
      "\n",
      "Epoch 5: val_loss improved from 1.71517 to 1.57129, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 350ms/step - accuracy: 0.4846 - loss: 1.6595 - val_accuracy: 0.5097 - val_loss: 1.5713\n",
      "Epoch 6/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:01\u001b[0m 320ms/step - accuracy: 0.5036 - loss: 1.5900\n",
      "Epoch 6: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_06_val_loss_1.5715.keras\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.57129\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5021 - loss: 1.5897 - val_accuracy: 0.5093 - val_loss: 1.5715\n",
      "Epoch 7/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.5118 - loss: 1.5577\n",
      "Epoch 7: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_07_val_loss_1.4692.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:04.541194: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:50:04.541274: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:50:04.541316: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 1.57129 to 1.46917, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 353ms/step - accuracy: 0.5190 - loss: 1.5325 - val_accuracy: 0.5377 - val_loss: 1.4692\n",
      "Epoch 8/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:02\u001b[0m 322ms/step - accuracy: 0.5359 - loss: 1.4677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:20.586527: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_08_val_loss_1.4690.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 08:50:26.246684: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 08:50:26.246736: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 08:50:26.246769: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss improved from 1.46917 to 1.46902, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23ms/step - accuracy: 0.5343 - loss: 1.4735 - val_accuracy: 0.5382 - val_loss: 1.4690\n",
      "Epoch 9/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705ms/step - accuracy: 0.5386 - loss: 1.4617\n",
      "Epoch 9: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_09_val_loss_1.3968.keras\n",
      "\n",
      "Epoch 9: val_loss improved from 1.46902 to 1.39676, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 729ms/step - accuracy: 0.5437 - loss: 1.4431 - val_accuracy: 0.5567 - val_loss: 1.3968\n",
      "Epoch 10/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:36\u001b[0m 868ms/step - accuracy: 0.5494 - loss: 1.4061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:02:28.486692: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:02:28.486750: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:02:28.486780: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_10_val_loss_1.3992.keras\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.39676\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 35ms/step - accuracy: 0.5519 - loss: 1.4037 - val_accuracy: 0.5579 - val_loss: 1.3992\n",
      "Epoch 11/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730ms/step - accuracy: 0.5575 - loss: 1.3914\n",
      "Epoch 11: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_11_val_loss_1.3481.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:14:38.653176: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:14:38.653247: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:14:38.653279: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: val_loss improved from 1.39676 to 1.34815, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 750ms/step - accuracy: 0.5616 - loss: 1.3772 - val_accuracy: 0.5715 - val_loss: 1.3481\n",
      "Epoch 12/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:45\u001b[0m 431ms/step - accuracy: 0.5701 - loss: 1.3446\n",
      "Epoch 12: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_12_val_loss_1.3475.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:15:02.140081: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:15:02.140180: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:15:02.140243: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss improved from 1.34815 to 1.34746, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 28ms/step - accuracy: 0.5688 - loss: 1.3474 - val_accuracy: 0.5729 - val_loss: 1.3475\n",
      "Epoch 13/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.5735 - loss: 1.3339\n",
      "Epoch 13: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_13_val_loss_1.3130.keras\n",
      "\n",
      "Epoch 13: val_loss improved from 1.34746 to 1.31297, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 453ms/step - accuracy: 0.5756 - loss: 1.3263 - val_accuracy: 0.5818 - val_loss: 1.3130\n",
      "Epoch 14/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:22\u001b[0m 470ms/step - accuracy: 0.5816 - loss: 1.3076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:22:36.433668: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:22:36.433759: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:22:36.433808: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_14_val_loss_1.3156.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:22:41.908626: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:22:41.908676: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:22:41.908712: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss did not improve from 1.31297\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5819 - loss: 1.3079 - val_accuracy: 0.5809 - val_loss: 1.3156\n",
      "Epoch 15/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.5829 - loss: 1.2999\n",
      "Epoch 15: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_15_val_loss_1.2893.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:28:13.040540: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:28:13.040615: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:28:13.040655: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 1.31297 to 1.28933, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 354ms/step - accuracy: 0.5845 - loss: 1.2943 - val_accuracy: 0.5900 - val_loss: 1.2893\n",
      "Epoch 16/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:57\u001b[0m 317ms/step - accuracy: 0.5880 - loss: 1.2776\n",
      "Epoch 16: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_16_val_loss_1.2897.keras\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.28933\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5877 - loss: 1.2800 - val_accuracy: 0.5897 - val_loss: 1.2897\n",
      "Epoch 17/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5908 - loss: 1.2720\n",
      "Epoch 17: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_17_val_loss_1.2776.keras\n",
      "\n",
      "Epoch 17: val_loss improved from 1.28933 to 1.27764, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 351ms/step - accuracy: 0.5912 - loss: 1.2712 - val_accuracy: 0.5925 - val_loss: 1.2776\n",
      "Epoch 18/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:48\u001b[0m 306ms/step - accuracy: 0.5925 - loss: 1.2690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:34:26.541810: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_18_val_loss_1.2734.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:34:31.937368: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:34:31.937436: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:34:31.937479: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: val_loss improved from 1.27764 to 1.27344, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 24ms/step - accuracy: 0.5918 - loss: 1.2707 - val_accuracy: 0.5922 - val_loss: 1.2734\n",
      "Epoch 19/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.5937 - loss: 1.2615\n",
      "Epoch 19: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_19_val_loss_1.2719.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:11.416009: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:40:11.416059: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:40:11.416088: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: val_loss improved from 1.27344 to 1.27194, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 349ms/step - accuracy: 0.5938 - loss: 1.2617 - val_accuracy: 0.5939 - val_loss: 1.2719\n",
      "Epoch 20/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:46\u001b[0m 432ms/step - accuracy: 0.5905 - loss: 1.2647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:27.182574: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:40:27.182611: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:40:27.182624: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_20_val_loss_1.2742.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:40:32.812566: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_6]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss did not improve from 1.27194\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.5927 - loss: 1.2595 - val_accuracy: 0.5936 - val_loss: 1.2742\n",
      "Epoch 21/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.5967 - loss: 1.2513\n",
      "Epoch 21: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_21_val_loss_1.2586.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:46:06.785287: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: val_loss improved from 1.27194 to 1.25856, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 355ms/step - accuracy: 0.5971 - loss: 1.2500 - val_accuracy: 0.5979 - val_loss: 1.2586\n",
      "Epoch 22/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:14\u001b[0m 334ms/step - accuracy: 0.5949 - loss: 1.2538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:46:25.549340: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:46:25.549398: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:46:25.549408: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_22_val_loss_1.2531.keras\n",
      "\n",
      "Epoch 22: val_loss improved from 1.25856 to 1.25311, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25ms/step - accuracy: 0.5947 - loss: 1.2556 - val_accuracy: 0.5983 - val_loss: 1.2531\n",
      "Epoch 23/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6008 - loss: 1.2362\n",
      "Epoch 23: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_23_val_loss_1.2417.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:12.076908: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:52:12.076971: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:52:12.077014: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss improved from 1.25311 to 1.24175, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 351ms/step - accuracy: 0.6008 - loss: 1.2369 - val_accuracy: 0.6028 - val_loss: 1.2417\n",
      "Epoch 24/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:05\u001b[0m 324ms/step - accuracy: 0.6091 - loss: 1.2174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:27.019364: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:52:27.019429: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:52:27.019476: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_24_val_loss_1.2405.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:52:33.971344: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: val_loss improved from 1.24175 to 1.24047, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 27ms/step - accuracy: 0.6082 - loss: 1.2172 - val_accuracy: 0.6039 - val_loss: 1.2405\n",
      "Epoch 25/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6034 - loss: 1.2273\n",
      "Epoch 25: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_25_val_loss_1.2424.keras\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.24047\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6035 - loss: 1.2274 - val_accuracy: 0.6027 - val_loss: 1.2424\n",
      "Epoch 26/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:13\u001b[0m 334ms/step - accuracy: 0.6108 - loss: 1.2128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:58:27.301510: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:58:27.301568: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:58:27.301610: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_26_val_loss_1.2345.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 09:58:33.248979: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 09:58:33.249054: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 09:58:33.249096: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: val_loss improved from 1.24047 to 1.23450, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 27ms/step - accuracy: 0.6071 - loss: 1.2163 - val_accuracy: 0.6042 - val_loss: 1.2345\n",
      "Epoch 27/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6053 - loss: 1.2199\n",
      "Epoch 27: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_27_val_loss_1.2418.keras\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.23450\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6060 - loss: 1.2190 - val_accuracy: 0.6027 - val_loss: 1.2418\n",
      "Epoch 28/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:03\u001b[0m 323ms/step - accuracy: 0.6040 - loss: 1.2317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:04:27.238359: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:04:27.238402: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:04:27.238416: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_28_val_loss_1.2368.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:04:32.950511: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:04:32.950580: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:04:32.950627: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: val_loss did not improve from 1.23450\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16ms/step - accuracy: 0.6027 - loss: 1.2304 - val_accuracy: 0.6042 - val_loss: 1.2368\n",
      "Epoch 29/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6096 - loss: 1.2070\n",
      "Epoch 29: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_29_val_loss_1.2273.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:10:06.744126: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:10:06.744220: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:10:06.744275: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss improved from 1.23450 to 1.22727, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 360ms/step - accuracy: 0.6085 - loss: 1.2107 - val_accuracy: 0.6064 - val_loss: 1.2273\n",
      "Epoch 30/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:36\u001b[0m 422ms/step - accuracy: 0.6065 - loss: 1.2136\n",
      "Epoch 30: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_30_val_loss_1.2253.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:10:35.569342: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:10:35.569447: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:10:35.569500: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 1.22727 to 1.22529, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 33ms/step - accuracy: 0.6076 - loss: 1.2113 - val_accuracy: 0.6075 - val_loss: 1.2253\n",
      "Epoch 31/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.6106 - loss: 1.2022\n",
      "Epoch 31: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_31_val_loss_1.2184.keras\n",
      "\n",
      "Epoch 31: val_loss improved from 1.22529 to 1.21836, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 359ms/step - accuracy: 0.6103 - loss: 1.2039 - val_accuracy: 0.6107 - val_loss: 1.2184\n",
      "Epoch 32/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:47\u001b[0m 433ms/step - accuracy: 0.6154 - loss: 1.2010\n",
      "Epoch 32: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_32_val_loss_1.2210.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:16:52.296359: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:16:52.296405: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:16:52.296436: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6152 - loss: 1.1957 - val_accuracy: 0.6104 - val_loss: 1.2210\n",
      "Epoch 33/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.6114 - loss: 1.2001\n",
      "Epoch 33: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_33_val_loss_1.2269.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:30.271103: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:22:30.271166: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:22:30.271211: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6106 - loss: 1.2032 - val_accuracy: 0.6069 - val_loss: 1.2269\n",
      "Epoch 34/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:48\u001b[0m 306ms/step - accuracy: 0.6105 - loss: 1.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:40.097738: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_34_val_loss_1.2302.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:22:45.946858: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:22:45.946952: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:22:45.947009: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.6091 - loss: 1.2034 - val_accuracy: 0.6069 - val_loss: 1.2302\n",
      "Epoch 35/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6109 - loss: 1.2008\n",
      "Epoch 35: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_35_val_loss_1.2273.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:23.954625: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:23.954685: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:23.954725: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 346ms/step - accuracy: 0.6101 - loss: 1.2044 - val_accuracy: 0.6072 - val_loss: 1.2273\n",
      "Epoch 36/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:33\u001b[0m 418ms/step - accuracy: 0.6091 - loss: 1.2121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:35.584793: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:35.584839: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:35.584850: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_36_val_loss_1.2289.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:28:41.231392: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:28:41.231465: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:28:41.231511: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: val_loss did not improve from 1.21836\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.6105 - loss: 1.2076 - val_accuracy: 0.6086 - val_loss: 1.2289\n",
      "Epoch 37/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6106 - loss: 1.2022\n",
      "Epoch 37: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_37_val_loss_1.2107.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:12.315030: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:12.315111: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:12.315154: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: val_loss improved from 1.21836 to 1.21068, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 357ms/step - accuracy: 0.6118 - loss: 1.1984 - val_accuracy: 0.6114 - val_loss: 1.2107\n",
      "Epoch 38/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:50\u001b[0m 309ms/step - accuracy: 0.6182 - loss: 1.1806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:32.856592: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:32.856678: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:32.856773: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_38_val_loss_1.2154.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:34:38.459152: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:34:38.459195: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:34:38.459227: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6164 - loss: 1.1874 - val_accuracy: 0.6116 - val_loss: 1.2154\n",
      "Epoch 39/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6149 - loss: 1.1880\n",
      "Epoch 39: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_39_val_loss_1.2148.keras\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 347ms/step - accuracy: 0.6144 - loss: 1.1893 - val_accuracy: 0.6112 - val_loss: 1.2148\n",
      "Epoch 40/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:09\u001b[0m 329ms/step - accuracy: 0.6215 - loss: 1.1756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:40:24.788370: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:40:24.788424: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_40_val_loss_1.2156.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:40:30.418866: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:40:30.418925: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:40:30.418961: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: val_loss did not improve from 1.21068\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.6181 - loss: 1.1813 - val_accuracy: 0.6114 - val_loss: 1.2156\n",
      "Epoch 41/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6157 - loss: 1.1846\n",
      "Epoch 41: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_41_val_loss_1.2101.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:46:03.312628: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:46:03.312665: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:46:03.312687: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss improved from 1.21068 to 1.21006, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 350ms/step - accuracy: 0.6153 - loss: 1.1861 - val_accuracy: 0.6123 - val_loss: 1.2101\n",
      "Epoch 42/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:57\u001b[0m 316ms/step - accuracy: 0.6204 - loss: 1.1675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:46:17.680799: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:46:17.680844: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:46:17.680854: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_42_val_loss_1.2146.keras\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.6200 - loss: 1.1721 - val_accuracy: 0.6116 - val_loss: 1.2146\n",
      "Epoch 43/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6158 - loss: 1.1837\n",
      "Epoch 43: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_43_val_loss_1.2137.keras\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6157 - loss: 1.1854 - val_accuracy: 0.6125 - val_loss: 1.2137\n",
      "Epoch 44/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:59\u001b[0m 318ms/step - accuracy: 0.6139 - loss: 1.1921\n",
      "Epoch 44: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_44_val_loss_1.2118.keras\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.21006\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6133 - loss: 1.1915 - val_accuracy: 0.6131 - val_loss: 1.2118\n",
      "Epoch 45/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6171 - loss: 1.1792\n",
      "Epoch 45: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_45_val_loss_1.2038.keras\n",
      "\n",
      "Epoch 45: val_loss improved from 1.21006 to 1.20376, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 348ms/step - accuracy: 0.6171 - loss: 1.1799 - val_accuracy: 0.6160 - val_loss: 1.2038\n",
      "Epoch 46/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:09\u001b[0m 457ms/step - accuracy: 0.6232 - loss: 1.1655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 10:58:07.739056: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 10:58:07.739111: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 10:58:07.739123: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_46_val_loss_1.2071.keras\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.6208 - loss: 1.1719 - val_accuracy: 0.6155 - val_loss: 1.2071\n",
      "Epoch 47/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.6178 - loss: 1.1768\n",
      "Epoch 47: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_47_val_loss_1.2048.keras\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 346ms/step - accuracy: 0.6175 - loss: 1.1785 - val_accuracy: 0.6149 - val_loss: 1.2048\n",
      "Epoch 48/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:59\u001b[0m 318ms/step - accuracy: 0.6211 - loss: 1.1768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:03:59.646990: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:03:59.647056: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:03:59.647068: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_48_val_loss_1.2110.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:04:06.535451: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:04:06.535542: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:04:06.535590: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.6207 - loss: 1.1766 - val_accuracy: 0.6139 - val_loss: 1.2110\n",
      "Epoch 49/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.6183 - loss: 1.1764\n",
      "Epoch 49: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_49_val_loss_1.2070.keras\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.20376\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 347ms/step - accuracy: 0.6189 - loss: 1.1740 - val_accuracy: 0.6142 - val_loss: 1.2070\n",
      "Epoch 50/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:03\u001b[0m 323ms/step - accuracy: 0.6200 - loss: 1.1736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:09:50.284128: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:09:50.284182: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:09:50.284217: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_50_val_loss_1.2032.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:09:55.896736: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n",
      "2025-08-30 11:09:55.896812: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18257925123837337519\n",
      "2025-08-30 11:09:55.896860: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9461568589138220430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50: val_loss improved from 1.20376 to 1.20318, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 43ms/step - accuracy: 0.6217 - loss: 1.1707 - val_accuracy: 0.6152 - val_loss: 1.2032\n",
      "Epoch 51/100\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.6186 - loss: 1.1746\n",
      "Epoch 51: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_51_val_loss_1.1951.keras\n",
      "\n",
      "Epoch 51: val_loss improved from 1.20318 to 1.19508, saving model to best_model.keras\n",
      "\u001b[1m955/955\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 360ms/step - accuracy: 0.6196 - loss: 1.1713 - val_accuracy: 0.6186 - val_loss: 1.1951\n",
      "Epoch 52/100\n",
      "\u001b[1m 14/955\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:58\u001b[0m 445ms/step - accuracy: 0.6215 - loss: 1.1607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 11:16:18.098477: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10633177541358495391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: saving model to /home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_52_val_loss_1.1959.keras\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    # Save model every epoch with epoch number\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='/home/akshat/GPT_from_scratch/notebooks/word_level_checkpoints/model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.keras',\n",
    "        save_freq='epoch',\n",
    "        save_best_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model separately\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # More reasonable early stopping\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,  # Wait 10 epochs before stopping\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    keras.callbacks.CSVLogger('training_log.csv'),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=\"./word_logs\", \n",
    "        histogram_freq=1, \n",
    "        profile_batch=0,\n",
    "        write_graph=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total epochs: \",EPOCHS)\n",
    "print(f\"Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"Total steps: {EPOCHS * STEPS_PER_EPOCH}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_64,\n",
    "    validation_data=val_ds_64,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "if 'learning_rate' in history.history:\n",
    "    plt.plot(history.history['learning_rate'])\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "train_perplexity = [np.exp(loss) for loss in history.history['loss']]\n",
    "val_perplexity = [np.exp(loss) for loss in history.history['val_loss']]\n",
    "plt.plot(train_perplexity, label='Train Perplexity')\n",
    "plt.plot(val_perplexity, label='Val Perplexity')\n",
    "plt.title('Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c498d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now training should work\n",
    "# history = model.fit(\n",
    "#     train_ds_32,\n",
    "#     validation_data=val_ds_32,\n",
    "#     epochs=50,\n",
    "#     steps_per_epoch=steps_32,\n",
    "#     callbacks=[\n",
    "#         keras.callbacks.ModelCheckpoint(filepath='model_epoch_{epoch:02d}.keras',save_freq='epoch'),  # saves with epoch number   save_freq='epoch',                        # save every epoch    save_best_only=False,                     # save all epochs    verbose=1)\n",
    "#         keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, verbose=1),\n",
    "#         keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, verbose=1),\n",
    "#         keras.callbacks.CSVLogger('training_log.csv'),\n",
    "#         keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1, profile_batch=0)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e45dbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Positional embeddings are working. Shape: (1, 128, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
      "  return saving_lib.save_model(model, filepath)\n"
     ]
    }
   ],
   "source": [
    "# pick a small dummy batch\n",
    "import tensorflow as tf\n",
    "\n",
    "dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1)\n",
    "dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)  # batch_size=1, length=CONTEXT_LEN\n",
    "dum_model.save('model_epoch_1.keras')\n",
    "# run the embeddings layer only\n",
    "pos_layer = dum_model.get_layer('init_embeddings')  # or however your layer is named\n",
    "try:\n",
    "    pos_emb = pos_layer(dummy_input)\n",
    "    print(\"✅ Positional embeddings are working. Shape:\", pos_emb.shape)\n",
    "except Exception as e:\n",
    "    print(\"❌ Embedding test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0b33b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'final_gpt_model.keras'\n",
      "Training history saved as 'training_history.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save the final model\n",
    "model.save('final_gpt_model.keras')\n",
    "print(\"Model saved as 'final_gpt_model.keras'\")\n",
    "\n",
    "# Optional: Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved as 'training_history.pkl'\")\n",
    "\n",
    "# Step 5: Load model later (when needed)\n",
    "def load_trained_model():\n",
    "    \"\"\"Load your saved model\"\"\"\n",
    "    loaded_model = keras.models.load_model('best_model.keras')  # or 'final_gpt_model.keras'\n",
    "    return loaded_model\n",
    "\n",
    "# Usage for inference later:\n",
    "# model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "87a42df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_model.save('dum_GPT.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "306ce64e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded best_model.keras\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras. Please ensure the file is an accessible `.keras` zip file.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded epoch 163 model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras. Please ensure the file is an accessible `.keras` zip file.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[307]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded epoch 163 model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded epoch 161 model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m CONTEXT_LEN = model._context_length  \u001b[38;5;66;03m# Use the model's actual context length\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:200\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format.load_model_from_hdf5(\n\u001b[32m    197\u001b[39m         filepath, custom_objects=custom_objects, \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m\n\u001b[32m    198\u001b[39m     )\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    207\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    217\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmight have a different name).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    218\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: File not found: filepath=/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# token_to_id_dict = tokenize_and_build_vocabulary_tf([r'/home/akshat/GPT_from_scratch/text_data/jane_austen_clean.txt'])\n",
    "id_to_token_dict = {id_val: token for token, id_val in token_to_id_dict.items()}\n",
    "\n",
    "# Use the latest and best model - try the best_model.keras first, then latest checkpoint\n",
    "try:\n",
    "    model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_299_val_loss_0.0130.keras')\n",
    "    print(\"✅ Loaded best_model.keras\")\n",
    "except:\n",
    "    try:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_163_val_loss_0.0303.keras')\n",
    "        print(\"✅ Loaded epoch 163 model\")\n",
    "    except:\n",
    "        model = keras.models.load_model(r'/home/akshat/GPT_from_scratch/notebooks/char_level_checkpoints/model_epoch_161_val_loss_0.0305.keras')\n",
    "        print(\"✅ Loaded epoch 161 model\")\n",
    "\n",
    "CONTEXT_LEN = model._context_length  # Use the model's actual context length\n",
    "\n",
    "# Debug: Print vocabulary info\n",
    "print(f\"Vocabulary size: {len(token_to_id_dict)}\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Get model info from your custom GPT model\n",
    "try:\n",
    "    print(f\"Model vocab size: {model._vocab_size}\")\n",
    "    print(f\"Model context length: {model._context_length}\")\n",
    "    print(f\"Model d_model: {model._d_model}\")\n",
    "    print(f\"Model attention heads: {model._attention_heads}\")\n",
    "    print(f\"Model decoder blocks: {model._decoder_blocks}\")\n",
    "    print(f\"Vocab size matches model: {model._vocab_size == len(token_to_id_dict)}\")\n",
    "    \n",
    "    if model._vocab_size != len(token_to_id_dict):\n",
    "        print(f\"⚠️  VOCAB SIZE MISMATCH! Model expects {model._vocab_size}, got {len(token_to_id_dict)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting model info: {e}\")\n",
    "\n",
    "print(f\"Sample characters in vocab: {list(token_to_id_dict.keys())[:30]}\")\n",
    "print(f\"Common characters present: {['a' in token_to_id_dict, 'e' in token_to_id_dict, ' ' in token_to_id_dict, '.' in token_to_id_dict]}\")\n",
    "\n",
    "# Check for problematic characters in the gibberish output\n",
    "gibberish = \"4ff.mtm 64m86rfstmfm?.fmmftms777mtmkf  tm7n7m77m77\"\n",
    "print(f\"Checking gibberish characters:\")\n",
    "for char in set(gibberish):\n",
    "    if char in token_to_id_dict:\n",
    "        print(f\"  '{char}' -> ID {token_to_id_dict[char]} ✓\")\n",
    "    else:\n",
    "        print(f\"  '{char}' -> NOT IN VOCAB ✗\")\n",
    "\n",
    "def encode_text(text, token_to_id_dict):\n",
    "    \"\"\"Encode text to token IDs using character-level tokenizer - convert to lowercase since dataset is lowercase\"\"\"\n",
    "    # Convert input to lowercase since your dataset was lowercased\n",
    "    text = text.lower()\n",
    "    \n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "        if char in token_to_id_dict:\n",
    "            token_ids.append(token_to_id_dict[char])\n",
    "        else:\n",
    "            print(f\"Warning: '{char}' (ord: {ord(char)}) not in vocabulary, skipping\")\n",
    "            continue\n",
    "    return token_ids\n",
    "\n",
    "def decode_ids(token_ids, id_to_token_dict):\n",
    "    \"\"\"Decode token IDs back to text using character-level tokenizer\"\"\"\n",
    "    text = \"\"\n",
    "    for token_id in token_ids:\n",
    "        if token_id in id_to_token_dict:\n",
    "            text += id_to_token_dict[token_id]\n",
    "        else:\n",
    "            print(f\"Warning: token ID {token_id} not in vocabulary\")\n",
    "    return text\n",
    "\n",
    "def get_special_token_ids():\n",
    "    \"\"\"Get special token IDs - adjust these based on your tokenizer setup\"\"\"\n",
    "    # For Jane Austen data, likely no special PAD token, use newline as EOS\n",
    "    pad_id = token_to_id_dict.get('<PAD>', None)\n",
    "    eos_id = token_to_id_dict.get('\\n', None)  # Use newline as natural stopping point\n",
    "    print(f\"Special tokens - PAD: {pad_id}, EOS (newline): {eos_id}\")\n",
    "    return pad_id, eos_id\n",
    "\n",
    "def top_k_sampling(logits, k=10):\n",
    "    \"\"\"Sample from logits using top-k sampling\"\"\"\n",
    "    # Ensure we don't sample more than available tokens\n",
    "    k = min(k, len(logits))\n",
    "    \n",
    "    values, indices = tf.math.top_k(logits, k=k)\n",
    "    last_val = values[-1]\n",
    "    filtered_logits = tf.where(\n",
    "        logits < last_val,\n",
    "        tf.fill(tf.shape(logits), float('-inf')),\n",
    "        logits\n",
    "    )\n",
    "    probs = tf.nn.softmax(filtered_logits).numpy()\n",
    "    \n",
    "    # Add small epsilon to avoid numerical issues\n",
    "    probs = probs + 1e-10\n",
    "    probs = probs / np.sum(probs)\n",
    "    \n",
    "    return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_k=10, use_argmax=False):\n",
    "    if not prompt.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"\\n--- Generation Debug ---\")\n",
    "    print(f\"Input prompt: '{prompt}' (will be lowercased)\")\n",
    "    \n",
    "    # Tokenize prompt with character-level tokenizer\n",
    "    input_tokens = encode_text(prompt, token_to_id_dict)\n",
    "    print(f\"Input tokens: {input_tokens}\")\n",
    "    print(f\"Input tokens decoded back: '{decode_ids(input_tokens, id_to_token_dict)}'\")\n",
    "    \n",
    "    if not input_tokens:\n",
    "        return \"Error: Could not tokenize input\"\n",
    "    \n",
    "    # Truncate if longer than context length\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    generated_tokens = input_tokens.copy()\n",
    "    pad_id, eos_id = get_special_token_ids()\n",
    "    \n",
    "    print(f\"Starting generation with {len(input_tokens)} input tokens...\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Prepare inputs - pad from left to maintain most recent context\n",
    "        input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "        \n",
    "        # Place tokens at the end of the context window\n",
    "        current_len = min(len(generated_tokens), CONTEXT_LEN)\n",
    "        start_idx = CONTEXT_LEN - current_len\n",
    "        input_ids[0, start_idx:] = generated_tokens[-current_len:]\n",
    "        attention_mask[0, start_idx:] = 1\n",
    "        \n",
    "        # Model forward pass\n",
    "        try:\n",
    "            logits = model((input_ids, attention_mask), training=False)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if not use_argmax:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "        except Exception as e:\n",
    "            print(f\"Model forward pass error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Sample next token\n",
    "        try:\n",
    "            if use_argmax:\n",
    "                # Use argmax (greedy) sampling for testing\n",
    "                next_token = int(np.argmax(next_token_logits))\n",
    "            else:\n",
    "                # Use top-k sampling\n",
    "                next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        except Exception as e:\n",
    "            print(f\"Sampling error: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Debug: Print first few tokens\n",
    "        if step < 10:\n",
    "            sampled_char = id_to_token_dict.get(next_token, f\"<UNK:{next_token}>\")\n",
    "            prob = float(tf.nn.softmax(next_token_logits)[next_token])\n",
    "            print(f\"Step {step}: Token {next_token} -> '{sampled_char}' (prob: {prob:.4f})\")\n",
    "        \n",
    "        # Check if token is valid\n",
    "        if next_token >= len(id_to_token_dict):\n",
    "            print(f\"Warning: Invalid token {next_token}, vocab size is {len(id_to_token_dict)}\")\n",
    "            break\n",
    "        \n",
    "        # Stop on special tokens\n",
    "        if pad_id is not None and next_token == pad_id:\n",
    "            print(f\"Stopping at step {step}: hit PAD token\")\n",
    "            break\n",
    "        if eos_id is not None and next_token == eos_id and step > 10:  # Don't stop too early\n",
    "            print(f\"Stopping at step {step}: hit EOS token (newline)\")\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(int(next_token))\n",
    "        \n",
    "        # Maintain sliding window\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    new_tokens = generated_tokens[len(input_tokens):]\n",
    "    response = decode_ids(new_tokens, id_to_token_dict)\n",
    "    print(f\"Generated {len(new_tokens)} new tokens: {new_tokens[:20]}...\")  # Show first 20\n",
    "    print(f\"Generated response: '{response}'\")\n",
    "    print(f\"--- End Debug ---\\n\")\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def chat_fn(message, history, temperature, max_length, top_k, use_argmax):\n",
    "    if not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    bot_response = generate_response(message, max_length=max_length, temperature=temperature, top_k=top_k, use_argmax=use_argmax)\n",
    "    history.append((message, bot_response))\n",
    "    return \"\", history\n",
    "\n",
    "# Quick test with the better model\n",
    "print(\"Testing improved model:\")\n",
    "test_cases = [\"the\", \"elizabeth\", \"it is a\"]\n",
    "\n",
    "for prompt in test_cases:\n",
    "    print(f\"\\nTesting with: '{prompt}'\")\n",
    "    tokens = encode_text(prompt, token_to_id_dict)\n",
    "    \n",
    "    # Create model input\n",
    "    input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "    attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "    input_ids[0, -len(tokens):] = tokens\n",
    "    attention_mask[0, -len(tokens):] = 1\n",
    "    \n",
    "    # Get model predictions\n",
    "    logits = model((input_ids, attention_mask), training=False)\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Show top 5 predictions\n",
    "    top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "    print(\"Top 5 predictions:\")\n",
    "    for i in range(5):\n",
    "        token_id = int(top_indices[i])\n",
    "        prob = float(top_probs[i])\n",
    "        char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "        print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "# Test with longer context\n",
    "print(f\"\\nTesting with longer Jane Austen context:\")\n",
    "long_prompt = \"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a\"\n",
    "tokens = encode_text(long_prompt, token_to_id_dict)\n",
    "print(f\"Context: '{long_prompt}'\")\n",
    "print(f\"Context length: {len(tokens)} tokens\")\n",
    "\n",
    "# Use reasonable context length\n",
    "context_tokens = tokens[-100:] if len(tokens) > 100 else tokens\n",
    "\n",
    "input_ids = np.zeros((1, 256), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, 256), dtype=np.int32)\n",
    "input_ids[0, -len(context_tokens):] = context_tokens\n",
    "attention_mask[0, -len(context_tokens):] = 1\n",
    "\n",
    "logits = model((input_ids, attention_mask), training=False)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "\n",
    "top_probs, top_indices = tf.nn.top_k(tf.nn.softmax(next_token_logits), k=5)\n",
    "print(\"Top 5 predictions after long context:\")\n",
    "for i in range(5):\n",
    "    token_id = int(top_indices[i])\n",
    "    prob = float(top_probs[i])\n",
    "    char = id_to_token_dict.get(token_id, f\"UNK_{token_id}\")\n",
    "    print(f\"  {i+1}. '{char}' (ID: {token_id}) - {prob:.4f}\")\n",
    "\n",
    "with gr.Blocks(title=\"My Character-Level GPT Bot Trained in Tensorflow\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Akshat's Character-Level GPT Model\")\n",
    "    gr.Markdown(\"Ask me anything! I'm a GPT model trained from scratch with character-level tokenization on Jane Austen data. Be gentle with me :)\")\n",
    "    \n",
    "    # Add vocab info\n",
    "    gr.Markdown(f\"**Model Info:** Vocabulary size: {len(token_to_id_dict)} characters\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=400, show_copy_button=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", placeholder=\"Type your message here...\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        temperature = gr.Slider(minimum=0.1, maximum=1.5, value=0.3, step=0.05, label=\"Temperature\")\n",
    "        max_length = gr.Slider(minimum=10, maximum=200, value=30, step=10, label=\"Max Length\")\n",
    "        top_k = gr.Slider(minimum=1, maximum=50, value=5, step=1, label=\"Top-K Sampling\")\n",
    "        use_argmax = gr.Checkbox(label=\"Use Argmax (Greedy) - for testing\", value=True)\n",
    "    \n",
    "    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    send_btn.click(chat_fn, [msg, chatbot, temperature, max_length, top_k, use_argmax], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        share=True,          # Generate public share link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=6019,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468aeb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:  VxxZEtRh5HHeYXZtxnhx5ICxxetRx5(ZxZMqx55xejhPxYW4Rx\n",
      "GPT:  RcT2-LeJPeRHVpMKhzo37xBoxti\n",
      "GPT:  qt-45HxGefn5ZZx48.UxMeTRuOLzRHxt\n",
      "GPT:  RxHxtX5RmeMxHd2tZ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[277]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Simple console loop\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     prompt = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prompt.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/ml-venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=50, temperature=1.0):\n",
    "    input_tokens = [token_to_id_dict.get(c, 0) for c in prompt if c in token_to_id_dict]\n",
    "    if len(input_tokens) == 0:\n",
    "        input_tokens = [0]\n",
    "\n",
    "    if len(input_tokens) > CONTEXT_LEN:\n",
    "        input_tokens = input_tokens[-CONTEXT_LEN:]\n",
    "\n",
    "    input_ids = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    input_ids[0, -len(input_tokens):] = input_tokens\n",
    "\n",
    "    attention_mask = np.zeros((1, CONTEXT_LEN), dtype=np.int32)\n",
    "    attention_mask[0, -len(input_tokens):] = 1\n",
    "\n",
    "    generated_tokens = input_tokens.copy()\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        next_token_logits = predictions[0, -1, :] / temperature\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        if next_token == 0:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "        # Update input_ids and attention_mask\n",
    "        if len(generated_tokens) > CONTEXT_LEN:\n",
    "            generated_tokens = generated_tokens[-CONTEXT_LEN:]\n",
    "\n",
    "        input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        attention_mask[0, -len(generated_tokens):] = 1\n",
    "\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    return ''.join([id_to_token.get(t, '') for t in generated_tokens[len(input_tokens):]]).strip()\n",
    "\n",
    "\n",
    "# Simple console loop\n",
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    if prompt.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "    response = generate_text(prompt, max_length=50, temperature=0.8)\n",
    "    print(\"GPT: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb3680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7fccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d11a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sinusoidal_lookup_table.shape)  # should be (CONTEXT_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67921403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive testing for YOUR custom model implementation...\n",
      "\n",
      "🔍 Testing sinusoidal lookup table creation...\n",
      "✅ Sinusoidal lookup table created successfully. Shape: (128, 128)\n",
      "   Table type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "   Table dtype: <dtype: 'float32'>\n",
      "\n",
      "🔍 Testing InitializePositionalEmbeddings layer...\n",
      "❌ Positional embeddings test failed: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing LayerNormalization layer...\n",
      "✅ LayerNormalization working. Input shape: (2, 128, 128), Output shape: (2, 128, 128)\n",
      "   Output mean (should be ~0): 0.000000\n",
      "   Output variance (should be ~1): 0.999990\n",
      "\n",
      "🔍 Testing YOUR SelfAttentionLayer...\n",
      "   Input embeddings shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "   Mask sample - seq 1: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] ... [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "✅ SelfAttentionLayer working. Output shape: (2, 128, 128)\n",
      "   Output range: [-0.7532, 0.7810]\n",
      "   Output mean: 0.0010\n",
      "   Output std: 0.0782\n",
      "   Attention heads: 8\n",
      "   d_head: 16\n",
      "   d_model: 128\n",
      "\n",
      "🔍 Testing YOUR DecoderBlock...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 29, in test_positional_embeddings\n",
      "    pos_layer = InitializePositionalEmbeddings(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_206177/3965528913.py\", line 11, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to InitializePositionalEmbeddings: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Input shape: (2, 128, 128)\n",
      "   Attention mask shape: (2, 128)\n",
      "✅ DecoderBlock working (training=False). Output shape: (2, 128, 128)\n",
      "✅ DecoderBlock working (training=True). Output shape: (2, 128, 128)\n",
      "   Input mean: 0.0003, Output mean: -0.0041\n",
      "   Output range: [-4.3376, 4.7553]\n",
      "\n",
      "🔍 Testing YOUR complete GPT model...\n",
      "❌ Full model test failed: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "\n",
      "🔍 Testing the specific embedding issue from your original code...\n",
      "❌ Original embedding test still fails: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n",
      "\n",
      "======================================================================\n",
      "🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\n",
      "======================================================================\n",
      "Sinusoidal Lookup Table........................... ✅ PASSED\n",
      "Positional Embeddings............................. ❌ FAILED\n",
      "Layer Normalization............................... ✅ PASSED\n",
      "YOUR Self Attention Layer......................... ✅ PASSED\n",
      "YOUR Decoder Block................................ ✅ PASSED\n",
      "YOUR Full Model Forward Pass...................... ❌ FAILED\n",
      "YOUR Original Embedding Issue..................... ❌ FAILED\n",
      "Model Compilation & Training...................... ❌ FAILED\n",
      "\n",
      "🏆 Overall: 4/8 tests passed\n",
      "⚠️  Some tests failed. Please fix the issues before training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 186, in test_your_full_model\n",
      "    model = GPT(\n",
      "            ^^^^\n",
      "  File \"/tmp/ipykernel_206177/1791215164.py\", line 64, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/models/model.py\", line 158, in __init__\n",
      "    Layer.__init__(self, *args, **kwargs)\n",
      "  File \"/home/akshat/ml/ml-venv/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 291, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized keyword arguments passed to GPT: {'sinusoidal_lookup_table': <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
      "array([[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00, ...,\n",
      "         1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
      "       [ 8.4147096e-01,  5.4030228e-01,  7.6172042e-01, ...,\n",
      "         1.0000000e+00,  1.1547820e-04,  1.0000000e+00],\n",
      "       [ 9.0929741e-01, -4.1614684e-01,  9.8704624e-01, ...,\n",
      "         9.9999994e-01,  2.3095640e-04,  1.0000000e+00],\n",
      "       ...,\n",
      "       [-6.1604047e-01,  7.8771454e-01,  9.9029869e-01, ...,\n",
      "         9.9986106e-01,  1.4434273e-02,  9.9989581e-01],\n",
      "       [ 3.2999083e-01,  9.4398415e-01,  7.4746519e-01, ...,\n",
      "         9.9985886e-01,  1.4549740e-02,  9.9989414e-01],\n",
      "       [ 9.7263008e-01,  2.3235910e-01, -2.1724481e-02, ...,\n",
      "         9.9985659e-01,  1.4665205e-02,  9.9989247e-01]],\n",
      "      shape=(128, 128), dtype=float32)>}\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_206177/3617421218.py\", line 255, in test_specific_embedding_issue\n",
      "    dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: GPT.__init__() takes from 1 to 8 positional arguments but 9 were given\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "CONTEXT_LEN = 128\n",
    "D_MODEL = 128\n",
    "VOCAB_SIZE = 94\n",
    "\n",
    "def test_sinusoidal_lookup_table():\n",
    "    \"\"\"Test the sinusoidal lookup table creation\"\"\"\n",
    "    print(\"🔍 Testing sinusoidal lookup table creation...\")\n",
    "    try:\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        print(f\"✅ Sinusoidal lookup table created successfully. Shape: {sinusoidal_lookup_table.shape}\")\n",
    "        print(f\"   Table type: {type(sinusoidal_lookup_table)}\")\n",
    "        print(f\"   Table dtype: {sinusoidal_lookup_table.dtype}\")\n",
    "        return sinusoidal_lookup_table\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sinusoidal lookup table creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_positional_embeddings(sinusoidal_lookup_table):\n",
    "    \"\"\"Test the positional embeddings layer\"\"\"\n",
    "    print(\"\\n🔍 Testing InitializePositionalEmbeddings layer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create the layer\n",
    "        pos_layer = InitializePositionalEmbeddings(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_pos_embeddings\"\n",
    "        )\n",
    "        \n",
    "        # Test input\n",
    "        dummy_input = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        print(f\"   Input shape: {dummy_input.shape}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Positional embeddings working. Output shape: {pos_emb.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {D_MODEL})\")\n",
    "        \n",
    "        # Check if embeddings are reasonable\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(pos_emb):.4f}\")\n",
    "        \n",
    "        return pos_layer, pos_emb\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Positional embeddings test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_layer_normalization():\n",
    "    \"\"\"Test LayerNormalization layer\"\"\"\n",
    "    print(\"\\n🔍 Testing LayerNormalization layer...\")\n",
    "    \n",
    "    try:\n",
    "        ln = LayerNormalization(eps=1e-5, name=\"test_ln\")\n",
    "        test_input = tf.random.normal((2, CONTEXT_LEN, D_MODEL))\n",
    "        \n",
    "        output = ln(test_input)\n",
    "        print(f\"✅ LayerNormalization working. Input shape: {test_input.shape}, Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check normalization properties\n",
    "        mean = tf.reduce_mean(output, axis=-1)\n",
    "        var = tf.reduce_mean(tf.square(output - tf.expand_dims(mean, -1)), axis=-1)\n",
    "        print(f\"   Output mean (should be ~0): {tf.reduce_mean(mean):.6f}\")\n",
    "        print(f\"   Output variance (should be ~1): {tf.reduce_mean(var):.6f}\")\n",
    "        \n",
    "        return ln, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LayerNormalization test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_self_attention():\n",
    "    \"\"\"Test YOUR custom SelfAttentionLayer\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR SelfAttentionLayer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your attention layer with correct parameter name\n",
    "        attn_layer = SelfAttentionLayer(attention_heads=8, name=\"test_attention\")\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        embeddings = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        # Make some positions masked (set to 0)\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 50:] = 0  # Mask second half of first sequence\n",
    "        attention_mask[1, 80:] = 0  # Mask last part of second sequence\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input embeddings shape: {embeddings.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample - seq 1: {attention_mask[0, :10].numpy()} ... {attention_mask[0, -10:].numpy()}\")\n",
    "        \n",
    "        # Test with your layer's expected input format: (embeddings, mask)\n",
    "        output = attn_layer([embeddings, attention_mask])\n",
    "        print(f\"✅ SelfAttentionLayer working. Output shape: {output.shape}\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        print(f\"   Output mean: {tf.reduce_mean(output):.4f}\")\n",
    "        print(f\"   Output std: {tf.math.reduce_std(output):.4f}\")\n",
    "        \n",
    "        # Verify attention heads are working\n",
    "        print(f\"   Attention heads: {attn_layer.attention_heads}\")\n",
    "        print(f\"   d_head: {attn_layer.d_head}\")\n",
    "        print(f\"   d_model: {attn_layer.d_model}\")\n",
    "        \n",
    "        return attn_layer, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ SelfAttentionLayer test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_decoder_block():\n",
    "    \"\"\"Test YOUR custom DecoderBlock\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR DecoderBlock...\")\n",
    "    \n",
    "    try:\n",
    "        # Create your decoder block\n",
    "        decoder = DecoderBlock(\n",
    "            d_model=D_MODEL,\n",
    "            n_heads=8,\n",
    "            dropout_rate=0.1,\n",
    "            epsilon=1e-5,\n",
    "            name=\"test_decoder\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        batch_size = 2\n",
    "        seq_len = CONTEXT_LEN\n",
    "        test_input = tf.random.normal((batch_size, seq_len, D_MODEL))\n",
    "        attention_mask = tf.ones((batch_size, seq_len), dtype=tf.float32)\n",
    "        \n",
    "        # Make some positions masked\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 60:] = 0\n",
    "        attention_mask[1, 90:] = 0\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Input shape: {test_input.shape}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        \n",
    "        # Test training=False\n",
    "        output = decoder(test_input, attention_mask, training=False)\n",
    "        print(f\"✅ DecoderBlock working (training=False). Output shape: {output.shape}\")\n",
    "        \n",
    "        # Test training=True\n",
    "        output_train = decoder(test_input, attention_mask, training=True)\n",
    "        print(f\"✅ DecoderBlock working (training=True). Output shape: {output_train.shape}\")\n",
    "        \n",
    "        # Check residual connections work (output should be different from input)\n",
    "        input_mean = tf.reduce_mean(test_input)\n",
    "        output_mean = tf.reduce_mean(output)\n",
    "        print(f\"   Input mean: {input_mean:.4f}, Output mean: {output_mean:.4f}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(output):.4f}, {tf.reduce_max(output):.4f}]\")\n",
    "        \n",
    "        return decoder, output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ DecoderBlock test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_your_full_model(sinusoidal_lookup_table):\n",
    "    \"\"\"Test YOUR complete GPT model\"\"\"\n",
    "    print(\"\\n🔍 Testing YOUR complete GPT model...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you do\n",
    "        model = GPT(\n",
    "            d_model=D_MODEL,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            context_length=CONTEXT_LEN,\n",
    "            attention_heads=8,\n",
    "            epsilon=1e-5,\n",
    "            decoder_blocks=4,\n",
    "            dropout_rate=0.1,\n",
    "            sinusoidal_lookup_table=sinusoidal_lookup_table,\n",
    "            name=\"test_gpt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare test inputs\n",
    "        token_ids = tf.constant([[1, 2, 3, 4, 5] + [0] * (CONTEXT_LEN - 5)], dtype=tf.int32)\n",
    "        attention_mask = tf.ones((1, CONTEXT_LEN), dtype=tf.float32)\n",
    "        \n",
    "        # Set mask to 0 for padding tokens\n",
    "        attention_mask = attention_mask.numpy()\n",
    "        attention_mask[0, 5:] = 0  # Only first 5 tokens are real\n",
    "        attention_mask = tf.constant(attention_mask)\n",
    "        \n",
    "        print(f\"   Token IDs shape: {token_ids.shape}\")\n",
    "        print(f\"   Token IDs sample: {token_ids[0, :10].numpy()}\")\n",
    "        print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "        print(f\"   Mask sample: {attention_mask[0, :10].numpy()}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        logits = model([token_ids, attention_mask], training=False)\n",
    "        print(f\"✅ Full model forward pass successful!\")\n",
    "        print(f\"   Output logits shape: {logits.shape}\")\n",
    "        print(f\"   Expected shape: (1, {CONTEXT_LEN}, {VOCAB_SIZE})\")\n",
    "        \n",
    "        # Check output properties\n",
    "        print(f\"   Logits range: [{tf.reduce_min(logits):.4f}, {tf.reduce_max(logits):.4f}]\")\n",
    "        print(f\"   Logits mean: {tf.reduce_mean(logits):.4f}\")\n",
    "        \n",
    "        # Test with different batch size\n",
    "        token_ids_batch = tf.constant([\n",
    "            [1, 2, 3] + [0] * (CONTEXT_LEN - 3),\n",
    "            [4, 5, 6, 7] + [0] * (CONTEXT_LEN - 4)\n",
    "        ], dtype=tf.int32)\n",
    "        attention_mask_batch = tf.ones((2, CONTEXT_LEN), dtype=tf.float32)\n",
    "        attention_mask_batch = attention_mask_batch.numpy()\n",
    "        attention_mask_batch[0, 3:] = 0  # First seq has 3 real tokens\n",
    "        attention_mask_batch[1, 4:] = 0  # Second seq has 4 real tokens\n",
    "        attention_mask_batch = tf.constant(attention_mask_batch)\n",
    "        \n",
    "        logits_batch = model([token_ids_batch, attention_mask_batch], training=False)\n",
    "        print(f\"✅ Batch processing successful! Output shape: {logits_batch.shape}\")\n",
    "        \n",
    "        # Test training mode\n",
    "        logits_train = model([token_ids, attention_mask], training=True)\n",
    "        print(f\"✅ Training mode successful! Output shape: {logits_train.shape}\")\n",
    "        \n",
    "        return model, logits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Full model test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "def test_specific_embedding_issue():\n",
    "    \"\"\"Test the specific embedding issue you encountered\"\"\"\n",
    "    print(\"\\n🔍 Testing the specific embedding issue from your original code...\")\n",
    "    \n",
    "    try:\n",
    "        # Create model exactly as you did\n",
    "        sinusoidal_lookup_table = prepare_sinusoidal_lookup_table(D_MODEL, CONTEXT_LEN)\n",
    "        dum_model = GPT(D_MODEL, VOCAB_SIZE, CONTEXT_LEN, 8, 0.00001, 4, 0.1, sinusoidal_lookup_table)\n",
    "        \n",
    "        # Test exactly as you did\n",
    "        dummy_input = tf.constant([[0] * CONTEXT_LEN], dtype=tf.int32)\n",
    "        \n",
    "        # Get the embeddings layer\n",
    "        pos_layer = dum_model.get_layer('init_embeddings')\n",
    "        \n",
    "        # Run the embeddings layer\n",
    "        pos_emb = pos_layer(dummy_input)\n",
    "        print(f\"✅ Your original embedding test now works! Shape: {pos_emb.shape}\")\n",
    "        print(f\"   Input was all zeros: {dummy_input[0, :5].numpy()}\")\n",
    "        print(f\"   Output range: [{tf.reduce_min(pos_emb):.4f}, {tf.reduce_max(pos_emb):.4f}]\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Original embedding test still fails: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_model_compilation_and_training(model):\n",
    "    \"\"\"Test model compilation and training capability\"\"\"\n",
    "    print(\"\\n🔍 Testing model compilation and training...\")\n",
    "    \n",
    "    try:\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.AdamW(learning_rate=1e-4),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"✅ Model compilation successful!\")\n",
    "        \n",
    "        # Create dummy training data\n",
    "        batch_size = 4\n",
    "        dummy_x = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        dummy_mask = tf.ones((batch_size, CONTEXT_LEN), dtype=tf.float32)\n",
    "        # Create some realistic masking\n",
    "        for i in range(batch_size):\n",
    "            seq_len = tf.random.uniform([], minval=10, maxval=CONTEXT_LEN, dtype=tf.int32)\n",
    "            dummy_mask = dummy_mask.numpy()\n",
    "            dummy_mask[i, seq_len:] = 0\n",
    "            dummy_mask = tf.constant(dummy_mask)\n",
    "        \n",
    "        dummy_y = tf.random.uniform((batch_size, CONTEXT_LEN), maxval=VOCAB_SIZE, dtype=tf.int32)\n",
    "        \n",
    "        print(f\"   Training data shapes: X={dummy_x.shape}, mask={dummy_mask.shape}, Y={dummy_y.shape}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        predictions = model.predict([dummy_x, dummy_mask], verbose=0)\n",
    "        print(f\"✅ Model prediction successful! Predictions shape: {predictions.shape}\")\n",
    "        \n",
    "        # Test training step\n",
    "        loss = model.train_on_batch([dummy_x, dummy_mask], dummy_y)\n",
    "        print(f\"✅ Training step successful! Loss: {loss}\")\n",
    "        \n",
    "        # Test model summary\n",
    "        print(f\"\\n📊 Model has {model.count_params():,} parameters\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model compilation/training test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def run_all_tests_for_your_model():\n",
    "    \"\"\"Run all tests specifically for your model implementation\"\"\"\n",
    "    print(\"🚀 Starting comprehensive testing for YOUR custom model implementation...\\n\")\n",
    "    \n",
    "    # Test 1: Sinusoidal lookup table\n",
    "    sinusoidal_lookup_table = test_sinusoidal_lookup_table()\n",
    "    if sinusoidal_lookup_table is None:\n",
    "        print(\"❌ Cannot proceed without sinusoidal lookup table\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Positional embeddings\n",
    "    pos_layer, pos_emb = test_positional_embeddings(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 3: Layer normalization\n",
    "    ln_layer, ln_output = test_layer_normalization()\n",
    "    \n",
    "    # Test 4: Your self attention\n",
    "    attn_layer, attn_output = test_your_self_attention()\n",
    "    \n",
    "    # Test 5: Your decoder block\n",
    "    decoder_layer, decoder_output = test_your_decoder_block()\n",
    "    \n",
    "    # Test 6: Your full model\n",
    "    model, logits = test_your_full_model(sinusoidal_lookup_table)\n",
    "    \n",
    "    # Test 7: Your specific embedding issue\n",
    "    embedding_issue_fixed = test_specific_embedding_issue()\n",
    "    \n",
    "    # Test 8: Model compilation and training\n",
    "    compilation_success = False\n",
    "    if model is not None:\n",
    "        compilation_success = test_model_compilation_and_training(model)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎯 TESTING SUMMARY FOR YOUR CUSTOM MODEL:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Sinusoidal Lookup Table\", sinusoidal_lookup_table is not None),\n",
    "        (\"Positional Embeddings\", pos_emb is not None),\n",
    "        (\"Layer Normalization\", ln_output is not None),\n",
    "        (\"YOUR Self Attention Layer\", attn_output is not None),\n",
    "        (\"YOUR Decoder Block\", decoder_output is not None),\n",
    "        (\"YOUR Full Model Forward Pass\", logits is not None),\n",
    "        (\"YOUR Original Embedding Issue\", embedding_issue_fixed),\n",
    "        (\"Model Compilation & Training\", compilation_success)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, result in tests if result)\n",
    "    total = len(tests)\n",
    "    \n",
    "    for test_name, result in tests:\n",
    "        status = \"✅ PASSED\" if result else \"❌ FAILED\"\n",
    "        print(f\"{test_name:.<50} {status}\")\n",
    "    \n",
    "    print(f\"\\n🏆 Overall: {passed}/{total} tests passed\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"🎉 All tests passed! Your model is working perfectly!\")\n",
    "        print(\"🚀 Your model is ready for training and inference!\")\n",
    "    elif passed >= total - 2:\n",
    "        print(\"🎊 Almost all tests passed! Your model is mostly working correctly!\")\n",
    "        print(\"🔧 Check the failed tests above for minor issues.\")\n",
    "    else:\n",
    "        print(\"⚠️  Some tests failed. Please fix the issues before training.\")\n",
    "    \n",
    "    return model if logits is not None else None\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == \"__main__\":\n",
    "    final_model = run_all_tests_for_your_model()\n",
    "    \n",
    "    if final_model is not None:\n",
    "        print(f\"\\n🎁 Model returned successfully!\")\n",
    "        print(f\"   Total parameters: {final_model.count_params():,}\")\n",
    "        print(f\"   Ready for: training, inference, and saving!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_bot(model, token_to_id_dict, prompt, max_length=100, temperature=0.7, context_len=128):\n",
    "    \"\"\"Generate text response from your GPT model\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_tokens = [token_to_id_dict.get(char, 0) for char in prompt]\n",
    "    \n",
    "    # Handle context length\n",
    "    if len(input_tokens) > context_len:\n",
    "        input_tokens = input_tokens[-context_len:]\n",
    "    \n",
    "    # Pad to context length\n",
    "    input_ids = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        input_ids[-len(input_tokens):] = input_tokens\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = np.zeros(context_len, dtype=np.int32)\n",
    "    if len(input_tokens) > 0:\n",
    "        attention_mask[-len(input_tokens):] = 1\n",
    "    \n",
    "    # Prepare for model\n",
    "    input_ids = np.expand_dims(input_ids, axis=0)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    \n",
    "    # Generate response token by token\n",
    "    generated_tokens = input_tokens.copy()\n",
    "    id_to_token = {v: k for k, v in token_to_id_dict.items()}\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "        \n",
    "        # Get last token logits (find the last non-zero position in attention mask)\n",
    "        last_pos = np.sum(attention_mask[0]) - 1\n",
    "        if last_pos < 0:\n",
    "            last_pos = 0\n",
    "        next_token_logits = predictions[0, last_pos, :] / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probabilities = tf.nn.softmax(next_token_logits).numpy()\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "        \n",
    "        # Stop if we hit a stop token or newline\n",
    "        if next_token == 0 or (next_token in token_to_id_dict.values() and id_to_token[next_token] == '\\n'):\n",
    "            break\n",
    "            \n",
    "        generated_tokens.append(next_token)\n",
    "        \n",
    "        # Update input for next iteration\n",
    "        if len(generated_tokens) > context_len:\n",
    "            generated_tokens = generated_tokens[-context_len:]\n",
    "        \n",
    "        # Create new input\n",
    "        new_input_ids = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_input_ids[0, -len(generated_tokens):] = generated_tokens\n",
    "        \n",
    "        new_attention_mask = np.zeros((1, context_len), dtype=np.int32)\n",
    "        if len(generated_tokens) > 0:\n",
    "            new_attention_mask[0, -len(generated_tokens):] = 1\n",
    "        \n",
    "        input_ids = new_input_ids\n",
    "        attention_mask = new_attention_mask\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    response_tokens = generated_tokens[len(input_tokens):]  # Only the new tokens\n",
    "    response = ''.join([id_to_token.get(token, '') for token in response_tokens])\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b200e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"]8'Vi3A;#sFöXöxr3ö][xöM6!dlwx—$pb:Orxx1JkW0:pöyyö;94œ!ööGHQöG:‘::$fwrg3Rg!R!/gxrgg/PöJIYPlö6öJ%6RpLp\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response_from_bot(model,token_to_id_dict,prompt = 'yoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
